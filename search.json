[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pensamiento estadístico",
    "section": "",
    "text": "Comenzaremos con una introducción a la ciencia de datos y el tipo de habilidades que los científicos de datos necesitan.\nDespués del módulo introductorio, el resto de este curso proporcionará la base estadística que sustenta el análisis de datos. La estadística juega un papel central en el enfoque de la ciencia de datos, en casi todas las soluciones basadas en datos, los científicos de datos ejercen el pensamiento estadístico al diseñar estrategias de recopilación de datos, obtener información de los métodos de visualización, obtener evidencia para la toma de decisiones y construir modelos para predecir tendencias futuras.\nEl módulo final presentará la filosofía de la inferencia bayesiana y las técnicas de modelado bayesiano utilizando estudios de casos ilustrativos.\n\n\n\n\n\nLicencia Creative Commons\n\n\nEste trabajo está bajo una licencia: Attribution-NonCommercial 4.0 International\nEres libre de adaptarlo para propósitos no comerciales otorgando el crédito correspondiente."
  },
  {
    "objectID": "est_cd.html",
    "href": "est_cd.html",
    "title": "Estadística y ciencia de datos",
    "section": "",
    "text": "La estadística es una rama de las matemáticas aplicadas que implica la recopilación, descripción, análisis e inferencia de conclusiones a partir de datos. Para hacer esto la estadística utiliza la teoría de la probabilidad.\nLa idea es que podemos aprender sobre las propiedades de grandes conjuntos de objetos o eventos (a las que llamamos población). Debido a que en muchos casos recopilar los datos completos sobre una población completa es demasiado costoso, difícil o imposible, las estadísticas comienzan estudiando un número menor de objetos o eventos que es un subconjunto de la población (al que llamamos muestra).\nUn científico de datos analiza y comunica el valor de los datos\nCuando el científico de datos trabaja dentro de una empresa, el objetivo es generalmente facilitar la mejora de los procesos de toma de decisiones. Desde Google Analytics hasta las encuestas de clientes, la mayoría de las empresas tendrán al menos una fuente de datos de clientes que se está recopilando. Pero si no se usan bien, los datos no son útiles. La importancia de la ciencia de datos se basa en la capacidad de usar los datos para generar conocimientos útiles.\nEl análisis de los datos puede empezar desde una simple búsqueda en internet, sin embargo, contestar una pregunta específica con los datos puede ser tan difícil como generar la inspiración acerca de qué preguntas hacer en primer lugar. Los científicos de datos necesitan saber estadística y saber programar, pero también necesitan capacidad para comunicar sus resultados, curiosidad, persistencia y un agudo sentido de lo que sus líderes encontrarán significante."
  },
  {
    "objectID": "est_cd.html#la-estadística-en-la-ciencia-de-datos",
    "href": "est_cd.html#la-estadística-en-la-ciencia-de-datos",
    "title": "Estadística y ciencia de datos",
    "section": "La estadística en la ciencia de datos",
    "text": "La estadística en la ciencia de datos\nNos enfocaremos en las etapas del flujo de trabajo en las que los científicos de datos usan principalmente la estadística.\nIdentificar fuentes de datos: En ocasiones las fuentes de datos que ya están disponibles (por ejemplo datos históricos de ventas o cuestionarios que haya realizado la empresa) pueden servir para contestar la pregunta en cuestión. Otras veces es necesaria la recolección de nuevos datos. Esto puede incluir datos de otras empresas competidoras, datos abiertos demográficos o algún estudio específico que se requiera realizar para contestar la pregunta en cuestión. El recolectar datos algunas veces implicará diseñar una muestra que sea representativa de la población, los diseños muestrales se hacen utilizando métodos estadísticos. Más aún, para entender cualquier fuente de datos es importante entender el procedimeinto en el que se recolectó o el diseño muestral en su caso.\nPreparar los datos: Preparar los datos se enfoca en limpiarlos para que puedan ser utilizados. Para grandes conjuntos de datos, el muestreo estadístico puede acelerar enormemente el procesamiento y la exploración que se requiere para hacer la limpieza. En este paso, también se utiliza la estadística para el tratamiento de datos faltantes. Por distintas causas las bases de datos pueden contener datos faltantes, y muchas técnicas de modelado no pueden manejar los datos faltantes, por lo que si falta parte de una observación, se debe decidir si eliminarla por completo o hacer una imputación. La imputación se puede realizar con algún método estadístico para extrapolar las observaciones faltantes utilizando el resto de observaciones. Por último, los datos tienden a ser sucios, a veces debido a errores humanos, por lo que las bases de datos deben de explorarse en busca de posibles errores antes de utilizarse. Algunas medidas estadísticas pueden ayudar en éstas búsquedas.\nAnálisis exploratorio de datos: Una vez que tenemos todos los datos necesarios para el proyecto, se pueden utilizar métodos estadísticos exploratorios para exponer cualquier relación en los datos. Este paso se centra en encontrar anomalías, patrones y relaciones. Esto ayudará a descubrir información que se puede utilizar para responder a la pregunta de investigación planteada.\nModelado de datos: Después de explorar los datos, el científico de datos debe de seleccionar el mejor modelo para el proyecto. El objetivo del modelado es representar el comportamiento general de la población bajo estudio, teniendo en cuenta la pregunta que se desea responder. La elección de un modelo estadístico no es sencilla, puede estar íntimamente ligada a la pregunta específica que se está investigando, también puede guiarse por la forma de las relaciones entre las variables que se descubrieron en el análisis exploratorio o por el campo de especialidad de quién realiza el análisis. Usualmente, los científicos de datos elegirán un conjunto de modelos con los cuales correrán el análisis para al final decidir cuál es el que mejor se ajusta a los datos. Todos los modelos tendrán cierta incertidumbre, es trabajo del científico de datos cuantificar la incertidumbre restante en cada tipo de análisis, de manera que se tome en cuenta adecuadamente al momento de tomar decisiones. Todo esto es estadístico, los modelos, la cuantificación de la incertidumbre, incluso los métodos de selección de un modelo entre un conjunto de ellos.\nComunicación de resultados: El objetivo de un análisis es articular la información detrás de los valores y proporcionar una recomendación procesable. Esta etapa parece simple; sin embargo, puede ser un gran desafío. Comunicar los resultados requiere un cuidadoso equilibrio de comunicación, psicología, estadística y diseño para poder entregar información valiosa.\n\nConclusiones\n\nEl entendimiento de la estadística es sumamente importante para el científico de datos. Incluso si el área de especialidad del científico de datos es más enfocada a la computación, siempre será una buena idea el tener un entendimeinto general de cómo otras partes del flujo de trabajo están funcionando.\nLos datos + la estadística no son iguales a la objetividad."
  },
  {
    "objectID": "est_cd.html#referencias",
    "href": "est_cd.html#referencias",
    "title": "Estadística y ciencia de datos",
    "section": "Referencias",
    "text": "Referencias\n\nPwC. (2018). Data and analytics. \nKozyrkov C. (2020). Do you make decisions rationally?"
  },
  {
    "objectID": "analisis-exp-1.html",
    "href": "analisis-exp-1.html",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "",
    "text": "“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone –as the first step.” — John Tukey\nMuchas veces se le llama análisis descriptivo a una combinación de resúmenes, gráficas y tablas cuyos propósitos pueden englobarse en:\nDe forma más moderna, estos tres puntos pueden englobarse dentro del último término: análisis exploratorio. Esta fase del análisis de datos es fundamental, como la cita de Tukey explica arriba, y se caracteríza por un enfoque de detective: quizá tenemos algunas preguntas, algunas sospechas, y en esta fase acumulamos indicios que nos indiquen caminos prometedores de investigación.\nEn contraste, tenemos el análisis confirmatorio, que busca validar hipótesis o dar respuestas correctamente cuantificadas en cuanto a su incertidumbre o grado de error. En esta parte somos más jueces que detectives, y utilizamos más maquinaria matemática (teoría de probabilidad) para especificar con claridad nuestros supuestos y poder hacer cálculos cuidadosos, generalmente basados en algún tipo de aleatorización.\nNinguno de los dos tipos de análisis funciona muy bien sin el otro, (Tukey (1980)) y explicaremos por qué un poco más adelante. Por el momento, para ilustrar el enfoque exploratorio, comenzaremos con datos que podemos describir de manera completa y efectiva sin necesidad de hacer resúmenes o aplicar técnicas avanzadas."
  },
  {
    "objectID": "analisis-exp-1.html#ejemplo-nacimientos",
    "href": "analisis-exp-1.html#ejemplo-nacimientos",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "1.1 Ejemplo: nacimientos",
    "text": "1.1 Ejemplo: nacimientos\nConsideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016. Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño, pueden representarse de manera razonablemente efectiva en una visualización de serie de tiempo\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\nnacimientos <- read_rds(\"datos/nacimientos/natalidad.rds\") |>\n   ungroup() |> \n   filter(year(fecha) == 2016, month(fecha) <= 3)\n\nExaminamos partes del contenido de la tabla:\n\ntab_1 <- nacimientos |> \n   select(fecha, n) |> \n   slice_head(n = 5)\ntab_2 <- nacimientos |> \n   select(fecha, n) |> \n   slice_tail(n = 5)\nkable(list(tab_1, tab_2)) |> kable_styling()\n\n\n\n\n  \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-01-01 \n    3952 \n  \n  \n    2016-01-02 \n    4858 \n  \n  \n    2016-01-03 \n    4665 \n  \n  \n    2016-01-04 \n    5948 \n  \n  \n    2016-01-05 \n    6087 \n  \n\n\n\n \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-03-27 \n    4112 \n  \n  \n    2016-03-28 \n    5805 \n  \n  \n    2016-03-29 \n    5957 \n  \n  \n    2016-03-30 \n    5766 \n  \n  \n    2016-03-31 \n    5497 \n  \n\n\n\n \n  \n\n\n\n\n\nEn un examen rápido de estos números no vemos nada fuera de orden. Los datos tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar de manera simple como sigue:\n\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\nEsta es una descripción de los datos, que quizá no es muy compacta pero muestra varios aspectos importantes. En este caso notamos algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:\n\ndomingos_tbl <- nacimientos |> \n   filter(weekdays(fecha) == \"Sunday\")\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = \"salmon\") +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\nObservamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente menos nacimentos. ¿Por qué crees que sea esto?\nAdicionalmente a estos patrones observamos otros aspectos interesantes:\n\nEl primero de enero hay considerablemente menos nacimientos de los que esperaríamos para un viernes. ¿Por qué?\nEl primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial este primero de marzo?\n¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?\n¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?\n\nLa confirmación de estas hipótesis, dependiendo de su forma, puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados con otros días de la semana) hasta muy compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes, procesos en el registro civil, etc.)"
  },
  {
    "objectID": "analisis-exp-1.html#procesos-generadores-de-datos",
    "href": "analisis-exp-1.html#procesos-generadores-de-datos",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "Procesos generadores de datos",
    "text": "Procesos generadores de datos\nDe este primer ejemplo donde usamos una gráfica simple:\n\n\n\n\n\n\nEl proceso generador de datos\n\n\n\nNótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales y de dominio para interpretar y hacer hipótesis acerca de lo que vemos en la gráfica. Una visión descontextualizada no tiene mucha utilidad. Las explicaciones son típicamente complejas e intervienen distintos aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados. Al conjunto de esos aspectos que determinan los datos que finalmente observamos le llamamos el proceso generador de datos.\n\n\nEl análisis de datos en general busca entender las partes importantes del proceso que los generó. En el análisis descriptivo y exploratorio buscamos iluminar ese proceso, proponer hipótesis y buscar caminos interesantes para investigar, ya sea con técnicas cuantitativas o con trabajo de campo (como sugiere el título de artículo de David A. Friedman: Statistical Models and Shoe Leather).\nCon la teoría de probabilidades podemos modelar más explícitamente partes de estos procesos generadores de datos, especialmente cuando controlamos parte de ese proceso generador mediante técnicas estadísticas de diseño, por ejemplo, usando aleatorización."
  },
  {
    "objectID": "analisis-exp-1.html#ejemplo-cálculos-renales",
    "href": "analisis-exp-1.html#ejemplo-cálculos-renales",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "Ejemplo (cálculos renales)",
    "text": "Ejemplo (cálculos renales)\nEn este ejemplo también intentaremos mostrar los datos completos sin intentar resumir.\nEste es un estudio real acerca de tratamientos para cálculos renales (Julious y Mullee (1994)). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.\nLa tabla original se ve como sigue (muestreamos algunos renglones):\n\ncalculos <- read_csv(\"./datos/kidney_stone_data.csv\")\nnames(calculos) <- c(\"tratamiento\", \"tamaño\", \"éxito\")\ncalculos <- calculos |> \n   mutate(tamaño = ifelse(tamaño == \"large\", \"grandes\", \"chicos\")) |> \n   mutate(resultado = ifelse(éxito == 1, \"mejora\", \"sin_mejora\")) |> \n   select(tratamiento, tamaño, resultado)\nnrow(calculos)\n\n[1] 700\n\ncalculos |> \n   sample_n(20) |> \n   kable()\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n  \n \n\n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    sin_mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    sin_mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    B \n    grandes \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n\n\n\n\n\nAunque estos datos contienen información de 700 pacientes (cada renglón es un paciente), los datos pueden resumirse sin pérdida de información contando como sigue:\n\ncalculos_agregada <- calculos |> \n   group_by(tratamiento, tamaño, resultado) |> \n   count()\ncalculos_agregada |> kable()\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n    n \n  \n \n\n  \n    A \n    chicos \n    mejora \n    81 \n  \n  \n    A \n    chicos \n    sin_mejora \n    6 \n  \n  \n    A \n    grandes \n    mejora \n    192 \n  \n  \n    A \n    grandes \n    sin_mejora \n    71 \n  \n  \n    B \n    chicos \n    mejora \n    234 \n  \n  \n    B \n    chicos \n    sin_mejora \n    36 \n  \n  \n    B \n    grandes \n    mejora \n    55 \n  \n  \n    B \n    grandes \n    sin_mejora \n    25 \n  \n\n\n\n\n\nEste resumen no es muy informativo, pero al menos vemos qué valores aparecen en cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:\n\ncalculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, tamaño, total, prop_mejora) |> \n   arrange(tamaño) |> \n   kable()\n\n\n\n \n  \n    tratamiento \n    tamaño \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    chicos \n    87 \n    0.93 \n  \n  \n    B \n    chicos \n    270 \n    0.87 \n  \n  \n    A \n    grandes \n    263 \n    0.73 \n  \n  \n    B \n    grandes \n    80 \n    0.69 \n  \n\n\n\n\n\nEsta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Sin embargo, esta tabla es apropiada para empezar a contestar la pregunta:\n\n¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?\n\nSupongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:\n\ncalculos |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable()\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\ny parece ser que el tratamiento \\(B\\) es mejor que el \\(A\\). Esta es una paradoja (un ejemplo de la paradoja de Simpson) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar \\(B\\)? ¿Si sabe debería recetar \\(A\\)? Esta discusión parece no tener mucho sentido.\nPodemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:\n\ncalculos |> group_by(tratamiento, tamaño) |> count() |> \n   kable()\n\n\n\n \n  \n    tratamiento \n    tamaño \n    n \n  \n \n\n  \n    A \n    chicos \n    87 \n  \n  \n    A \n    grandes \n    263 \n  \n  \n    B \n    chicos \n    270 \n  \n  \n    B \n    grandes \n    80 \n  \n\n\n\n\n\nNuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento \\(A\\) para cálculos grandes, y \\(B\\) para cálculos chicos. Esto quiere decir que en la tabla total el tratamiento \\(A\\) está en desventaja porque se usa en casos más difíciles, pero el tratamiento \\(A\\) parece ser en general mejor.\nIgual que en el ejemplo anterior, los resúmenes descriptivos están acompañados de hipótesis acerca del proceso generador de datos, y esto ilumina lo que estamos observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos."
  },
  {
    "objectID": "analisis-exp-1.html#inferencia-y-predicción",
    "href": "analisis-exp-1.html#inferencia-y-predicción",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "1.2 Inferencia y predicción",
    "text": "1.2 Inferencia y predicción\nEn los ejemplos anteriores, sólo vimos muestras de datos (algunos pacientes, algunas fechas). Nuestras descripciones son, estrictamente hablando, válidas para esa muestra de los datos.\nSi quisiéramos generalizar a la población de pacientes con cálculos (quizá en nuestra muestra el tratamiento A parece mejor, pero ¿qué podemos decir para la población de pacientes), o quisiéramos predecir cómo van a ser los nacimientos en 2021, requerimos otro tipo de análisis: inferencial y predictivo. Estos dos tipos de análisis, centrales en la estadística, buscan establecer condiciones para poder generalizar de nuestra muestra a datos no observados (otros pacientes, nacimientos en el futuro), y cuantificar qué tan bien o mal podemos hacerlo.\nPara llegar a este tipo de análisis, generalmente tenemos que comenzar con el análisis exploratorio, y con la comprensión de los fundamentos del proceso generador asociado a nuestros datos. En algunos casos, veremos que es posible usar herramientas matemáticas para modelar aspectos de nuestro proceso generador de datos, que cuando válidas, nos permiten generalizar y ampliar apropiadamente el rango de nuestras conclusiones.\nLa herramienta básica para construir, entender y operar con estos modelos es la teoría de probabilidad, que veremos más adelante."
  },
  {
    "objectID": "analisis-exp-1.html#ejercicio-admisiones-de-berkeley",
    "href": "analisis-exp-1.html#ejercicio-admisiones-de-berkeley",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "Ejercicio: admisiones de Berkeley",
    "text": "Ejercicio: admisiones de Berkeley\nConsideramos ahora los siguientes datos de admisión a distintos departamentos de Berkeley en 1975:\n\ndata(\"UCBAdmissions\")\nadm_original <- UCBAdmissions |> as_tibble() |> \n   pivot_wider(names_from = Admit, values_from = n) \nadm_original |> knitr::kable()\n\n\n\n \n  \n    Gender \n    Dept \n    Admitted \n    Rejected \n  \n \n\n  \n    Male \n    A \n    512 \n    313 \n  \n  \n    Female \n    A \n    89 \n    19 \n  \n  \n    Male \n    B \n    353 \n    207 \n  \n  \n    Female \n    B \n    17 \n    8 \n  \n  \n    Male \n    C \n    120 \n    205 \n  \n  \n    Female \n    C \n    202 \n    391 \n  \n  \n    Male \n    D \n    138 \n    279 \n  \n  \n    Female \n    D \n    131 \n    244 \n  \n  \n    Male \n    E \n    53 \n    138 \n  \n  \n    Female \n    E \n    94 \n    299 \n  \n  \n    Male \n    F \n    22 \n    351 \n  \n  \n    Female \n    F \n    24 \n    317 \n  \n\n\n\n\n\nCon algo de manipulación podemos ver tasas de admisión para Male y Female, y los totales de cada grupo que solicitaron en cada Departamento.\n\nadm_tbl <- adm_original |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected), 2), total = Admitted + Rejected) |> \n   select(Gender, Dept, prop_adm, total) |> \n   pivot_wider(names_from = Gender, values_from = prop_adm:total)\nadm_tbl |> knitr::kable()\n\n\n\n \n  \n    Dept \n    prop_adm_Male \n    prop_adm_Female \n    total_Male \n    total_Female \n  \n \n\n  \n    A \n    0.62 \n    0.82 \n    825 \n    108 \n  \n  \n    B \n    0.63 \n    0.68 \n    560 \n    25 \n  \n  \n    C \n    0.37 \n    0.34 \n    325 \n    593 \n  \n  \n    D \n    0.33 \n    0.35 \n    417 \n    375 \n  \n  \n    E \n    0.28 \n    0.24 \n    191 \n    393 \n  \n  \n    F \n    0.06 \n    0.07 \n    373 \n    341 \n  \n\n\n\n\n\nY complementamos con las tasas de aceptación a total por género, y tasas de aceptación por departamento:\n\nadm_original |> group_by(Gender) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n \n  \n    Gender \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    Female \n    557 \n    1278 \n    0.30 \n  \n  \n    Male \n    1198 \n    1493 \n    0.45 \n  \n\n\n\n\n\n\nadm_original |> group_by(Dept) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n \n  \n    Dept \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    A \n    601 \n    332 \n    0.64 \n  \n  \n    B \n    370 \n    215 \n    0.63 \n  \n  \n    C \n    322 \n    596 \n    0.35 \n  \n  \n    D \n    269 \n    523 \n    0.34 \n  \n  \n    E \n    147 \n    437 \n    0.25 \n  \n  \n    F \n    46 \n    668 \n    0.06 \n  \n\n\n\n\n\n\n¿Qué observas acerca de las tasas de admisión en cada departamento, diferenciadas por género? ¿Qué tiene qué ver con el número de personas que solicitan en cada departamento?\nEsta es una tabla descriptiva. Sin embargo, tiene que ser entendida en el contexto de los datos y su generación. ¿Qué hipótesis importantes sugieren estos datos? ¿Por qué hay tanta diferencia de género de solicitudes en algunos departamentos? ¿Por qué es sorprendente o no las variaciones en tasas de aceptación de estudiantes de cada género?"
  },
  {
    "objectID": "analisis-exp-1.html#formulación-de-preguntas",
    "href": "analisis-exp-1.html#formulación-de-preguntas",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "1.3 Formulación de preguntas",
    "text": "1.3 Formulación de preguntas\nEl análisis exploratorio es fundamentalmente un proceso creativo, lo que lo convierte en difícil de enseñar en las clases de ciencias de datos. Como la mayoría de los procesos creativos, la clave es desarrollar conocimiento detallado del dominio para hacer preguntas de calidad.\nEs difícil hacer preguntas reveladoras al comienzo del análisis porque no se sabe qué conocimientos están contenidos en el conjunto de datos, pero cada nueva pregunta expone aspectos de los datos y aumenta las posibilidades de hacer un descubrimiento. Muchos de los descubrimientos ocurren en el análisis exploratorio, por lo que no debe pasarse por alto, es esencial para descubrir información que se puede utilizar para responder a la pregunta de investigación planteada.\nA veces, al final de un análisis de datos exploratorio, la conclusión es que el conjunto de datos no es realmente apropiado para esta pregunta. En otros casos, no está claro que la pregunta que intentamos responder tenga relevancia inmediata. Uno de los objetivos del análisis exploratorio de datos es hacer que pensemos en estas posibilidades, en este punto, podemos refinar nuestra pregunta o recopilar nuevos datos, todo en un proceso iterativo para llegar al mejor resultado.\nExisten algunas prácticas generales que utilizamos para hacer validaciones y resúmenes simples de los datos que discutiremos más adelante. Por el momento, discutimos las razones por las que estamos haciendo ese análisis en un principio.\nEn general, comenzamos con algunas preguntas básicas que quisiéramos contestar con los datos. El análisis exploratorio juega un papel central para comenzar a responder:\n\n¿Es razonable la pregunta que queremos contestar?\n¿Podemos contestar la pregunta con los datos que tenemos?\n\nAunque estos dos incisos a veces parecen transparentes y simples de contestar, generalmente no lo son: las preguntas que queremos contestar y los problemas que queremos resolver usualmente son no triviales.\nEl proceso de la ciencia de datos no va desde las preguntas hasta las respuestas en un camino lineal.\nEn esta gráfica Roger Peng hay tres caminos: uno es uno ideal que pocas veces sucede, otro produce respuestas poco útiles pero es fácil, y otro es tortuoso pero que caracteriza el mejor trabajo de análisis de datos:\n\n\n\n\n\nAdaptado de R. Peng: Tukey, design thinking and better questions\n\n\n\n\nEl proceso típico involucra pasos como los siguientes, y es un proceso no lineal:\n\nHacer preguntas de la materia que nos interesa\nRecolectar, consumir y procesar los datos para abordarla\nExplorar estos datos y evaluar su calidad\nHacer análisis o modelos\nReportar los resultados de forma adecuada y con esto resolver y replantear las preguntas importantes.\n\nPor ejemplo, evaluar la calidad de los datos puede llevar a replantear la necesidad de obtener más información o de hacer estudios específicos. Así también, los modelos pueden dar luz sobre las preguntas que los originan.\n\n\n\n¿Por dónde empezar el análisis descriptivo y exploratorio? ¿Cómo sabemos que vamos por buen camino y qué hacer cuando sentimos que nos estancamos?"
  },
  {
    "objectID": "analisis-exp-1.html#cómo-saber-que-vamos-en-el-camino-correcto",
    "href": "analisis-exp-1.html#cómo-saber-que-vamos-en-el-camino-correcto",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "1.4 ¿Cómo saber que vamos en el camino correcto?",
    "text": "1.4 ¿Cómo saber que vamos en el camino correcto?\nComenzamos por discribir cuáles son los signos de calidad del análisis que piensa usarse como insumo para una decisión. Los principios del diseño analítico de Edward Tufte (Tufte (2006)) son:\nLos análisis exitosos:\n\nMuestran y explotan comparaciones, diferencias y variación.\nTienden a ser multivariados: estudian conjuntamente más de 1 o 2 variables.\nMuestran y explotan estructura sistemática, sugieren explicaciones. Cuando es posible, aportan evidencia de causalidad.\n\nTambién muy importantes pero en los que pondremos menos énfasis:\n\nDatos y procesos están bien documentados. El análisis es reproducible y transparente.\nIntentan integrar la evidencia completa: texto, explicaciones, tablas y gráficas.\n\nY finalmente, el principio general:\n\nLa calidad, relevancia, e integridad del contenido y los datos son los que al final sostienen al análisis - por sí mismos, el uso de técnicas sofisticadas, algoritmos novedosos, uso o no de grandes datos, estilo de visualizaciones o presentaciones no son marcas o sellos de un análisis de datos exitoso.\n\n\n\n\n\n\n\nTip\n\n\n\nEvaluar un análisis o resultado en estos seis puntos generalmente ayuda en el proceso de refinamiento de preguntas y respuestas."
  },
  {
    "objectID": "analisis-exp-1.html#gráfica-de-minard",
    "href": "analisis-exp-1.html#gráfica-de-minard",
    "title": "1  Análisis exploratorio y visualización de datos",
    "section": "1.5 Gráfica de Minard",
    "text": "1.5 Gráfica de Minard\nLa ilustración que Tufte usa para mostrar excelencia en diseño analítico es una gráfica de Minard que sirve para entender la campaña de Napoleón (1812) en Rusia. Es un ejemplo atípico, pero representa bien los principios y también muestra la importancia del ingenio en la construcción de un anállsis:\n\n\n\n\n\nMarcha de Napoleón de Charles Minard. Tomado de Wikipedia\n\n\n\n\n\n\n\n¿Cómo satisface los principios del diseño analítico este gráfico?\n\n\nLa gráfica de Minard da al espectador la mayor cantidad de ideas en el menor tiempo, con la menor cantidad de tinta, y en el espacio más pequeño. Combina muchas dimensiones: pérdida de vidas en un momento y lugar, temperatura, geografía, contexto histórico. Por ejemplo, muestra los puntos donde las tropas de Napoleón se dividen en subgrupos dividiendo la barra principal en ramas. Agrega líneas finas para representar los cruces de ríos en el viaje de regreso que diezmaron aún más las tropas decrecientes de Napoleón y es capaz de mostrar la drástica pérdida de vidas por la decisión de Napoleón en una sola esquina del diagrama.\n\n\n\n\nJulious, Steven A, y Mark A Mullee. 1994. «Confounding and Simpson’s paradox». BMJ 309 (6967): 1480-81. https://doi.org/10.1136/bmj.309.6967.1480.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Cheshire, CT: Graphics Press.\n\n\nTukey, John W. 1980. «We Need Both Exploratory and Confirmatory». The American Statistician 34 (1): 23-25. http://www.jstor.org/stable/2682991."
  },
  {
    "objectID": "analisis-exp-categoricos.html",
    "href": "analisis-exp-categoricos.html",
    "title": "2  Datos univariados categóricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones individuales. Consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "analisis-exp-categoricos.html#datos-categóricos-y-tablas",
    "href": "analisis-exp-categoricos.html#datos-categóricos-y-tablas",
    "title": "2  Datos univariados categóricos",
    "section": "2.1 Datos categóricos y tablas",
    "text": "2.1 Datos categóricos y tablas\nUna medición categórica es una que toma sus valores posibles en un conjunto que no es numérico. Consideremos los siguiente datos de 300 tomadores de té (Lê, Josse, y Husson (2008)):\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\n\n\n# cargamos y traducimos los datos\nte_tbl <- read_csv(\"./datos/tea.csv\") |> \n   mutate(id = row_number()) |> \n   select(id, Tea, How, sugar, how, price, age) |> \n   rename(tipo = Tea, complementos = How, azucar = sugar, \n          presentacion = how, precio = price, edad = age) |> \n   mutate(tipo = recode(tipo, black = \"negro\", green = \"verde\", `Earl Grey` = \"earl_grey\"),\n          complementos = recode(complementos, alone = \"solo\", milk = \"leche\", \n                                lemon = \"limón\", .default = \"otros\"),\n          azucar = recode(azucar, sugar = \"con_azúcar\", No.sugar = \"sin_azúcar\"),\n          presentacion = recode(presentacion, `tea bag`=\"bolsa\", \n                                unpackaged = \"suelto\", .default = \"mixto\"),\n          precio = recode(precio, p_upscale = \"fino\", p_branded = \"de_marca\",\n                          p_private_label = \"marca_propia\", p_variable = \"variable\",\n                          .default = \"no_sabe\"))\nsample_n(te_tbl, 10) |> kable()\n\n\n\n \n  \n    id \n    tipo \n    complementos \n    azucar \n    presentacion \n    precio \n    edad \n  \n \n\n  \n    282 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    variable \n    22 \n  \n  \n    139 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    de_marca \n    17 \n  \n  \n    281 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    variable \n    21 \n  \n  \n    219 \n    earl_grey \n    leche \n    con_azúcar \n    bolsa \n    de_marca \n    22 \n  \n  \n    208 \n    negro \n    limón \n    con_azúcar \n    suelto \n    fino \n    61 \n  \n  \n    2 \n    negro \n    leche \n    sin_azúcar \n    bolsa \n    variable \n    45 \n  \n  \n    54 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    fino \n    18 \n  \n  \n    89 \n    earl_grey \n    leche \n    sin_azúcar \n    bolsa \n    variable \n    51 \n  \n  \n    287 \n    earl_grey \n    leche \n    con_azúcar \n    suelto \n    variable \n    47 \n  \n  \n    245 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    21 \n  \n\n\n\n\n\nMediciones como tipo, presentación o azucar son variables categóricas. Desde el punto de vista univariado, generalmente no es necesario resumir, sino simplemente agrupar y contar cuántas veces ocurre cada categoría. Por ejemplo\n\ntabla_1 <- te_tbl |> count(tipo) |> \n   arrange(desc(n))\ntabla_1 |> kable()\n\n\n\n \n  \n    tipo \n    n \n  \n \n\n  \n    earl_grey \n    193 \n  \n  \n    negro \n    74 \n  \n  \n    verde \n    33 \n  \n\n\n\n\n\nUsualmente es más útil reportar la porporción o porcentaje de casos por categoría\n\ntabla_2 <- te_tbl |> \n   count(tipo) |> \n   mutate(n_total = sum(n), prop = n / n_total) |> \n   select(tipo, n_total, prop) |> \n   mutate(across(where(is.numeric), round, 2)) |> \n   arrange(desc(prop))\ntabla_2 |> kable()\n\n\n\n \n  \n    tipo \n    n_total \n    prop \n  \n \n\n  \n    earl_grey \n    300 \n    0.64 \n  \n  \n    negro \n    300 \n    0.25 \n  \n  \n    verde \n    300 \n    0.11 \n  \n\n\n\n\n\nPodemos hacer varias variables juntas de la siguiente manera:\n\nperfiles_col_tbl <- te_tbl |> select(id, tipo, complementos, presentacion, azucar) |> \n   pivot_longer(cols = tipo:azucar, names_to = \"variable\", values_to = \"valor\") |> \n   count(variable, valor) |> \n   group_by(variable) |> \n   mutate(n_total = sum(n), prop = n / n_total) |>\n   mutate(prop = round(prop, 2)) |> \n   arrange(desc(prop), .by_group = TRUE)\nperfiles_col_tbl |> kable()\n\n\n\n \n  \n    variable \n    valor \n    n \n    n_total \n    prop \n  \n \n\n  \n    azucar \n    sin_azúcar \n    155 \n    300 \n    0.52 \n  \n  \n    azucar \n    con_azúcar \n    145 \n    300 \n    0.48 \n  \n  \n    complementos \n    solo \n    195 \n    300 \n    0.65 \n  \n  \n    complementos \n    leche \n    63 \n    300 \n    0.21 \n  \n  \n    complementos \n    limón \n    33 \n    300 \n    0.11 \n  \n  \n    complementos \n    otros \n    9 \n    300 \n    0.03 \n  \n  \n    presentacion \n    bolsa \n    170 \n    300 \n    0.57 \n  \n  \n    presentacion \n    mixto \n    94 \n    300 \n    0.31 \n  \n  \n    presentacion \n    suelto \n    36 \n    300 \n    0.12 \n  \n  \n    tipo \n    earl_grey \n    193 \n    300 \n    0.64 \n  \n  \n    tipo \n    negro \n    74 \n    300 \n    0.25 \n  \n  \n    tipo \n    verde \n    33 \n    300 \n    0.11 \n  \n\n\n\n\n\nPara leer más fácil, imprimimos individualmente estas tablas, o hacemos algo como lo que sigue para mostrarlas todas juntas:\n\nperfiles_col_tbl |> \n   ungroup() |> \n   select(-variable, -n_total) |> \n   kable() |>  \n   pack_rows(index = table(perfiles_col_tbl$variable))\n\n\n\n \n  \n    valor \n    n \n    prop \n  \n \n\n  azucar\n\n    sin_azúcar \n    155 \n    0.52 \n  \n  \n    con_azúcar \n    145 \n    0.48 \n  \n  complementos\n\n    solo \n    195 \n    0.65 \n  \n  \n    leche \n    63 \n    0.21 \n  \n  \n    limón \n    33 \n    0.11 \n  \n  \n    otros \n    9 \n    0.03 \n  \n  presentacion\n\n    bolsa \n    170 \n    0.57 \n  \n  \n    mixto \n    94 \n    0.31 \n  \n  \n    suelto \n    36 \n    0.12 \n  \n  tipo\n\n    earl_grey \n    193 \n    0.64 \n  \n  \n    negro \n    74 \n    0.25 \n  \n  \n    verde \n    33 \n    0.11"
  },
  {
    "objectID": "analisis-exp-categoricos.html#comparando-grupos-con-variables-categóricas",
    "href": "analisis-exp-categoricos.html#comparando-grupos-con-variables-categóricas",
    "title": "2  Datos univariados categóricos",
    "section": "2.2 Comparando grupos con variables categóricas",
    "text": "2.2 Comparando grupos con variables categóricas\nEste análisis generalmente es más interesante cuando comparamos grupos. Supongamos que nos interesa ver si existe una relación entre usar el tipo de té que toman estas personas y el uso de complementos como leche o limón. Podríamos entonces dividir los datos según el uso de azúcar y repetir para cada grupo las tablas mostradas arriba:\n\nperfiles_col_tbl <- te_tbl |> count(complementos, tipo) |> \n   group_by(tipo) |> \n   mutate(prop = n / sum(n)) |>\n   group_by(complementos) |> \n   select(-n) |> \n   pivot_wider(names_from = tipo, values_from = prop, values_fill = 0)\nperfiles_col_tbl |>  kable(digits = 2, caption = \"Perfiles por columna\")\n\n\n\nPerfiles por columna\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n  \n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n  \n\n\n\n\n\nComparando los perfiles de las columnas observamos variaciones interesantes: por ejemplo, los tomadores de Earl Grey tienden a usar más limón como complemento que otros grupos. Son resúmenes univariados que ahora comparamos a lo largo de grupos. Podemos hacer las comparaciones más simples si hacemos todas contra una columna marginal del uso general en la muestra de los distintos complementos_\n\ncomp_tbl <- te_tbl |> count(complementos) |> mutate(total = n / sum(n))\nperfiles_col_tbl <- left_join(perfiles_col_tbl, comp_tbl) |> \n      arrange(desc(total)) |> \n      select(-n)\n\nJoining, by = \"complementos\"\n\nperfiles_col_tbl |> kable(digits = 2)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n    0.65 \n  \n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n    0.21 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n    0.11 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n    0.03 \n  \n\n\n\n\n\nEn este punto, vemos que hay coincidencias y diferencias entre los grupos de tomadores de té. Podemos expresar esto de manera simple calculando índices contra la columna de total:\n\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric), ~ .x / total)) |> \n   select(-total)\nres_tbl |> kable(digits = 2)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    solo \n    1.02 \n    0.87 \n    1.17 \n  \n  \n    leche \n    0.94 \n    1.22 \n    0.87 \n  \n  \n    limón \n    1.13 \n    0.86 \n    0.55 \n  \n  \n    otros \n    0.52 \n    2.70 \n    0.00 \n  \n\n\n\n\n\nValores por encima de 1 indican columnas por arriba de la población general, y análogamente para valores por debajo de uno. Estas cantidades pueden escribirse en términos porcentuales, o se les puede restar 1 para terminar como una variación porcentual del promedio. A estas cantidades se les llama residuales crudos:\n\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) \nres_tbl |> kable(digits = 2)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.70 \n    -1.00 \n    0.03 \n  \n\n\n\n\n\nPodemos finalmente marcar la tabla:\n\nres_tbl |>  mutate(across(where(is.numeric), round, 2)) |> \n   mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.1, \"black\", \n                                         ifelse(.x < -0.1, \"red\", \"gray\"))))) |>\n   arrange(desc(total)) |> \n   kable(escape = FALSE) \n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03 \n  \n\n\n\n\n\n\n\n\n\n\n\nPrefiles\n\n\n\nA este tipo de análisis de tablas cruzadas a veces se le llama análisis de perfiles columna. Nos permite entender cómo varía la distribución de la variable de los renglones según el grupo indicado por la columna. - Desviaciones grandes en los residuales indican asociaciones fuertes entre la variable de los reglones y de las columnas - Recordemos que este análisis aplica a la muestra de datos que tenemos. Columnas con pocos individuos tienden a mostrar más variación y debemos ser cuidadosos al generalizar.\n\n\nPodemos incluir también totales para ayudarnos a juzgar las variaciones:\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n     \n    193 \n    74 \n    33 \n    1.00 \n  \n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03"
  },
  {
    "objectID": "analisis-exp-categoricos.html#observación-perfiles-renglón-y-columna",
    "href": "analisis-exp-categoricos.html#observación-perfiles-renglón-y-columna",
    "title": "2  Datos univariados categóricos",
    "section": "2.3 Observación: perfiles renglón y columna",
    "text": "2.3 Observación: perfiles renglón y columna\nEl análisis también lo podemos hacer con los perfiles de los renglones. Los residuales crudos que usamos para interpretar son los mismos. La razón es la siguiente:\nPara los perfiles columna, si escribimos \\(n_{+j}\\) como los totales por columna, y \\(n_{i+}\\) los totales por renglón, tenemos que los perfiles columna son:\n\\[c_{i,j} = \\frac{n_{i,j}}{n_{+j}}\\]\nEscribimos también \\(c_i = \\frac{n_{i+}}{n}\\) y \\(r_j = \\frac{n_{+j}}{n}\\) como los porcentajes marginales por columna y por renglón respectivamente.\nLos residuales son entonces\n\\[r_{i,j} = \\frac{\\frac{n_{i,j}}{n_{+,j}}} { \\frac{n_{i,+}}{n}} - 1 = \\frac{p_{i,j} - r_ic_j}{r_ic_j}\\]\nNótese que no importa entonces cómo comencemos el cálculo, por renglones o por columnas, el resultado es el mismo.\n\nDiscute qué sentido tiene comparar \\(p_{i,j}\\) contra \\(r_ic_j\\). ¿Qué interpretación tiene esta última cantidad?\n\nEn algunos casos se utilizan residuales estandarizados para hacer el análisis, que están dados por\n\\[ \\frac{p_{i,j} - r_ic_j}{\\sqrt{r_ic_j}}\\]\nVeremos más adelante cuál es la razón de esto: tiene que ver con inferencia y variabilidad muestral de perfiles y residuales, aunque el análisis básico que presentamos arriba generalmente es suficiente para extraer de manera clara patrones importantes en los datos."
  },
  {
    "objectID": "analisis-exp-categoricos.html#visualización-de-tablas-cruzadas",
    "href": "analisis-exp-categoricos.html#visualización-de-tablas-cruzadas",
    "title": "2  Datos univariados categóricos",
    "section": "2.4 Visualización de tablas cruzadas",
    "text": "2.4 Visualización de tablas cruzadas\nPara tablas más grandes, muchas veces las técnicas que mostramos arriba no son suficientes para entender y presentar patrones importantes en los datos. En estos casos, buscamos reducir la dimensionalidad de los datos para poder presentarlos en una gráfica de dos dimensiones.\nPodemos utilizar análisis de correspondencias. A grandes rasgos (ver (Izenman 2009) para los detalles) buscamos una representación tal que:\n\nCada categoría de las columnas está representada por una flecha que sale del origen de nuestra gráfica\nCada categoría de los renglones está representada por un punto en nuestra gráfica\nSi proyectamos los puntos (renglones) sobre las direcciones de las columnas, entonces el tamaño de la proyección es lo más cercano posible a el residual correspondiente de las tablas del análisis mostrado arriba.\n\nPara construir esta gráfica, entonces, existe un proceso de optimización que busca representar lo más fielmente los residuales del análisis mostrado arriba en dos dimensiones, y de esta forma buscamos recuperar una buena parte de la información de los residuales de una manera más compacta."
  },
  {
    "objectID": "analisis-exp-categoricos.html#ejemplo-tés-y-complementos",
    "href": "analisis-exp-categoricos.html#ejemplo-tés-y-complementos",
    "title": "2  Datos univariados categóricos",
    "section": "2.5 Ejemplo: tés y complementos",
    "text": "2.5 Ejemplo: tés y complementos\n\nlibrary(ca)\ncorr_te <- ca(table(te_tbl$complementos, te_tbl$tipo))\nplot(corr_te, map = \"rowgreen\", arrows = c(FALSE, TRUE))\n\n\n\n\nLa contribución de cada dimensión a la aproximación se indica en los ejes. Como vemos en la gráfica, y la suma de las contribuciones nos da la calidad de la representación, que en este caso es perfecta.\n::: {.cell type=‘comentario’}\n\n\nEl análisis de correspondencias es un tema relativamente avanzado de estadística multivariada, y su definición precisa requiere de matemáticas más avanzadas (por ejemplo la descomposición en valores singulares).\nCualquier hallazgo obtenido en este tipo de análisis debe ser verificado en las tablas correspondientes de perfiles\nHay distintos tipos de gráficas (biplots) asociadas al análisis de correspondencias, que privilegian representar mejor a distintos tipos de características de los datos\n\n:::"
  },
  {
    "objectID": "analisis-exp-categoricos.html#ejemplo-robo-en-tiendas",
    "href": "analisis-exp-categoricos.html#ejemplo-robo-en-tiendas",
    "title": "2  Datos univariados categóricos",
    "section": "2.6 Ejemplo: robo en tiendas",
    "text": "2.6 Ejemplo: robo en tiendas\nConsideramos los siguientes datos de robos en tiendas en Holanda por personas de distintas edades y genéros (Izenman (2009)). En este caso, las variables ya están cruzadas:\n\nhurto_tbl <- read_csv(\"./datos/hurto.csv\") |> \n   mutate(grupo = ifelse(grupo == \"-12 h\", \"01-12 h\", grupo),\n          grupo = ifelse(grupo == \"-12 m\", \"01-12 m\", grupo))\n\nRows: 18 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): grupo\ndbl (13): ropa, accesorios, tabaco, escritura, libros, discos, bienes, dulce...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhurto_tbl |> kable()\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    discos \n    bienes \n    dulces \n    juguetes \n    joyería \n    perfumes \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    81 \n    66 \n    150 \n    667 \n    67 \n    24 \n    47 \n    430 \n    743 \n    132 \n    32 \n    197 \n    209 \n  \n  \n    12-14 h \n    138 \n    204 \n    340 \n    1409 \n    259 \n    272 \n    117 \n    637 \n    684 \n    408 \n    57 \n    547 \n    550 \n  \n  \n    15-17 h \n    304 \n    193 \n    229 \n    527 \n    258 \n    368 \n    98 \n    246 \n    116 \n    298 \n    61 \n    402 \n    454 \n  \n  \n    18-20 h \n    384 \n    149 \n    151 \n    84 \n    146 \n    141 \n    61 \n    40 \n    13 \n    71 \n    52 \n    138 \n    252 \n  \n  \n    21-29 h \n    942 \n    297 \n    313 \n    92 \n    251 \n    167 \n    193 \n    30 \n    16 \n    130 \n    111 \n    280 \n    624 \n  \n  \n    30-39 h \n    359 \n    109 \n    136 \n    36 \n    96 \n    67 \n    75 \n    11 \n    16 \n    31 \n    54 \n    200 \n    195 \n  \n  \n    40-49 h \n    178 \n    53 \n    121 \n    36 \n    48 \n    29 \n    50 \n    5 \n    6 \n    14 \n    41 \n    152 \n    88 \n  \n  \n    50-64 h \n    137 \n    68 \n    171 \n    37 \n    56 \n    27 \n    55 \n    17 \n    3 \n    11 \n    50 \n    211 \n    90 \n  \n  \n    64+ h \n    45 \n    28 \n    145 \n    17 \n    41 \n    7 \n    29 \n    28 \n    8 \n    10 \n    28 \n    111 \n    34 \n  \n  \n    01-12 m \n    71 \n    19 \n    59 \n    224 \n    19 \n    7 \n    22 \n    137 \n    113 \n    162 \n    70 \n    15 \n    24 \n  \n  \n    12-14 m \n    241 \n    98 \n    111 \n    346 \n    60 \n    32 \n    29 \n    240 \n    98 \n    548 \n    178 \n    29 \n    58 \n  \n  \n    15-17 m \n    477 \n    114 \n    58 \n    91 \n    50 \n    27 \n    41 \n    80 \n    14 \n    303 \n    141 \n    9 \n    72 \n  \n  \n    18-20 m \n    436 \n    108 \n    76 \n    18 \n    32 \n    12 \n    32 \n    12 \n    10 \n    74 \n    70 \n    14 \n    67 \n  \n  \n    21-29 m \n    1180 \n    207 \n    132 \n    30 \n    61 \n    21 \n    65 \n    16 \n    12 \n    100 \n    104 \n    30 \n    157 \n  \n  \n    30-39 m \n    1009 \n    165 \n    121 \n    27 \n    43 \n    9 \n    74 \n    14 \n    31 \n    48 \n    81 \n    36 \n    107 \n  \n  \n    40-49 m \n    517 \n    102 \n    93 \n    23 \n    31 \n    7 \n    51 \n    10 \n    8 \n    22 \n    46 \n    24 \n    66 \n  \n  \n    50-64 m \n    488 \n    127 \n    214 \n    27 \n    57 \n    13 \n    79 \n    23 \n    17 \n    26 \n    69 \n    35 \n    64 \n  \n  \n    64+ m \n    173 \n    64 \n    215 \n    13 \n    44 \n    0 \n    39 \n    42 \n    6 \n    12 \n    41 \n    11 \n    55 \n  \n\n\n\n\n\nEsta tabla es más grande y difícil de entender tal cual está. Comenzamos por examinar las marginales:\n\nhurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop)) |> \n   kable(digits = 2)\n\n\n\n \n  \n    producto \n    n \n    prop \n  \n \n\n  \n    ropa \n    7160 \n    0.22 \n  \n  \n    escritura \n    3704 \n    0.11 \n  \n  \n    otros \n    3166 \n    0.10 \n  \n  \n    tabaco \n    2835 \n    0.09 \n  \n  \n    herramientas \n    2441 \n    0.07 \n  \n  \n    joyería \n    2400 \n    0.07 \n  \n  \n    accesorios \n    2171 \n    0.07 \n  \n  \n    dulces \n    2018 \n    0.06 \n  \n  \n    juguetes \n    1914 \n    0.06 \n  \n  \n    libros \n    1619 \n    0.05 \n  \n  \n    perfumes \n    1286 \n    0.04 \n  \n  \n    discos \n    1230 \n    0.04 \n  \n  \n    bienes \n    1157 \n    0.03 \n  \n\n\n\n\n\n\ngrupos_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(grupo) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop))\ngrupos_tbl |> kable(digits = 2)\n\n\n\n \n  \n    grupo \n    n \n    prop \n  \n \n\n  \n    12-14 h \n    5622 \n    0.17 \n  \n  \n    15-17 h \n    3554 \n    0.11 \n  \n  \n    21-29 h \n    3446 \n    0.10 \n  \n  \n    01-12 h \n    2845 \n    0.09 \n  \n  \n    21-29 m \n    2115 \n    0.06 \n  \n  \n    12-14 m \n    2068 \n    0.06 \n  \n  \n    30-39 m \n    1765 \n    0.05 \n  \n  \n    18-20 h \n    1682 \n    0.05 \n  \n  \n    15-17 m \n    1477 \n    0.04 \n  \n  \n    30-39 h \n    1385 \n    0.04 \n  \n  \n    50-64 m \n    1239 \n    0.04 \n  \n  \n    40-49 m \n    1000 \n    0.03 \n  \n  \n    18-20 m \n    961 \n    0.03 \n  \n  \n    01-12 m \n    942 \n    0.03 \n  \n  \n    50-64 h \n    933 \n    0.03 \n  \n  \n    40-49 h \n    821 \n    0.02 \n  \n  \n    64+ m \n    715 \n    0.02 \n  \n  \n    64+ h \n    531 \n    0.02 \n  \n\n\n\n\n\nIntentamos análisis de correspondencias para comparar los perfiles columna:\n\nhurto_df <- as.data.frame(hurto_tbl)\nrownames(hurto_df) <- hurto_tbl$grupo\nhurto_df$grupo <- NULL\ncorr_hurto <- ca(hurto_df)\ngrafica_datos <- plot(corr_hurto, map = \"rowgreen\", arrows = c(FALSE, TRUE))\n\n\n\n\n\nSegún esta gráfica, ¿qué categorias de productos están sobrerrepresentadas en cada grupo de edad? ¿Cómo tendrían que verse el análisis de perfiles columna?\n\nComo se aprecia, en la siguiente tabla, es difícil entender los patrones generales en los datos. Quitamos algunas columnas para imprimir más fácilmente\n\nperfiles_hurto_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   mutate(prop = n / sum(n)) |> \n   select(-n) |> \n   pivot_wider(names_from = producto, values_from = prop) \nperfiles_hurto_tbl |> \n   select(-bienes, -discos, -perfumes) |> \n   kable(digits = 2) |> \n   kable_styling(font_size = 10)\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    0.01 \n    0.03 \n    0.05 \n    0.18 \n    0.04 \n    0.21 \n    0.39 \n    0.06 \n    0.08 \n    0.07 \n  \n  \n    12-14 h \n    0.02 \n    0.09 \n    0.12 \n    0.38 \n    0.16 \n    0.32 \n    0.36 \n    0.17 \n    0.22 \n    0.17 \n  \n  \n    15-17 h \n    0.04 \n    0.09 \n    0.08 \n    0.14 \n    0.16 \n    0.12 \n    0.06 \n    0.12 \n    0.16 \n    0.14 \n  \n  \n    18-20 h \n    0.05 \n    0.07 \n    0.05 \n    0.02 \n    0.09 \n    0.02 \n    0.01 \n    0.03 \n    0.06 \n    0.08 \n  \n  \n    21-29 h \n    0.13 \n    0.14 \n    0.11 \n    0.02 \n    0.16 \n    0.01 \n    0.01 \n    0.05 \n    0.11 \n    0.20 \n  \n  \n    30-39 h \n    0.05 \n    0.05 \n    0.05 \n    0.01 \n    0.06 \n    0.01 \n    0.01 \n    0.01 \n    0.08 \n    0.06 \n  \n  \n    40-49 h \n    0.02 \n    0.02 \n    0.04 \n    0.01 \n    0.03 \n    0.00 \n    0.00 \n    0.01 \n    0.06 \n    0.03 \n  \n  \n    50-64 h \n    0.02 \n    0.03 \n    0.06 \n    0.01 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.09 \n    0.03 \n  \n  \n    64+ h \n    0.01 \n    0.01 \n    0.05 \n    0.00 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.05 \n    0.01 \n  \n  \n    01-12 m \n    0.01 \n    0.01 \n    0.02 \n    0.06 \n    0.01 \n    0.07 \n    0.06 \n    0.07 \n    0.01 \n    0.01 \n  \n  \n    12-14 m \n    0.03 \n    0.05 \n    0.04 \n    0.09 \n    0.04 \n    0.12 \n    0.05 \n    0.23 \n    0.01 \n    0.02 \n  \n  \n    15-17 m \n    0.07 \n    0.05 \n    0.02 \n    0.02 \n    0.03 \n    0.04 \n    0.01 \n    0.13 \n    0.00 \n    0.02 \n  \n  \n    18-20 m \n    0.06 \n    0.05 \n    0.03 \n    0.00 \n    0.02 \n    0.01 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n  \n  \n    21-29 m \n    0.16 \n    0.10 \n    0.05 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.04 \n    0.01 \n    0.05 \n  \n  \n    30-39 m \n    0.14 \n    0.08 \n    0.04 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n    0.02 \n    0.01 \n    0.03 \n  \n  \n    40-49 m \n    0.07 \n    0.05 \n    0.03 \n    0.01 \n    0.02 \n    0.00 \n    0.00 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    50-64 m \n    0.07 \n    0.06 \n    0.08 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    64+ m \n    0.02 \n    0.03 \n    0.08 \n    0.00 \n    0.03 \n    0.02 \n    0.00 \n    0.00 \n    0.00 \n    0.02 \n  \n\n\n\n\n\n\nres_hurto_tbl <- left_join(perfiles_hurto_tbl, grupos_tbl |> rename(total = prop)) |> \n    select(-n) |> \n    select(-bienes, -discos, -perfumes) |> \n    mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) |> \n    mutate(across(where(is.numeric), round, 2)) \n\nJoining, by = \"grupo\"\n\nres_hurto_tbl |> \n    mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    select(-total) |> \n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    -0.87 \n    -0.65 \n    -0.38 \n    1.1 \n    -0.52 \n    1.48 \n    3.52 \n    -0.36 \n    -0.06 \n    -0.23 \n  \n  \n    12-14 h \n    -0.89 \n    -0.45 \n    -0.29 \n    1.24 \n    -0.06 \n    0.86 \n    1.1 \n    0 \n    0.32 \n    0.02 \n  \n  \n    15-17 h \n    -0.6 \n    -0.17 \n    -0.25 \n    0.33 \n    0.48 \n    0.14 \n    -0.44 \n    0.16 \n    0.53 \n    0.34 \n  \n  \n    18-20 h \n    0.06 \n    0.35 \n    0.05 \n    -0.55 \n    0.77 \n    -0.61 \n    -0.87 \n    -0.42 \n    0.11 \n    0.57 \n  \n  \n    21-29 h \n    0.26 \n    0.31 \n    0.06 \n    -0.76 \n    0.49 \n    -0.86 \n    -0.92 \n    -0.48 \n    0.1 \n    0.89 \n  \n  \n    30-39 h \n    0.2 \n    0.2 \n    0.15 \n    -0.77 \n    0.42 \n    -0.87 \n    -0.8 \n    -0.69 \n    0.96 \n    0.47 \n  \n  \n    40-49 h \n    0 \n    -0.02 \n    0.72 \n    -0.61 \n    0.2 \n    -0.9 \n    -0.87 \n    -0.76 \n    1.51 \n    0.12 \n  \n  \n    50-64 h \n    -0.32 \n    0.11 \n    1.14 \n    -0.65 \n    0.23 \n    -0.7 \n    -0.94 \n    -0.84 \n    2.07 \n    0.01 \n  \n  \n    64+ h \n    -0.61 \n    -0.2 \n    2.19 \n    -0.71 \n    0.58 \n    -0.14 \n    -0.74 \n    -0.74 \n    1.83 \n    -0.33 \n  \n  \n    01-12 m \n    -0.65 \n    -0.69 \n    -0.27 \n    1.13 \n    -0.59 \n    1.39 \n    1.07 \n    1.37 \n    -0.78 \n    -0.73 \n  \n  \n    12-14 m \n    -0.46 \n    -0.28 \n    -0.37 \n    0.5 \n    -0.41 \n    0.9 \n    -0.18 \n    2.65 \n    -0.81 \n    -0.71 \n  \n  \n    15-17 m \n    0.49 \n    0.18 \n    -0.54 \n    -0.45 \n    -0.31 \n    -0.11 \n    -0.84 \n    1.83 \n    -0.92 \n    -0.49 \n  \n  \n    18-20 m \n    1.1 \n    0.71 \n    -0.08 \n    -0.83 \n    -0.32 \n    -0.8 \n    -0.82 \n    0.06 \n    -0.8 \n    -0.27 \n  \n  \n    21-29 m \n    1.58 \n    0.49 \n    -0.27 \n    -0.87 \n    -0.41 \n    -0.88 \n    -0.9 \n    -0.35 \n    -0.81 \n    -0.22 \n  \n  \n    30-39 m \n    1.64 \n    0.43 \n    -0.2 \n    -0.86 \n    -0.5 \n    -0.87 \n    -0.7 \n    -0.62 \n    -0.72 \n    -0.37 \n  \n  \n    40-49 m \n    1.39 \n    0.56 \n    0.09 \n    -0.79 \n    -0.37 \n    -0.84 \n    -0.86 \n    -0.7 \n    -0.67 \n    -0.31 \n  \n  \n    50-64 m \n    0.82 \n    0.56 \n    1.02 \n    -0.81 \n    -0.06 \n    -0.7 \n    -0.76 \n    -0.71 \n    -0.62 \n    -0.46 \n  \n  \n    64+ m \n    0.12 \n    0.36 \n    2.51 \n    -0.84 \n    0.26 \n    -0.04 \n    -0.85 \n    -0.77 \n    -0.79 \n    -0.2 \n  \n\n\n\n\n\n\nCompara tus conclusiones del mapa de correspondencias con esta información de los residuales\n\nNota adicionalmente que el ordenamiento de las categorías en la primera dimensión del mapa de correspondencias ayuda a interpretar:\n\nres_hurto_tbl |> select(\"grupo\", \"escritura\", \"juguetes\", \"dulces\", \"joyería\",\n                         \"herramientas\", \"otros\", \"libros\", \"tabaco\", \"accesorios\", \n                         \"ropa\") |> \n    mutate(across(where(is.numeric), \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n \n  \n    grupo \n    escritura \n    juguetes \n    dulces \n    joyería \n    herramientas \n    otros \n    libros \n    tabaco \n    accesorios \n    ropa \n  \n \n\n  \n    01-12 h \n    1.1 \n    3.52 \n    1.48 \n    -0.36 \n    -0.06 \n    -0.23 \n    -0.52 \n    -0.38 \n    -0.65 \n    -0.87 \n  \n  \n    12-14 h \n    1.24 \n    1.1 \n    0.86 \n    0 \n    0.32 \n    0.02 \n    -0.06 \n    -0.29 \n    -0.45 \n    -0.89 \n  \n  \n    15-17 h \n    0.33 \n    -0.44 \n    0.14 \n    0.16 \n    0.53 \n    0.34 \n    0.48 \n    -0.25 \n    -0.17 \n    -0.6 \n  \n  \n    18-20 h \n    -0.55 \n    -0.87 \n    -0.61 \n    -0.42 \n    0.11 \n    0.57 \n    0.77 \n    0.05 \n    0.35 \n    0.06 \n  \n  \n    21-29 h \n    -0.76 \n    -0.92 \n    -0.86 \n    -0.48 \n    0.1 \n    0.89 \n    0.49 \n    0.06 \n    0.31 \n    0.26 \n  \n  \n    30-39 h \n    -0.77 \n    -0.8 \n    -0.87 \n    -0.69 \n    0.96 \n    0.47 \n    0.42 \n    0.15 \n    0.2 \n    0.2 \n  \n  \n    40-49 h \n    -0.61 \n    -0.87 \n    -0.9 \n    -0.76 \n    1.51 \n    0.12 \n    0.2 \n    0.72 \n    -0.02 \n    0 \n  \n  \n    50-64 h \n    -0.65 \n    -0.94 \n    -0.7 \n    -0.84 \n    2.07 \n    0.01 \n    0.23 \n    1.14 \n    0.11 \n    -0.32 \n  \n  \n    64+ h \n    -0.71 \n    -0.74 \n    -0.14 \n    -0.74 \n    1.83 \n    -0.33 \n    0.58 \n    2.19 \n    -0.2 \n    -0.61 \n  \n  \n    01-12 m \n    1.13 \n    1.07 \n    1.39 \n    1.37 \n    -0.78 \n    -0.73 \n    -0.59 \n    -0.27 \n    -0.69 \n    -0.65 \n  \n  \n    12-14 m \n    0.5 \n    -0.18 \n    0.9 \n    2.65 \n    -0.81 \n    -0.71 \n    -0.41 \n    -0.37 \n    -0.28 \n    -0.46 \n  \n  \n    15-17 m \n    -0.45 \n    -0.84 \n    -0.11 \n    1.83 \n    -0.92 \n    -0.49 \n    -0.31 \n    -0.54 \n    0.18 \n    0.49 \n  \n  \n    18-20 m \n    -0.83 \n    -0.82 \n    -0.8 \n    0.06 \n    -0.8 \n    -0.27 \n    -0.32 \n    -0.08 \n    0.71 \n    1.1 \n  \n  \n    21-29 m \n    -0.87 \n    -0.9 \n    -0.88 \n    -0.35 \n    -0.81 \n    -0.22 \n    -0.41 \n    -0.27 \n    0.49 \n    1.58 \n  \n  \n    30-39 m \n    -0.86 \n    -0.7 \n    -0.87 \n    -0.62 \n    -0.72 \n    -0.37 \n    -0.5 \n    -0.2 \n    0.43 \n    1.64 \n  \n  \n    40-49 m \n    -0.79 \n    -0.86 \n    -0.84 \n    -0.7 \n    -0.67 \n    -0.31 \n    -0.37 \n    0.09 \n    0.56 \n    1.39 \n  \n  \n    50-64 m \n    -0.81 \n    -0.76 \n    -0.7 \n    -0.71 \n    -0.62 \n    -0.46 \n    -0.06 \n    1.02 \n    0.56 \n    0.82 \n  \n  \n    64+ m \n    -0.84 \n    -0.85 \n    -0.04 \n    -0.77 \n    -0.79 \n    -0.2 \n    0.26 \n    2.51 \n    0.36 \n    0.12 \n  \n\n\n\n\n\n\nOtras dimensiones\nEn el caso anterior, la calidad de la representación es cercana al 80%. Existen algunas desviaciones que la posiblemente la gŕafica no explica del todo, y algunas proyecciones son aproximadas. Podemos ver cómo se ven otras dimensiones de este análisis para entender desviaciones adicionales:\n\nplot(corr_hurto, dim = c(1, 3), map = \"rowgreen\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\n\n\nIzenman, A. J. 2009. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts en Statistics. Springer New York. https://books.google.com.mx/books?id=1CuznRORa3EC.\n\n\nLê, Sébastien, Julie Josse, y François Husson. 2008. «FactoMineR: An R Package for Multivariate Analysis». Journal of Statistical Software, Articles 25 (1): 1-18. https://doi.org/10.18637/jss.v025.i01."
  },
  {
    "objectID": "analisis-exp-numericos.html",
    "href": "analisis-exp-numericos.html",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones numéricas. Igual que en la sección anterior, consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "analisis-exp-numericos.html#cuantiles-o-percentiles-de-una-variable",
    "href": "analisis-exp-numericos.html#cuantiles-o-percentiles-de-una-variable",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.1 Cuantiles o percentiles de una variable",
    "text": "3.1 Cuantiles o percentiles de una variable\nEmpezamos explicando algunas ideas que no serán útiles más adelante.\nPor ejemplo, los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n# renombramos variables y niveles\n\nY vemos una muestra\n\nsample_n(propinas, 10) \n\n# A tibble: 10 × 6\n   cuenta_total propina fumador dia   momento num_personas\n          <dbl>   <dbl> <chr>   <chr> <chr>          <dbl>\n 1         10.8    1.47 No      Sab   Cena               2\n 2         16.5    3.23 Si      Jue   Comida             3\n 3         13      2    Si      Jue   Comida             2\n 4         20.5    4.06 Si      Sab   Cena               2\n 5         15.7    3    Si      Sab   Cena               3\n 6         27.2    2    Si      Sab   Cena               2\n 7         14.7    2.2  No      Sab   Cena               2\n 8         14.3    4    Si      Sab   Cena               2\n 9         14.8    3.02 No      Dom   Cena               2\n10         20.6    3.35 No      Sab   Cena               3\n\n\nAquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta. Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día (Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida.\nEl primer tipo de comparaciones que nos interesa hacer es para una medición numérica es: ¿Varían mucho o poco los datos? ¿Cuáles son valores típicos o centrales? ¿Existen valores muy extremos alejados de valores típicos?\nSupongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales:\n\npropinas <- propinas %>% \n  mutate(orden_cuenta = rank(cuenta_total, ties.method = \"first\"), \n         f = (orden_cuenta - 0.5) / n()) \ncuenta <- propinas %>% select(orden_cuenta, f, cuenta_total) %>% arrange(f)\nbind_rows(head(cuenta), tail(cuenta)) %>% knitr::kable()\n\n\n\n \n  \n    orden_cuenta \n    f \n    cuenta_total \n  \n \n\n  \n    1 \n    0.0020492 \n    3.07 \n  \n  \n    2 \n    0.0061475 \n    5.75 \n  \n  \n    3 \n    0.0102459 \n    7.25 \n  \n  \n    4 \n    0.0143443 \n    7.25 \n  \n  \n    5 \n    0.0184426 \n    7.51 \n  \n  \n    6 \n    0.0225410 \n    7.56 \n  \n  \n    239 \n    0.9774590 \n    44.30 \n  \n  \n    240 \n    0.9815574 \n    45.35 \n  \n  \n    241 \n    0.9856557 \n    48.17 \n  \n  \n    242 \n    0.9897541 \n    48.27 \n  \n  \n    243 \n    0.9938525 \n    48.33 \n  \n  \n    244 \n    0.9979508 \n    50.81 \n  \n\n\n\n\n\ny graficamos los datos en orden, interpolando valores consecutivos.\n\n\n\n\n\nA esta función le llamamos la función de cuantiles para la variable cuenta total. Nos sirve para comparar directamente los distintos valores que observamos los datos según el orden que ocupan.\n\n\n\n\n\n\nCuantiles de datos numéricos\n\n\n\nEl cuantil \\(f\\) de un bonche de datos numéricos es el valor \\(q(f)\\), en la escala de medición de nuestros datos, tal que aproximadamente una fracción \\(f\\) de los datos está por abajo de \\(q(f)\\).\n\nAl cuantil \\(f=0.5\\) le llamamos la mediana.\nA los cuantiles \\(f=0.25\\) y \\(f=0.75\\) les llamamos cuantiles inferior y superior.\n\n\n\nDispersión y valores centrales\n\nEl rango de datos va de unos 3 dólares hasta 50 dólares\nLos valores centrales (del cuantil 0.25 al 0.75, por ejemplo), están entre unos 13 y 25 dólares\nPodemos usar el cuantil 0.5 (mediana) para dar un valor central de esta distribución, que está alrededor de 18 dólares.\n\nY podemos dar resúmenes más refinados si es necesario\n\nEl cuantil 0.95 es de unos 35 dólares - sólo 5% de las cuentas son de más de 35 dólares\nEl cuantil 0.05 es de unos 8 dólares - sólo 5% de las cuentas son de 8 dólares o menos.\n\nFinalmente, la forma de la gráfica se interpreta usando su pendientes, haciendo comparaciones de diferentes partes de la gráfica:\n\nEntre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente es baja, lo que significa que al avanzar en los cuantiles, los valores observados no cambian mucho.\nCuando la pendiente es alta, quiere decir que los datos tienen más dispersión local o están más separados.\n\nY podemos considerar qué sucede en las colas de la distribucion:\n\nLa distribución de valores tiene asimetría: el 10% de las cuentas más altas tiene considerablemente más dispersión que el 10% de las cuentas más bajas. A veces decimos que la cola de la derecha es más larga que la cola de la izquierda\n\nEn algunos casos, es más natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos cuántos datos caen en cada cubeta:\n\n\n\n\n\nEs una gráfica más popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos.\nFinalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión menos común de Spear/Tufte (ST):\n\nlibrary(ggthemes)\ncuartiles <- quantile(cuenta$cuenta_total)\ncuartiles\n\n     0%     25%     50%     75%    100% \n 3.0700 13.3475 17.7950 24.1275 50.8100 \n\ng_1 <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + \n  labs(subtitle = \"Gráfica de cuantiles: Cuenta total\") +\n  geom_hline(yintercept = cuartiles[2], colour = \"gray\") + \n  geom_hline(yintercept = cuartiles[3], colour = \"gray\") +\n  geom_hline(yintercept = cuartiles[4], colour = \"gray\") +\n  geom_point(alpha = 0.5) + geom_line() \ng_2 <- ggplot(cuenta, aes(x = factor(\"ST\", levels =c(\"ST\")), y = cuenta_total)) + \n  geom_tufteboxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_3 <- ggplot(cuenta, aes(x = factor(\"T\"), y = cuenta_total)) + geom_boxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_4 <- ggplot(cuenta, aes(x = factor(\"P\"), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_1 + g_2 + g_3 + g_4 +\n  plot_layout(widths = c(8, 2, 2, 2))\n\n\n\n\nNota: Hay varias maneras de definir los cuantiles. Si tenemos \\(n\\) datos, podríamos poner \\(q(4/n)\\) como el cuarto dato (ordenando del más chico al más grande), y así sucesivamente. Esto implica por ejemplo que \\(q(1)\\) está definido como el valor más grande de los datos, y esto no es tan coveniente cuando trabajamos con modelos de probabilidad. Por eso preferimos definir al \\(k\\)-ésimo dato como el cuantil \\(q(\\frac{k - 0.5}{n})\\). Para las gráficas que estamos haciendo por el momento esto no es muy importante."
  },
  {
    "objectID": "analisis-exp-numericos.html#media-y-desviación-estándar",
    "href": "analisis-exp-numericos.html#media-y-desviación-estándar",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.2 Media y desviación estándar",
    "text": "3.2 Media y desviación estándar\nOtras medidas más comunes de localización y dispersión para conjuntos de datos son media y desviación estándar muestral.\nLa media de un conjunto de datos \\(x_1,\\ldots, x_n\\) es\n\\[\\bar{x} = \\frac{1}{n}\\sum x_i\\]\ny la desviación estándar es\n\\[\\hat{\\sigma} =\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\nEn general, no son muy apropiadas para iniciar el análisis exploratorio, pues:\n\nSon medidas más difíciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Como ejercicio, intenta explicar intuitivamente qué es la media. Después prueba con la desviación estándar.\nNo son resistentes a valores atípicos o erróneos. Su falta de resistencia los vuelve poco útiles en las primeras etapas de limpieza y descripción.\n\nSin embargo,\n\nLa media y desviación estándar son computacionalmente convenientes, y para el trabajo de modelado, por ejemplo, tienen ventajas claras (cuando se cumplen supuestos). Por lo tanto regresaremos a estas medidas una vez que estudiemos modelos de probabilidad básicos.\nMuchas veces, ya sea por tradición, porque así se ha hecho el análisis antes, conviene usar estas medidas conocidas."
  },
  {
    "objectID": "analisis-exp-numericos.html#comparando-grupos-con-variables-numéricas",
    "href": "analisis-exp-numericos.html#comparando-grupos-con-variables-numéricas",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.3 Comparando grupos con variables numéricas",
    "text": "3.3 Comparando grupos con variables numéricas"
  },
  {
    "objectID": "analisis-exp-numericos.html#ejemplo-precios-de-casas",
    "href": "analisis-exp-numericos.html#ejemplo-precios-de-casas",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "Ejemplo: precios de casas",
    "text": "Ejemplo: precios de casas\nConsideramos datos de precios de ventas de la ciudad de Ames, Iowa. Nos interesa entender la variación del precio de las casas.\n\n\n\nCalculamos primeros unos cuantiles de los precios de las casas:\n\nquantile(casas %>% pull(precio_miles)) \n\n   0%   25%   50%   75%  100% \n 37.9 132.0 165.0 215.0 755.0 \n\n\nUna primera comparación que podemos hacer es considerar las distintas zonas de la ciudad. Podemos usar diagramas de caja y brazos para comparar precios en distintas zonas de la ciudad:\n\nggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip()\n\n\n\n\nNótese que de cada zona, los datos tienen una cola derecha más larga que la izquierda, e incluso hay valores extremos en la cola derecha que exceden el rango de variación usual. Una razón por la que puede suceder esto es que haya características particulares que agregan valor considerable a una casa, por ejemplo, el tamaño, una alberca, etc.\nEn primer lugar, podemos considerar el área de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo:\n\n\n\n\nggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip()\n\n\n\n\nNótese ahora que la variación alrededor de la media es mucho más simétrica, y ya no vemos tantos datos extremos. Aún más, la variación dentro de cada zona parece ser similar, y podríamos describir restos datos de la siguiente forma:\nCuantificamos la variación que observamos de zona a zona y la variación que hay dentro de zonas. La variación que vemos entre las medianas de la zona es:\n\ncasas %>% group_by(nombre_zona) %>% \n  summarise(mediana_zona = median(precio_m2)) %>% \n  pull(mediana_zona) %>% quantile %>% round\n\n  0%  25%  50%  75% 100% \n 963 1219 1298 1420 1725 \n\n\nY las variaciones con respecto a las medianas dentro de cada zona, agrupadas, se resume como:\n\nquantile(casas %>% group_by(nombre_zona) %>% \n  mutate(residual = precio_m2 - median(precio_m2)) %>% \n  pull(residual)) %>% round\n\n  0%  25%  50%  75% 100% \n-765 -166    0  172 1314 \n\n\nNótese que este último paso tiene sentido pues la variación dentro de las zonas, en términos de precio por metro cuadrado, es similar. Esto no lo podríamos hacer de manera efectiva si hubiéramos usado el precio de las casas sin ajustar por su tamaño.\nY vemos que la mayor parte de la variación del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tamaño de las casas. La variación dentro de cada zona es aproximadamente simétrica, aunque la cola derecha es ligeramente más larga con algunos valores extremos.\nPodemos seguir con otro indicador importante: la calificación de calidad de los terminados de las casas. Como primer intento podríamos hacer:\n\n\n\n\n\nLo que indica que las calificaciones de calidad están distribuidas de manera muy distinta a lo largo de las zonas, y que probablemente no va ser simple desentrañar qué variación del precio se debe a la zona y cuál se debe a la calidad."
  },
  {
    "objectID": "analisis-exp-numericos.html#distribuciones-sesgadas-y-atípicos",
    "href": "analisis-exp-numericos.html#distribuciones-sesgadas-y-atípicos",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.4 Distribuciones sesgadas y atípicos",
    "text": "3.4 Distribuciones sesgadas y atípicos\nEn algunos casos tenemos que trabajar con mediciones que tienen una cola (usualmente la derecha) mucho más larga que la otra. Veamos cuáles son consecuencias típicas.\nConsideremos por ejemplos una muestra de los datos de ENIGH 2018\n\nenigh <- read_csv(\"./datos/enigh-ejemplo.csv\")\n\nY los deciles de ingreso son\n\nenigh <- mutate(enigh, ingreso_mensual_miles = INGTOT / 3000)\n\nenigh %>% \n  summarise(\n    f = seq(0, 1, 0.1),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = seq(0, 1, 0.1))) %>% \n  kable(digits = 2)\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    0.0 \n    0.81 \n  \n  \n    0.1 \n    2.58 \n  \n  \n    0.2 \n    3.86 \n  \n  \n    0.3 \n    5.53 \n  \n  \n    0.4 \n    6.75 \n  \n  \n    0.5 \n    8.27 \n  \n  \n    0.6 \n    9.99 \n  \n  \n    0.7 \n    12.95 \n  \n  \n    0.8 \n    16.20 \n  \n  \n    0.9 \n    22.18 \n  \n  \n    1.0 \n    317.53 \n  \n\n\n\n\n\ndonde podemos ver cómo cuando nos movemos a deciles más altos, la dispersión aumenta. Existen algunos valores muy grandes. Un histograma no funciona muy bien con estos datos.\n\nggplot(enigh, aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSi filtramos los valores muy grandes, de todas formas encontramos una forma similar con una cola larga a la derecha:\n\nggplot(enigh %>% filter(ingreso_mensual_miles < 90), \n       aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNótese que la media de estos datos no es un resúmen muy útil, porque es difícil de interpretar. Por los valores grandes, la media es considerablemente más alta que la mediana:\n\nenigh %>% \n  summarise(\n    media = mean(ingreso_mensual_miles),\n    mediana =  quantile(ingreso_mensual_miles, probs = 0.5)) %>% \n  kable(digits = 2)\n\n\n\n \n  \n    media \n    mediana \n  \n \n\n  \n    12.04 \n    8.27 \n  \n\n\n\n\n\nEsta es otra razón para incluir información de cuantiles en la etapa descriptiva. Por ejemplo, podríamos resumir:\n\nenigh %>% \n  summarise(\n    f = c(\"min\", 0.5, \"0.50\",  0.95, \"max\"),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = c(0, 0.05, 0.5, 0.95, 1))) %>% \n  kable(digits = 2)\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    min \n    0.81 \n  \n  \n    0.5 \n    1.92 \n  \n  \n    0.50 \n    8.27 \n  \n  \n    0.95 \n    32.24 \n  \n  \n    max \n    317.53 \n  \n\n\n\n\n\nPara obtener una gŕafica más informativa, podemos utilizar una escala logarítmica. El logaritmo de los ingresos es más fácil de describir y veremos también más fácil de trabajar.\n\nggplot(enigh, \n       aes(x = ingreso_mensual_miles)) + \n  geom_histogram(binwidth = 0.12) +\n  scale_x_log10(breaks = c(1, 2, 4, 8, 16, 32, 64, 128, 256))"
  },
  {
    "objectID": "analisis-exp-numericos.html#factor-y-respuesta-numéricos",
    "href": "analisis-exp-numericos.html#factor-y-respuesta-numéricos",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.5 Factor y respuesta numéricos",
    "text": "3.5 Factor y respuesta numéricos\nEn las secciones anteriores vimos cómo describir “bonches” de datos numéricos y categóricos. Adicionalmente, vimos cómo usar esas técnicas para comparar las descripciones a lo largo de varios subconjuntos de los datos.\nEn estos casos, muchas veces llamamos factor a la variables que forma los grupos, y respuesta a la variable que estamos comparando. Por ejemplo, en el caso de tomadores de té comparamos uso de complementos (respuesta) a lo largo de consumidores de distintos tipos de té (factor) En el caso de los precios de las casas comparamos el precio de las casas (respuesta) dependiendo del vecindario (factor) dónde se encuentran.\nCuando tenemos una factor numérico y una respuesta numérica podemos comenzar haciendo diagramas de dispersión. Por ejemplo,"
  },
  {
    "objectID": "analisis-exp-numericos.html#ejemplo-cuenta-total-y-propina",
    "href": "analisis-exp-numericos.html#ejemplo-cuenta-total-y-propina",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "Ejemplo: cuenta total y propina",
    "text": "Ejemplo: cuenta total y propina\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n\nPodríamos comenzar haciendo:\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_point() + geom_rug(colour = \"salmon\", alpha = 0.5)\n\n\n\n\nAhora queremos comparar la distribución de propina (respuesta) para distintos niveles del factor (cuenta_total). Por ejemplo, ¿cómo se compara propina cuando la cuenta es de 15 dólares vs 30 dólares?\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_vline(xintercept = c(15, 30), colour = \"red\") +\n   geom_point() \n\n\n\n\nVemos que los datos de propinas alrededor de 30 dólares están centrados en valores más grandes que en el nivel de 15 dólares, y también que hay más dispersión en el nivel de 30 dólares. Sin embargo, vemos que tenemos un problema: existen realmente muy pocos datos que tengan exactamente 15 o 30 dólares de cuenta. La estrategia es entonces considerar qué sucede cuando la cuenta está alrededor de 15 o alrededor de 30 dólares, donde alrededor depende del problema particular y de cuántos datos tenemos:\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.5) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.5) +\n   geom_point() \n\n\n\n\nConsiderando estos grupos de datos, podemos describir de las siguiente forma, por ejemplo:\n\npropinas %>% \n   mutate(grupo = cut(cuenta_total,  breaks = c(0, 13, 17, 28, 32))) %>% \n   filter(grupo %in% c(\"(13,17]\", \"(28,32]\")) %>% \n   group_by(grupo) %>% \n   summarise(\n      n = n(),\n      q10 = quantile(propina, 0.10),\n      mediana = quantile(propina, 0.5),\n      q90 = quantile(propina, 0.90),\n      rango_cuartiles = quantile(propina, 0.75) - quantile(propina, 0.25)) %>% \n   kable(digits = 2)\n\n\n\n \n  \n    grupo \n    n \n    q10 \n    mediana \n    q90 \n    rango_cuartiles \n  \n \n\n  \n    (13,17] \n    57 \n    1.85 \n    2.47 \n    3.49 \n    1.0 \n  \n  \n    (28,32] \n    16 \n    2.02 \n    3.69 \n    5.76 \n    2.2 \n  \n\n\n\n\n\nConde confirmamos que el nivel general de propinas es más alto alrededor de cuentas de total 30 que de total 15, y la dispersión también es mayor. Podríamos hacer un diagrama de caja y brazos también."
  },
  {
    "objectID": "analisis-exp-numericos.html#suavizadores-locales",
    "href": "analisis-exp-numericos.html#suavizadores-locales",
    "title": "3  Descripción de datos univariados numéricos",
    "section": "3.6 Suavizadores locales",
    "text": "3.6 Suavizadores locales\nEl enfoque del ejemplo anterior puede ayudar en algunos casos nuestra tarea descriptiva, pero quisiéramos tener un método más general y completo para entender cómo es una respuesta numérica cuando el factor es también numérico.\nEn este caso, podemos hacer por ejemplo medias o medianas locales. La idea general es, en términos de nuestro ejemplo de propinas:\n\nQueremos producir un resumen en un valor de cuenta total \\(x\\).\nConsideramos valores de propina asociados a cuentas totales en un intervalo \\([x-e, x+e]\\).\nCalculamos estadísticas resumen en este rango para la respuesta\nUsualmente también ponderamos más alto valores que están cerca de \\(x\\) y ponderamos menos valores más lejanos a \\(x\\)\n\nEste tipo de suavizadores se llaman a veces suavizadores loess (ver (Cleveland 1993)).\nPor ejemplo,\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_smooth(method = \"loess\", span = 0.5, degree= 0, \n               method.args = list(family = \"symmetric\"), se = FALSE) \n\nWarning: Ignoring unknown parameters: degree\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n# symmetric es un método robusto iterativo, que reduce el peso de atípicos\n\nEl parametro span controla el tamaño de la ventana de datos que se toma en cada punto. Nótese como alrededor de 15 y 30 los valores por donde pasa el suavizador son similares a las medianas que escribimos arriba.\nPodemos ajustar en cada ventana tambien rectas de minimos cuadrados, y obtener un suavizador de tipo lineal. En la siguiente gráfica mostramos cómo funciona este suavizador para distintos tamaños de ventanas (span)\n\n\n\nSuavizador loess\n\n\n\n\n\nLos suavizadores loess tienen como fin mostrar alrededor de qué valor de distribuye la respuesta (eje vertical) para distintos valores del factor (eje horizontal). Se escoge span suficientemente baja de forma que mostremos patrones claros en los datos y casi no capturemos variación debida a los tamaños de muestra chicos.\n\n\nEn la animación anterior, un valor de span de 0.15 funciona aprpiadamente, y uno de 0.05 es demasiado bajo y uno de 1.0 es demasiado alto. Es importante explorar con el valor de span pues depende de cuántos datos tenemos y cómo es su dispersión.\nPodemos también mostrar estimaciones de medianas y cuantiles de la siguiente forma (nota: es necesario escoger lambda con cuidado, cuanto más alto sea lambda más suave es la curva obtenida):\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75)) +\n   scale_y_continuous(breaks = seq(0, 10, 1))\n\nWarning: Computation failed in `stat_quantile()`:\n\n\n\n\n\nFinalmente, entendimiento de los datos no permite también hacer gráficas más útiles. En este ejemplo particular podría por ejemplo calcular el porcentaje de la propina sobre la cuenta total:\n\npropinas <- mutate(propinas, pct_propina = propina / cuenta_total)\nggplot(propinas, aes(x = cuenta_total, y = pct_propina)) +\n   geom_point() +\n   scale_y_continuous(breaks = seq(0,1, 0.05)) +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75))\n\nWarning: Computation failed in `stat_quantile()`:\n\n\n\n\n\nObserva que la descripción es más simple que si usamos propina cruda y cuenta\n\nPara cuentas chicas, el porcentaje de propina puede ser muy alto (aún cuando la propina en sí no es tan grande):\n\n\nfilter(propinas, pct_propina > 0.30) %>% \n  arrange(desc(pct_propina)) %>% \n  kable(digits = 2)\n\n\n\n \n  \n    cuenta_total \n    propina \n    fumador \n    dia \n    momento \n    num_personas \n    pct_propina \n  \n \n\n  \n    7.25 \n    5.15 \n    Si \n    Dom \n    Cena \n    2 \n    0.71 \n  \n  \n    9.60 \n    4.00 \n    Si \n    Dom \n    Cena \n    2 \n    0.42 \n  \n  \n    3.07 \n    1.00 \n    Si \n    Sab \n    Cena \n    1 \n    0.33 \n  \n\n\n\n\n\n\nPara cuentas relativamente chicas (10 dólares, el porcentaje de propina está por encima de 15%). Este porcentaje tiende a reducirse a valores 10% y 15% para cuentas más grandes\nExiste variación considerable alrededor de estos valores centrales. El rango intercuartiles es aproximadamente de 5 puntos porcentuales.\n\nO de manera más resumida:\n\nLa mediana de propinas está ligeramente por arriba de 15% para cuantas relativamente chicas. Esta mediana baja hasta alrededor de 10%-15% para cuentas más grandes (más de 40 dólares)\nLa mitad de las propinas no varía más de unos 3 puntos porcentuales alrededor de estas medianas.\nExisten propinas atípicas: algunas muy bajas de 1 dólar, muy por debajo del 15%, y ocasionalmente algunas muy altas en porcentaje. Estas últimas ocurren ocasinalmente especialmente en cuentas chicas (por ejemplo, una propina de 1 dólar en una cuenta de 3 dólares).\n\n\n\n\n\nCleveland, William S. 1993. Visualizing Data. Hobart Press."
  },
  {
    "objectID": "basicos-inferencia.html",
    "href": "basicos-inferencia.html",
    "title": "Inferencia estadística",
    "section": "",
    "text": "A grandes rasgos, en la inferencia estadística buscamos hacer afirmaciones acerca de una colección de datos de la cual sólo tenemos información parcial.\nNos concentraremos en dos de las situaciones más comunes:\nPor ejemplo, consideremos esta población de 15 personas:\nPara una muestra de ellos tenemos información acerca de su estatura y peso. ¿Qué podríamos decir acerca de la estatura y el peso de la población general?\nEn este caso, la situación se ve como sigue. Imaginemos que tenemos 15 personas con dolor de cabeza, y obtenemos los siguientes datos:\nNuestra pregunta en este caso es del tipo: ¿ayuda la aspirina a reducir el dolor de cabeza en esta población? ¿qué tanto ayudaa? Igualmente, tenemos información incompleta, en el sentido de que sólo observamos un resultado potencial de cada persona, dependiendo de si tomó aspirina o no. Si supiéramos los dos resultados potenciales de cada persona entonces podríamos contestar la pregunta sin dificultad."
  },
  {
    "objectID": "basicos-inferencia.html#proceso-de-selección-o-asignación",
    "href": "basicos-inferencia.html#proceso-de-selección-o-asignación",
    "title": "Inferencia estadística",
    "section": "Proceso de selección o asignación",
    "text": "Proceso de selección o asignación\nLas preguntas que planteamos arriba son difíciles de contestar cuando no conocemos bien el proceso de selección de individuos en la muestra o no conocemos el proceso de asignación de la aspirina.\nPor ejemplo, llegaríamos a conclusiones muy distintas si nos dijeran que:\nPrimera población:\n\nEscogimos las 5 personas que usan ropa talla chica.\nEscogimos las 5 personas que llegaron primero en una carrera de 100 metros.\nEscogimos las personas cuyo día de nacimiento era más bajo.\n\nSegunda población:\n\nSólo dimos aspirinas a las personas que reportaron un nivel de dolor de cabeza muy alto.\nSolo dimos aspirina a las personas que llegaron primero en una carrera de 100 metros.\nDimos una aspirina exclusivamente a las personas cuyo día de nacimiento es par.\n\nDiscute qué conclusiones podrías llegar en cada uno de estos escenarios.\nLos casos 1 y 2 en ambas poblaciones son en general más difíciles de resolver adecuadamente, y explicaremos con más ejemplos. Adicionalmente, es también más difícil cuantificar el nivel de incertidumbre de nuestras respuestas, pues dependen de muchos detalles del proceso de selección o asignación.\n\n\n\n\n\n\nSelección y asignación\n\n\n\nCuando el proceso de selección o asignación tiene relaciones complicadas con las cantidades de interés, puede ser muy difícil dar respuesta a preguntas inferenciales de manera adecuada.\n\n\nAdecuada en este contexto quiere decir correctamente calibrada, lo cual explicaremos más adelante."
  },
  {
    "objectID": "basicos-inferencia.html#asignación-aleatoria-de-tratamientos",
    "href": "basicos-inferencia.html#asignación-aleatoria-de-tratamientos",
    "title": "Inferencia estadística",
    "section": "Asignación aleatoria de tratamientos",
    "text": "Asignación aleatoria de tratamientos\nVeremos primero unos ejemplos de qué sucede cuando el tratamiento no se asigna al azar. En un estudio de 1981, investigadores reportaron una asociación de cáncer de páncreas con consumo de café. Sus resultados agregados fueron:\n\ntab_cafe <- crossing(num_tazas = c(0, 1.5, 3.5, 5), enf_pancreas = c(\"si\", \"no\")) |> \n  mutate(n = c(88, 20, 271, 153, 154, 106, 88, 130))\ntab_cafe |> pivot_wider(names_from = enf_pancreas, values_from = n) |> \n  mutate(prop_si = si / (no + si))\n\n# A tibble: 4 × 4\n  num_tazas    no    si prop_si\n      <dbl> <dbl> <dbl>   <dbl>\n1       0      88    20   0.185\n2       1.5   271   153   0.361\n3       3.5   154   106   0.408\n4       5      88   130   0.596\n\n\nLo que indica una asociación fuerte entre consumo de café y proporción de pacientes con cáncer de páncres. Los pacientes fueron entrevistados en varios hospitales. Se seleccionaron pacientes con cáncer de páncreas y pacientes control fueron entrevistados que corresponden a los mismos doctores. En el artículo señala que por la naturaleza de las enfermedades, había una cantidad considerable de pacientes control con condiciones gastrointestinales.\nEn este caso, el proceso de asignación de quién toma café y cuánto toma es complicado. Sabemos sin embargo que:\n\nPacientes con problemas gastrointestinales muchas veces tienen dietas restringidas y no se les permite tomar café, o sólo o una cantidad baja de café.\nLas razones para ser seleccionados con más alta probabilidad en el estudio son: tener cáncer de páncreas, o tener otros problemas gastrointestinales.\n\nEstas tres relaciones causales podemos representarlas como sigue:\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ndag_cafe <- dagitty('dag{Cafe [exposure,pos=\"-1,-2\"]\n  Cancer [outcome,pos=\"1,-2\"]\n  Entrevistado [pos=\"0,-3\"]\n  Gastro [pos=\"-1,-2.5\"]\n  Gastro -> Cafe\n  Cancer -> Entrevistado; Gastro  -> Entrevistado; Cafe -> Cancer}')\ndag_cafe_tidy <- tidy_dagitty(dag_cafe) |> \n  mutate(tipo = ifelse(name == \"Cafe\" & to == \"Cancer\", \"dotted\", \"solid\"))\ndag_cafe_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, colour = tipo )) + \n  geom_dag_edges(aes(edge_linetype = tipo)) +\n  geom_dag_point(colour = \"salmon\") +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\nLas consecuencias de estas tres relaciones casuales es la siguiente: si alguien es entrevistado, puede ser por dos razones diferentes: tiene cáncer, o si no tiene cáncer, tiene probabilidad alta de tener problemas gastrointestinales. Esto último qui2312 implica, por restricciones de dieta, que tienden a tomar menos café. Con esto, hemos demostrado que con este esquema de selección puede aparecer naturalmente una asociación entre cáncer y consumo de café, aún cuando no exista una relación causal entre tomar café y cáncer de páncreas.\nEl problema es que la exposición a nuestro “tratamiento” está siendo influida por una variable que tiene asociación con nuestro “resultado”. Esto puede pasar de diferentes maneras. Quizá en este ejemplo podemos controlar por enfermedad gastrointestinal, pero no podemos estar seguros que no existan otras dificultades: por ejemplo, si café estuviera asociado con fumar, entonces esa podría ser otra razón por la que observaríamos una relación entre cáncer y café aún cuando no haya relación causal entre estas dos variables.\nSin embargo, si aleatorizamos el tratamiento, la sitación se hace mucho más simple. Si pudiéramos escoger a un grupo de personas, y asignar un grupo a tomar café y otro grupo a no tomarlo, tendríamos el diagrama:\n\ndag_cafe <- dagitty(\"dag{Gastro -> Cafe;  Cafe -> Cancer}\")\ndag_cafe_tidy <- tidy_dagitty(dag_cafe) |> \n  mutate(tipo = ifelse(name == \"Cafe\" & to == \"Cancer\", \"dotted\", \"solid\"))\ndag_cafe_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, colour = tipo )) + \n  geom_dag_edges(aes(edge_linetype = tipo)) +\n  geom_dag_point(colour = \"salmon\") +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\nY en este caso, podemos ver directamente la relación causal entre café y cáncer, aún cuando problemas gastrointestinales pueden afectar la asignación del tratamiento."
  },
  {
    "objectID": "basicos-inferencia.html#selección-aleatoria-de-muestra",
    "href": "basicos-inferencia.html#selección-aleatoria-de-muestra",
    "title": "Inferencia estadística",
    "section": "Selección aleatoria de muestra",
    "text": "Selección aleatoria de muestra"
  },
  {
    "objectID": "pruebas-hipotesis.html",
    "href": "pruebas-hipotesis.html",
    "title": "4  Inferencia y Remuestreo",
    "section": "",
    "text": "En una buena parte de los problemas de análisis de datos tenemos información incompleta. Por ejemplo:\nEn ambos casos, las comparaciones que hacemos están sujetas a variabilidad que no controlamos. Podemos pensar en esta variabiliad de dos formas:\nLa incertidumbre del segundo caso es más fácil de cuantificar bajo ciertas condiciones:"
  },
  {
    "objectID": "pruebas-hipotesis.html#pruebas-de-hipótesis",
    "href": "pruebas-hipotesis.html#pruebas-de-hipótesis",
    "title": "4  Inferencia y Remuestreo",
    "section": "Pruebas de hipótesis",
    "text": "Pruebas de hipótesis\nLas primeras técnicas que veremos intentan contestar la siguiente pregunta:\n\nSi observamos cierto patrón en los datos, ¿cómo podemos cuantificar la evidencia de que es un patrón notable y no sólo debido a fluctuaciones en los datos particulares que tenemos? ¿Cómo sabemos que no estamos sobreinterpretando esas fluctuaciones?\n\nPor ejemplo:\n\nUn sistema tiene cierto comportamiento “usual” para el cual tenemos datos históricos. El sistema presenta fluctuaciones en el tiempo.\nObservamos la última salida de nuestro sistema. Naturalmente, tiene fluctuaciones. ¿Esas fluctuaciones son consistentes con la operación usual del sistema? ¿Existe evidencia para pensar que algo en el sistema cambió?"
  },
  {
    "objectID": "pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "href": "pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "title": "4  Inferencia y Remuestreo",
    "section": "Comparación con poblaciones de referencia",
    "text": "Comparación con poblaciones de referencia\nEn las prueba de hipótesis, tratamos de construir distribuciones de referencia para comparar resultados que obtengamos con un “estándar” de variación, y juzgar si nuestros resultados son consistentes con la referencia o no (Box et al. (1978)).\nEn algunos casos, ese estándar de variación puede construirse con datos históricos.\n\nEjemplo\nSupongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado “normal” durante un tiempo considerable, y cuando observamos nuevos datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando de manera similar.\nDigamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo mostramos cinco ejemplos donde el sistema opera normalmente, que muestra la variabilidad en el tiempo en ventanas cortas del sistema.\nAhora suponemos que obtenemos una nueva ventana de datos. ¿Hay evidencia en contra de que el sistema sigue funcionando de manera similar?\nNuestra primera inclinación debe ser comparar: en este caso, compararamos ventanas históricas con nuestra nueva serie:\n\n\n\n\n# usamos datos simulados para este ejemplo\nset.seed(8812)\nhistoricos <- simular_serie(2000)\n\n\n\n\n\n\n¿Vemos algo diferente en los datos nuevos (el panel de color diferente)?\nIndpendientemente de la respuesta, vemos que hacer este análisis de manera tan simple no es siempre útil: seguramente podemos encontrar maneras en que la nueva muestra (4) es diferente a muestras históricas. Por ejemplo, ninguna de muestras tiene un “forma de montaña” tan clara.\nNos preguntamos si no estamos sobreinterpretando variaciones que son parte normal del proceso.\nPodemos hacer un mejor análisis si extraemos varias muestras del comportamiento usual del sistema, graficamos junto a la nueva muestra, y revolvemos las gráficas para que no sepamos cuál es cuál. Entonces la pregunta es:\n\n¿Podemos detectar donde están los datos nuevos?\n\nEsta se llama una prueba de lineup, o una prueba de ronda de sospechosos (Hadley Wickham et al. (2010)). En la siguiente gráfica, en uno de los páneles están los datos recientemente observados. ¿Hay algo en los datos que distinga al patrón nuevo?\n\n# nuevos datos\nobs <- simular_serie(500, x_inicial = last(obs$obs))\n# muestrear datos históricos\nprueba_tbl <- muestrear_ventanas(historicos, obs[1:20, ], n_ventana = 20)\n# gráfica de pequeños múltiplos\nggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + \n     facet_wrap(~rep, nrow = 4) + scale_y_log10()\n\n\n\n\nEjercicio: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Qué implica que la gráfica que escogamos como “más diferente” no sean los datos nuevos? ¿Qué implica que le “atinemos” a la gráfica de los datos nuevos?\nAhora observamos al sistema en otro momento y repetimos la comparación. En el siguiente caso obtenemos:\n\n\n\n\n\nAunque es imposible estar seguros de que ha ocurrido un cambio, la diferencia de una de las series es muy considerable. Si identificamos los datos correctos, la probabilidad de que hayamos señalado la nueva serie “sobreinterpretando” fluctuaciones en un proceso que sigue comportándose normalente es 0.05 - relativamente baja. Detectar los datos diferentes es evidencia en contra de que el sistema sigue funcionando de la misma manera que antes.\nObservaciones y terminología:\n\nLlamamos hipótesis nula a la hipótesis de que los nuevos datos son producidos bajo las mismas condiciones que los datos de control o de referencia.\nSi no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia en contra de la hipótesis nula.\nSi escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia en contra de la hipótesis nula.\n\n¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos?\n\nCuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. Decimos que el nivel de significancia de la prueba es la probabilidad de seleccionar a los datos correctos cuando la hipótesis nula es cierta (el sistema no ha cambiado). En el caso de 20 paneles, la significancia es de 1/20 = 0.05. Cuando detectamos los datos nuevos, niveles de significancia más bajos implican más evidencia en contra de la nula.\nSi acertamos, y la diferencia es más notoria y fue muy fácil detectar la gráfica diferente (pues sus diferencias son más extremas), esto también sugiere más evidencia en contra de la hipótesis nula.\nFinalmente, esta prueba rara vez (o nunca) nos da seguridad completa acerca de ninguna conclusión, aún cuando hiciéramos muchos páneles."
  },
  {
    "objectID": "pruebas-hipotesis.html#comparando-distribuciones",
    "href": "pruebas-hipotesis.html#comparando-distribuciones",
    "title": "4  Inferencia y Remuestreo",
    "section": "Comparando distribuciones",
    "text": "Comparando distribuciones\nAhora intentamos un ejemplo más típico.\nSupongamos tenemos muestras para tres grupos a, b y c, que quiere decir que dentro de cada grupo, el proceso e selección de los elementos se hace de manera al azar y de manera simétrica (por ejemplo cada elemento tiene a misma probabiidad de ser seleccionado, y las extracciones se hacen de manera independiente.)\nQueremos comparar las distribuciones de los datos obtenidos para cada grupo. Quizá la pregunta detrás de esta comparación es: el grupo de clientes b recibió una promoción especial. ¿Están gastando más? La medición que comparamos es el gasto de los clientes.\n\n\n\n\n\nEn la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias que observamos se deben variación muestral, por ejemplo.\nPodemos construir ahora una hipótesis nula, que establece que las observaciones provienen de una población similar:\n\nLas tres poblaciones (a, b, c) son prácticamente indistiguibles. En este caso, la variación que observamos se debería a que tenemos información incompleta.\n\nComo en el ejemplo anterior necesitamos construir o obtener una distribución de referencia para comparar qué tan extremos o diferentes son los datos que observamos. Esa distribución de referencia debería estar basada en el supuesto de que los grupos producen datos de distribuciones similares.\nSi tuvieramos mediciones similares históricas de estos tres grupos, quizá podríamos extraer datos de referencia y comparar, como hicimos en el ejempo anterior. Pero esto es menos común en este tipo de ejemplos."
  },
  {
    "objectID": "pruebas-hipotesis.html#permutaciones-y-el-lineup",
    "href": "pruebas-hipotesis.html#permutaciones-y-el-lineup",
    "title": "4  Inferencia y Remuestreo",
    "section": "Permutaciones y el lineup",
    "text": "Permutaciones y el lineup\nPara abordar este problema podemos pensar en usar permutaciones de los grupos de la siguiente forma (Box et al. (1978), Hesterberg (2015)):\n\nSi los grupos producen datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información.\nPodríamos permutar al azar las etiquetas y observar nuevamente la gráfica de caja y brazos por grupos.\nSi la hipótesis nula es cierta (grupos idénticos), esta es una muestra tan verosímil como la que obtuvimos.\nAsí que podemos construir datos de referencia permutando las etiquetas de los grupos al azar, y observando la variación que ocurre.\nSi la hipótesis nula es cercana a ser cierta, no deberíamos de poder distinguir fácilmente los datos observados de los producidos con las permutaciones al azar.\n\nVamos a intentar esto, por ejemplo usando una gráfica de cuantiles simplificada. Hacemos un lineup, o una rueda de sospechosos (usamos el paquete H. Wickham, Chowdhury, y Cook (2012), ver Hadley Wickham et al. (2010)), donde 19 de los acusados son generados mediante permutaciones al azar de la variable del grupo, y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera\nUsamos una gráfica que muestra los cuantes 0.10, 0.50, 0.90:\n\nset.seed(88)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\ndecrypt(\"Jn3o HPdP I7 LQgIdIQ7 eA\")\n\nreps_mezcla <- reps |>  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177))\ngrafica_cuantiles(reps_mezcla, grupo_1, x) + \n    facet_wrap(~.sample, ncol = 5) + ylab(\"x\") + \n    labs(caption = \"Mediana y percentiles 10% y 90%\")+ geom_point(aes(colour = grupo_1))\n\n`summarise()` has grouped output by 'grupo_1'. You can override using the\n`.groups` argument.\n\n\n\n\n\nY la pregunta que hacemos es podemos distinguir nuestra muestra entre todas las replicaciones producidas con permutaciones?\nEjercicio: ¿dónde están los datos observados? Según tu elección, ¿qué tan diferentes son los datos observados de los datos nulos?\nEn este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones similares y es factible que las diferencias que observamos se deban a variación muestral.\n\nSi la persona escoge los verdaderos datos, encontramos evidencia en contra de la hipótesis nula (los tres grupos son equivalentes). En algunos contextos, se dice que los datos son significativamente diferentes al nivel 0.05. Esto es evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo.\nSi la persona escoge uno de los datos permutados, no encontramos evidencia en contra de que los tres grupos producen datos con distribuciones similares."
  },
  {
    "objectID": "pruebas-hipotesis.html#comparaciones-con-lineup-2",
    "href": "pruebas-hipotesis.html#comparaciones-con-lineup-2",
    "title": "4  Inferencia y Remuestreo",
    "section": "Comparaciones con lineup 2",
    "text": "Comparaciones con lineup 2\nRepitimos el ejemplo para otra muestra (en este ejemplo el proceso generador de datos es diferente para el grupo b):\n\n\n\n\n\nHacemos primero la prueba del lineup:\n\nset.seed(121)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\ndecrypt(\"Jn3o HPdP I7 LQgIdIQ7 ex\")\n\ngrafica_cuantiles(reps |>  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), \n                             grupo_escondido, x) + facet_wrap(~.sample) + ylab(\"x\") +\n    coord_flip() + geom_point(aes(colour = grupo_escondido))\n\n`summarise()` has grouped output by 'grupo_escondido'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nPodemos distinguir más o menos claramente que está localizada en valores más altos y tiene mayor dispersión. En este caso, como en general podemos identificar los datos, obtenemos evidencia en contra de que los tres grupos tienen distribuciones iguales."
  },
  {
    "objectID": "pruebas-hipotesis.html#prueba-de-permutaciones-para-proporciones",
    "href": "pruebas-hipotesis.html#prueba-de-permutaciones-para-proporciones",
    "title": "4  Inferencia y Remuestreo",
    "section": "Prueba de permutaciones para proporciones",
    "text": "Prueba de permutaciones para proporciones\nVeremos otro ejemplo donde podemos hacer más concreta la idea de distribución nula o de referencia usando pruebas de permutaciones. Supongamos que con nuestra muestra de tomadores de té, queremos probar la siguiente hipótesis nula:\n\nLos tomadores de té en bolsas exclusivamente usan azúcar más a tasas simillares que los tomadores de té suelto (que pueden o no también tomar té en bolsita).\n\nLos datos que obtuvimos en nuestra encuesta, en conteos, son:\n\n\n\n\nte_azucar <- tea |> select(how, sugar) |> \n  mutate(how = ifelse(how == \"tea bag\", \"bolsa_exclusivo\", \"suelto o bolsa\"))\nte_azucar |> group_by(how, sugar) |> tally() |> \n  spread(how, n) |> \n  formatear_tabla()\n\n\n\n \n  \n    sugar \n    bolsa_exclusivo \n    suelto o bolsa \n  \n \n\n  \n    No.sugar \n    81 \n    74 \n  \n  \n    sugar \n    89 \n    56 \n  \n\n\n\n\n\nY en proporciones tenemos que:\n\n\n\n\n \n  \n    how \n    prop_azucar \n    n \n  \n \n\n  \n    bolsa_exclusivo \n    0.52 \n    170 \n  \n  \n    suelto o bolsa \n    0.43 \n    130 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEn este ejemplo, podemos usar una estádistica de prueba numérica, por ejemplo, la diferencia entre las dos proporciones:\n\\[p_1 - p_2\\].\n(tomadores de en bolsa solamente vs. suelto y bolsa). El proceso sería entonces:\n\nLa hipótesis nula es que los dos grupos tienen distribuciones iguales, que este caso quiere decir que en la población, tomadores de té solo en bolsa usan azúcar a las mismas tasas que tomadores de suelto o bolsas.\nBajo nuestra hipótesis nula (proporciones iguales), producimos una cantidad grande (por ejemplo 10 mil o más) de muestras permutando las etiquetas de los grupos.\nEvaluamos nuestra estadística de prueba en cada una de las muestras permutadas.\nEl conjunto de valores obtenidos nos da nuestra distribución de referencia (ya no estamos limitados a 20 replicaciones como en las pruebas gráficas).\nY la pregunta clave es: ¿el valor de la estadística en nuestra muestra es extrema en comparación a la distribución de referencia?\n\n\n# ESta función calcula la diferencia entre grupos de interés\ncalc_diferencia <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(how) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(how, prop_azucar) |> \n    mutate(diferencia_prop = bolsa_exclusivo - `suelto o bolsa`) |> pull(diferencia_prop)\n}\n# esta función hace permutaciones y calcula la diferencia para cada una\npermutaciones_est <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}}:= permutar({{variable}})) |> calc_diferencia()))\n  bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n}\n\nLa diferencia observada es:\n\ndif_obs <- calc_diferencia(te_azucar)\ndif_obs |> round(3)\n\n[1] 0.093\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\nvalores_ref <- permutaciones_est(te_azucar, how, calc_diferencia, n = 10000)\n\nY graficamos nuestros resultados (con un histograma y una gráfica de cuantiles, por ejemplo). la estadística evaluada un cada una de nuestras muestras permutadas:\n\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\nEste es el rango de fluctuación usual para nuestra estadística *bajo la hipótesis de que los dos grupos de tomadores de té consumen té a la misma tasa.\nEl valor que obtuvimos en nuestros datos es 0.0927602, que no es un valor extremo en la distribución de referencia que vimos arriba: esta muestra no aporta mucha evidencia en contra de que los grupos tienen distribuciones similares.\nPodemos graficar otra vez marcando el valor de referencia:\n\n# Función de distribución acumulada (inverso de función de cuantiles)\ndist_perm <- ecdf(valores_ref$diferencia)\n# Calculamos el percentil del valor observado\npercentil_obs <- dist_perm(dif_obs)\n\n\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\") +\n    geom_hline(yintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = 0.3, y = dif_obs - 0.05, label = \"diferencia observada\", colour = \"red\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \") +\n    geom_vline(xintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = dif_obs, y = 2000, label = percentil_obs,vjust = -0.2, colour = \"red\")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\nY vemos que es un valor algo (pero no muy) extremo en la distribución de referencia que vimos arriba: esta muestra no aporta una gran cantidad de evidencia en contra de que los grupos tienen distribuciones similares, que en este caso significa que los dos grupos usan azúcar a tasas similares.\n\nValor p\nNótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un valor p. Podemos calcular, por ejemplo:\n\nValor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la probabilidad de observar una diferencia tan extrema o más extrema de lo que observamos?\n\nConsiderando en este caso interpretamos extrema como que cae lejos de donde a mayoría de la distribución se concentra, podemos calcular el valor p como sigue. A partir de el valor observado, consideramos cuál dato es menor: la probabilidad bajo lo hipótesis nula de observar una diferencia mayor de a que observamos, o la probabilidad de observar una diferencia menor a la que observamos. Tomamos el mínimo y multiplicamos por dos (Hesterberg (2015)):\n\n2 * min(dist_perm(dif_obs), (1 - dist_perm(dif_obs)))\n\n[1] 0.085\n\n\nEste valor p se considera como evidencia “moderada” en contra de la hipótesis nula. Valores p más chicos (observaciones más extremas en comparación con la referencia) aportan más evidencia en contra de la hipótesis de que los grupos de tomadores de té , y valores más grandes aportan menos evidencia."
  },
  {
    "objectID": "pruebas-hipotesis.html#tomadores-de-té-2",
    "href": "pruebas-hipotesis.html#tomadores-de-té-2",
    "title": "4  Inferencia y Remuestreo",
    "section": "Tomadores de té 2",
    "text": "Tomadores de té 2\nAhora hacemos una prueba de permutaciones otro par de proporciones con el mismo método. La hipótesis nula ahora es:\n\nLos tomadores de té Earl Gray usan azúcar a una tasa similar a los tomadores de té negro\n\nLos datos que obtuvimos en nuestra encuesta, en conteos, son: ::: {.cell} ::: {.cell-output-display}\n\n\n \n  \n    sugar \n    black \n    Earl Grey \n  \n \n\n  \n    No.sugar \n    51 \n    84 \n  \n  \n    sugar \n    23 \n    109 \n  \n\n\n\n::: :::\nY en porcentajes tenemos que:\n\nprop_azucar <- te_azucar |> group_by(Tea, sugar) |> tally() |> \n  group_by(Tea) |> mutate(prop = 100 * n / sum(n), n = sum(n)) |> \n  filter(sugar == \"sugar\") |> select(Tea, prop_azucar = prop, n) |> \n  mutate('% usa azúcar' = round(prop_azucar)) |> select(-prop_azucar)\nprop_azucar |> formatear_tabla()\n\n\n\n \n  \n    Tea \n    n \n    % usa azúcar \n  \n \n\n  \n    black \n    74 \n    31 \n  \n  \n    Earl Grey \n    193 \n    56 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEscribimos la función que calcula diferencias para cada muestra:\n\ncalc_diferencia_2 <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(Tea) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(Tea, prop_azucar) |> \n    mutate(diferencia_prop = `Earl Grey` - black) |> pull(diferencia_prop)\n}\n\nLa diferencia observada es:\n\n\n[1] 0.254\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\nset.seed(2)\nvalores_ref <- permutaciones_est(te_azucar, Tea, calc_diferencia_2, n = 10000)\n\nY podemos graficar la distribución de referencia otra vez marcando el valor observado\n\n\n\n\n\n\n\n\nEn este caso, la evidencia es muy fuerte en contra de la hipótesis nula, pues el resultado que obtuvimos es muy extremo en relación a la distribución de referencia. El valor p es cercano a 0."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión",
    "href": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión",
    "title": "4  Inferencia y Remuestreo",
    "section": "Ejemplo: tiempos de fusión",
    "text": "Ejemplo: tiempos de fusión\nConsideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce con información verbal).\nPodemos usar una prueba de permutaciones, esta vez justificándola por el hecho de que los tratamientos se asignan al azar: si los tratamientos son indistinguibles, entonces las etiquetas de los grupos son solo etiquetas, y permutarlas daría muestras igualmente verosímiles.\nEn este caso, compararemos gráficas de cuantiles de los datos con los producidos por permutaciones:\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  n = col_double(),\n  time = col_double(),\n  nv.vv = col_character()\n)\n\n\ndecrypt(\"Jn3o HPdP I7 LQgIdIQ7 ee\")\n\n\n\n\n\nEjercicio: ¿Podemos identificar los datos? En general, muy frecuentemente las personas identifican los datos correctamente, lo que muestra evidencia considerable de que la instrucción verbal altera los tiempos de respuesta de los partipantes, y en este caso ayuda a reducir el tiempo de fusión de los estereogramas."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión-2",
    "href": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión-2",
    "title": "4  Inferencia y Remuestreo",
    "section": "Ejemplo: tiempos de fusión 2",
    "text": "Ejemplo: tiempos de fusión 2\nPodemos usar las pruebas de permutaciones para distintos de tipos de estadísticas: medianas, medias, comparar dispersión usando rangos intercuartiles o varianzas, etc.\nRegresamos a los tiempos de fusión. Podemos hacer una prueba de permutaciones para la diferencia de las medias o medianas, por ejemplo. En este ejemplo usaremos una medida de centralidad un poco diferente, como ilustración: el promedio de los cuartiles superior e inferior de las dos distribuciones. Usaremos el cociente de estas dos cantidades para medir su diferencia\n\nstat_fusion <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\ncalc_fusion <- function(stat_fusion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion(time)) |> \n      spread(nv.vv, est) |> mutate(dif = VV / NV ) |> pull(dif)\n  }\n  fun\n}\n\n\ncalc_cociente <- calc_fusion(stat_fusion)\ndif_obs <- calc_cociente(fusion)\n# permutar\nvalores_ref <- permutaciones_est(fusion, nv.vv, calc_cociente, n = 10000)\ndist_perm_nv <- ecdf(valores_ref$diferencia) \ncuantil_obs <- dist_perm_nv(dif_obs)\n\n\n\n\n\n\nY el valor p de dos colas es\n\ndist_perm_nv <- ecdf(valores_ref$diferencia)\n2 * min(dist_perm_nv(dif_obs), 1- dist_perm_nv(dif_obs))\n\n[1] 0.0354\n\n\nLo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas: la caja del diagrama de caja y brazos para el grupo VV está encogida por un factor menor a 1."
  },
  {
    "objectID": "pruebas-hipotesis.html#ojo-otros-tipos-de-hipótesis-nulas",
    "href": "pruebas-hipotesis.html#ojo-otros-tipos-de-hipótesis-nulas",
    "title": "4  Inferencia y Remuestreo",
    "section": "Ojo: otros tipos de hipótesis nulas",
    "text": "Ojo: otros tipos de hipótesis nulas\nLa pruebas de permutaciones son más útiles cuando nuestra hipótesis nula se refiere que la distribución de los grupos son muy similares, o la independencia entre observaciones y grupo. Esto también aplica cuando queremos probar por ejemplo, que una variable numérica Y es independiente de X.\n\nHay algunas hipótesis que no se pueden probar con este método, como por ejemplo, las que se refieren a una sola muestra: ¿los datos son consistentes con que su media es igual a 5?\nAdicionalmente, en algunas ocasiones queremos probar aspectos más específicos de las diferencias: como ¿son iguales las medias o medianas de dos grupos de datos? ¿Tienen dispersión similar?\n\nLas pruebas de permutaciones no están tan perfectamente adaptadas a este problema, pues prueban todos los aspectos de las distribuciones que se comparan, aún cuando escogamos una estadística particular que pretende medir, por ejemplo, diferencia de medias. Eso quiere decir que podemos rechazar igualdad de medias, por ejemplo, cuando en realidad otra característica de las distribuciones es la que difiere mucho en las poblaciones\nEn algunas referencias (ver (chitim?), Efron y Tibshirani (1993)) se argumenta que de todas formas las pruebas de permutaciones son relativamente robustas a esta desadaptación. Un caso excepcional, por ejemplo, es cuando las poblaciones que comparamos resultan tener dispersión extremadamente distinta, y adicionalmente los tamaños de muestra de los grupos son muy desiguales (otra vez, ver ejemplos en (chitim?))."
  },
  {
    "objectID": "pruebas-hipotesis.html#separación-de-grupos",
    "href": "pruebas-hipotesis.html#separación-de-grupos",
    "title": "4  Inferencia y Remuestreo",
    "section": "Separación de grupos",
    "text": "Separación de grupos\nEste ejemplo tomado de Chowdhury et al. (2015) (tanto la idea como el código). La pregunta que se aborda en ese estudio es:\n\nExisten métodos de clasificación (supervisados o no supervisados) para formar grupos en términos de variables que describen a los individuos\nEstos métodos (análisis discriminante, o k-means, por ejemplo), pretenden formar grupos compactos, bien separados entre ellos. Cuando aplicamos el método, obtenemos clasificadores basados en las variables de entrada.\nLa pregunta es: ¿los grupos resultantes son producto de patrones que se generalizan a la población, o capitalizaron en variación aleatoria para formarse?\nEspecialmente cuando tenemos muchas mediciones de los individuos, y una muestra relativamente chica, Es relativamente fácil encontrar combinaciones de variables que separan los grupos, aunque estas combinaciones y diferencias están basadas en ruido y no generalizan a la población.\n\nComo muestran en Chowdhury et al. (2015), el lineup es útil para juzgar si tenemos evidencia en contra de que los grupos en realidad son iguales, y usamos variación muestral para separarlos.\n\nAvispas (opcional)\nEn el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total), y para cada individuo se miden expresiones de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas dependiendo de sus mediciones?\nEn este se usó análisis discriminante para buscar proyecciones de los datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles.\nPara probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos LDA y observamos los resultados.\n\n\n\n\n\nY vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos bien definidos: la búsqueda es suficientemente agresiva para encontrar combinaciones lineales que los separan. Que no podamos distinguir los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede servir para separar los grupos claramente.\nOtro enfoque sería separar los datos en una muestra de entrenamiento y una de prueba (que discutiremos en la última sesión). Aplicamos el procedimiento a la muestra de entrenamiento y luego vemos qué pasa con los datos de prueba:\n\nset.seed(8)\nwasps_1 <- wasps |> mutate(u = runif(nrow(wasps), 0, 1))\nwasps_entrena <- wasps_1 |> filter(u <= 0.8)\nwasps_prueba <- wasps_1 |> filter(u > 0.8)                            \n                            \nwasp.lda <- MASS::lda(Group ~ ., data=wasps_entrena[,-1])\nwasp_ld_entrena <- predict(wasp.lda,  dimen=2)$x |> \n    as_tibble(.name_repair = \"universal\") |>\n     mutate(tipo = \"entrenamiento\") |> \n    mutate(grupo = wasps_entrena$Group)\nwasp_ld_prueba <- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x  |> \n    as_tibble(.name_repair = \"universal\") |>\n    mutate(tipo = \"prueba\")|> \n    mutate(grupo = wasps_prueba$Group)\nwasp_lda <- bind_rows(wasp_ld_entrena, wasp_ld_prueba)\nggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) +\n    facet_wrap(~tipo) + scale_color_colorblind()\n\n\n\n\nAunque esta separación de datos es menos efectiva en este ejemplo por la muestra chica, podemos ver que la separación lograda en los datos de entrenamiento probablemente se debe a variación muestral."
  },
  {
    "objectID": "pruebas-hipotesis.html#la-crisis-de-replicabilidad",
    "href": "pruebas-hipotesis.html#la-crisis-de-replicabilidad",
    "title": "4  Inferencia y Remuestreo",
    "section": "La “crisis de replicabilidad”",
    "text": "La “crisis de replicabilidad”\nRecientemente (Ioannidis (2005)) se ha reconocido en campos como la sicología la crisis de replicabilidad. Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados posteriormente por otros investigadores. Por ejemplo:\n\nHacer poses poderosas produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas\nMostrar palabras relacionadas con “viejo” hacen que las personas caminen más lento (efectos de priming)\n\nEn todos estos casos, el argumento de la evidencia de estos efectos fue respaldada por una prueba de hipótesis nula con un valor p menor a 0.05. La razón es que ese es el estándar de publicación seguido por varias áreas y revistas. La tasa de no replicabilidad parece ser mucho más alta (al menos la mitad o más según algunas fuentes, como la señalada arriba) que lo sugeriría la tasa de falsos positivos (menos de 5%)\nEste problema de replicabilidad parece ser más frecuente cuando:\n\nSe trata de estudios de potencia baja: mediciones ruidosas y tamaños de muestra chicos.\nEl plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando se están investigando “fenómenos no estudiados antes”)\n\n¿A qué se atribuye esta crisis de replicabilidad?"
  },
  {
    "objectID": "pruebas-hipotesis.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "href": "pruebas-hipotesis.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "title": "4  Inferencia y Remuestreo",
    "section": "El jardín de los senderos que se bifurcan",
    "text": "El jardín de los senderos que se bifurcan\nAunque haya algunos ejemplos de manipulaciones conscientes –e incluso, en menos casos, malintencionadas– para obtener resultados publicables o significativos (p-hacking), como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando estamos buscando las comparaciones correctas. Algunas pueden ser:\n\nTransformar los datos (tomar o no logaritmos, u otra transformación)\nEditar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)\nDistintas maneras de interpretar los criterios de inclusión de un estudio (por ejemplo, algunos participantes mostraron tener gripa, o revelaron que durmieron muy poco la noche anterior, etc. ¿los dejamos o los quitamos?)\n\nDado un conjunto de datos, las justificaciones de las decisiones que se toman en cada paso son razonables, pero con datos distintos las decisiones podrían ser diferentes. Este es el jardín de los senderos que se bifurcan Gelman, que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.\nEsto es exacerbado por:\n\nTamaños de muestra chicos y efectos “inestables” que se quieren medir (por ejemplo en sicología)\nEl hecho de que el criterio de publicación es obtener un valor p < 0.05, y la presión fuerte sobre los investigadores para producir resultados publicables (p < 0.05)\nEl que estudios o resultados similares que no obtuvieron valores \\(p\\) por debajo del umbral no son publicados o reportados.\n\nVer por ejemplo el comunicado de la ASA.\nOjo: esas presiones de publicación no sólo ocurre para investigadores en sicología. Cuando trabajamos en problemas de análisis de datos en problemas que son de importancia, es común que existan intereses de algunas partes o personas involucradas por algunos resultados u otros (por ejemplo, nuestros clientes de consultoría o clientes internos). Eso puede dañar nuestro trabajo como analistas, y el avance de nuestro equipo. Aunque esas presiones son inevitables, se vuelven manejables cuando hay una relación de confianza entre las partes involucradas."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "href": "pruebas-hipotesis.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "title": "4  Inferencia y Remuestreo",
    "section": "Ejemplo: decisiones de análisis y valores p",
    "text": "Ejemplo: decisiones de análisis y valores p\nEn el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como ilustración. Ahora intentamos usar distintas mediciones de la diferencia entre los grupos, usando distintas medidas resumen y transformaciones (por ejemplo, con o sin logaritmo). Aquí hay unas 12 combinaciones distintas para hacer el análisis (multiplicadas por criterios de “aceptación de datos en la muestra”, que simulamos tomando una submuestra al azar):\n\ncalc_fusion <- function(stat_fusion, trans, comparacion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion({{ trans }}(time))) |> \n      spread(nv.vv, est) |> mutate(dif = {{ comparacion }}) |> pull(dif)\n  }\n  fun\n}\nvalor_p <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}} := permutar({{variable}})) |> calc_diferencia()))\n  perms <- bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n  perms_ecdf <- ecdf(perms$diferencia)\n  dif <- calc_diferencia(datos)\n  2 * min(perms_ecdf(dif), 1- perms_ecdf(dif))\n}\n\n\nset.seed(7272)\nmedia_cuartiles <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\n# nota: usar n=10000 o más, esto solo es para demostración:\ncalc_dif <- calc_fusion(mean, identity, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.072\n\ncalc_dif <- calc_fusion(mean, log, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.024\n\ncalc_dif <- calc_fusion(median, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.016\n\ncalc_dif <- calc_fusion(media_cuartiles, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.026\n\n\nSi existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces los valores p pueden tener poco significado."
  },
  {
    "objectID": "pruebas-hipotesis.html#alternativas-o-soluciones",
    "href": "pruebas-hipotesis.html#alternativas-o-soluciones",
    "title": "4  Inferencia y Remuestreo",
    "section": "Alternativas o soluciones",
    "text": "Alternativas o soluciones\nEl primer punto importante es reconocer que la mayor parte de nuestro trabajo es exploratorio (recordemos el proceso complicado del análisis de datos de refinamiento de preguntas). En este tipo de trabajo, reportar valores p puede tener poco sentido, y mucho menos tiene sentido aceptar algo “verdadero” cuando pasa un umbral de significancia dado.\nNuestro interés principal al hacer análisis es expresar correctamente y de manera útil la incertidumbre asociada a las conclusiones o patrones que mostramos (asociada a variación muestral, por ejemplo) para que el proceso de toma de decisiones sea informado. Un resumen de un número (valor p, o el que sea) no puede ser tomado como criterio para tomar una decisión que generalmente es compleja. En la siguiente sección veremos cómo podemos mostrar parte de esa incertidumbre de manera más útil.\nPor otra parte, los estudios confirmatorios (donde se reportan valores p) también tienen un lugar. En áreas como la sicología, existen ahora movimientos fuertes en favor de la repetición de estudios prometedores pero donde hay sospecha de grados de libertad del investigador. Este movimiento sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos.\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York.\n\n\nChowdhury, Niladri Roy, Dianne Cook, Heike Hofmann, Mahbubul Majumder, Eun-Kyung Lee, y Amy L Toth. 2015. «Using visual statistical inference to better understand random class separations in high dimension, low sample size data». Computational Statistics 30 (2): 293-316.\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86.\n\n\nIoannidis, John PA. 2005. «Why most published research findings are false». PLoS medicine 2 (8): e124.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, y Andreas Buja. 2010. «Graphical inference for infovis». IEEE Transactions on Visualization and Computer Graphics 16 (6): 973-79.\n\n\nWickham, H, NR Chowdhury, y D Cook. 2012. «nullabor: Tools for graphical inference». R package version 0.2 1: 213."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Ejemplo: estimación e intervalos de confianza",
    "text": "Ejemplo: estimación e intervalos de confianza\nRegresamos a nuestro ejemplo anterior donde muestreamos 3 grupos, y nos preguntábamos acerca de la diferencia de sus medianas. En lugar de hacer pruebas de permutaciones (con gráficas o numéricas), podríamos considerar qué tan precisa es cada una de nuestras estimacione para las medianas de los grupos, por ejemplo.\nNuestros resultados podríamos presentarlos como sigue:\n\n\n\n\n\nDonde en rojo está nuestro estimador puntual de la mediana de cada grupo (la mediana muestral), y las rectas mustran un intervalo de 95% para nuestra estimación de la mediana: esto quiere decir que los valores poblacionales tienen probabilidad aproximada de 95% de estar dentro del intervalo.\nEste análisis comunica correctamente que tenemos incertidumbre alta acerca de nuestras estimaciones (especialmente grupos b y c), y que no tenemos mucha evidencia de que el grupo b tenga una mediana poblacional considerablemente más alta que a o c."
  },
  {
    "objectID": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Interpretación de intervalos de confianza",
    "text": "Interpretación de intervalos de confianza\nGeneralmente, “intervalo de confianza” (de 90% de confianza, por ejemplo) significa, desde el punto de vista frecuentista:\n\nCada muestra produce un intervalo distinto. Para el 90% de las muestras posibles, el intervalo cubre al valor poblacional.\nAsí que con alta probabilidad, el valor poblacional está dentro del intervalo.\nIntervalos más anchos nos dan más incertidumbre acerca de dónde está el verdadero valor poblacional (y al revés para intervalos más angostos)\n\nExisten también “intervalos creíbles” (de 90% de probabilidad, por ejemplo), que se interpetan de forma bayesiana:\n\nCon alta probabilidad, creemos que el valor poblacional está dentro del intervalo creíble.\n\nLa técnica que veremos a continuación (bootstrap) se puede interpretar de las dos maneras.\n\nLa interpretación bayesiana puede ser más natural\nLa interpretación frecuentista nos da maneras empíricas de probar si los intervalos de confianza están bien calibrados o no: es un mínimo que “intervalos del 90%” debería satisfacer.\n\nAsí que tomamos el punto de vista bayesiano en la intepretación, pero buscamos que nuestros intervalos cumplan o aproximen bien garantías frecuentistas (discutimos esto más adelante)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "href": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Cómo producir intervalos para estimación",
    "text": "Cómo producir intervalos para estimación\nExisten muchas técnicas para construir estos intervalos que muestran la incertidumbre en nuestras estimaciones: métodos basados en distribuciones estándar, métodos paramétricos y no paramétricos, distintos métodos bayesianos (entonces se llaman intervalos creíbles o de probabilidad), etc.\nEn este curso, como ejemplo, y también por ser una técnica versátil, presentaremos el bootstrap no paramétrico (ver Efron y Tibshirani (1993)), donde utilizaremos simulación (y poder de cómputo) para producir este tipo de intervalos, bajo ciertas condiciones de extracción de la muestra que discutiremos más adelante."
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Distribución de Muestreo",
    "text": "Distribución de Muestreo\nSupongamos que consideramos la población de casas de nuestro ejemplo anterior\n\ncasas_pob <- casas |> select(id, precio_miles, nombre_zona)\ncasas_pob |> sample_n(20) |> formatear_tabla()\n\n\n\n \n  \n    id \n    precio_miles \n    nombre_zona \n  \n \n\n  \n    721 \n    275.0 \n    StoneBr \n  \n  \n    237 \n    185.5 \n    CollgCr \n  \n  \n    270 \n    148.0 \n    Edwards \n  \n  \n    269 \n    120.5 \n    IDOTRR \n  \n  \n    362 \n    145.0 \n    BrkSide \n  \n  \n    1152 \n    149.9 \n    Edwards \n  \n  \n    973 \n    99.5 \n    SawyerW \n  \n  \n    1037 \n    315.5 \n    Timber \n  \n  \n    1092 \n    160.0 \n    Somerst \n  \n  \n    1286 \n    132.5 \n    BrkSide \n  \n  \n    1208 \n    200.0 \n    CollgCr \n  \n  \n    540 \n    272.0 \n    CollgCr \n  \n  \n    367 \n    159.0 \n    NAmes \n  \n  \n    283 \n    207.5 \n    NridgHt \n  \n  \n    52 \n    114.5 \n    BrkSide \n  \n  \n    761 \n    127.5 \n    NAmes \n  \n  \n    1297 \n    155.0 \n    NAmes \n  \n  \n    822 \n    93.0 \n    OldTown \n  \n  \n    956 \n    145.0 \n    Crawfor \n  \n  \n    790 \n    187.5 \n    ClearCr \n  \n\n\n\n\n\nY nos interesa saber, para la población, cuál es la mediana de los precios de casas. Suponemos que no tenemos acceso a los datos poblacionales, y decidimos diseñar una encuesta para tomar una muestra de 50 casas que fueron vendidas en cierto periodo. Suponemos una muestra aleatoria simple con reemplazo (la población es grande y no hay mucha diferencia entre hacerlo con o sin reemplazo) de tamaño fijo, por ejemplo \\(n = 50\\)\nBuscamos estimar la mediana poblacional con la mediana de nuestra muestra:\n\nfun_muestra <- function(x){\n    median(x)\n}\n\nComo es de esperarse, distintas muestras dan distintas estimaciones de la mediana\n\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     156\n\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     145\n\n\nEn estimación, uno de los conceptos básicos el de la distribución de muestreo. La distribución de muestreo son los valores que puede tomar nuestro estimador bajo todas las posibles muestras que pudiéramos obtener.\n¿Por qué es importante este concepto? La distribución de muestreo del estimador nos indica qué tan lejos o cerca vamos a caer del verdadero valor poblacional que queremos estimar. No sabemos qué muestra vamos a obtener, pero con la distribución de muestreo podemos saber qué tan mal o bien nos puede ir y con qué probabilidades."
  },
  {
    "objectID": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Aproximando la distribución de muestreo",
    "text": "Aproximando la distribución de muestreo\nEn nuestro ejemplo tenemos la población (esto normalmente no es cierto) y podemos extraer un número muy grande de muestras de tamaño 50. Calculamos el estimador para cada una de esas muestras. El código es simple:\n\n# Repetir 5000 veces\nmediana_muestras <- map_dbl(1:5000, ~ casas_pob |> \n    sample_n(50, replace = T) |>  # muestra de 50\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) # calcular mediana de la muestra\n\nAhora examinamos la distribución de los valores que obtuvimos:\n\nsims_dm <- tibble(muestra = 1:length(mediana_muestras), mediana_precio = mediana_muestras)\nvalor_poblacional <- median(casas$precio_miles) \nggplot(sims_dm, aes(sample = mediana_muestras)) + geom_qq(distribution = stats::qunif) +\n    ylab(\"Mediana muestral\") + xlab(\"f\") + labs(subtitle = \"Distribución de muestreo para mediana (n = 50)\") +\n    geom_hline(yintercept = valor_poblacional, colour = \"red\") +\n    annotate(\"text\", x = 0.2, y = valor_poblacional+5, label = \"Mediana poblacional\", colour = \"red\")\n\n\n\n\n\nCon esta gráfica podemos juzgar qué tan lejos puede caer nuestra estimación muestral del valor poblacional. Cuanto más concentrada esté alrededor del valor poblacional, la probabilidad es más alta de que obtengamos una estimación precisa cuando tomemos una muestra particular. Podemos hacer un histograma también:\n\n\n\n\n\n\nLos cuantiles que cubren a un 95% de las muestras son:\n\nquantile(mediana_muestras - valor_poblacional , c(0.025, 0.975)) |> round(1)\n\n 2.5% 97.5% \n-21.5  21.9 \n\n\n\nEsto quiere decir que si estimamos con una muestra el valor poblacional, esperamos con 95% de probabilidad que el error sea menos de unas 20 unidades. *Esta es la precisión de nuestro estimador.\n\nSi usamos una muestra más grande (n = 200, por ejemplo) podemos obtener un resultado más preciso:\n\n\n\n\n\nY como es de esperarse, vemos que muestras más grandes resultan en menos variablidad, y menor error de estimación.\n\nMejores distribuciones de muestreo: más concentradas alrededor del verdadero valor poblacional"
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Distribución de muestreo y distribución poblacional",
    "text": "Distribución de muestreo y distribución poblacional\nUna confusión inicial que es común es entre la distribución de muestreo y la distribución poblacional. La poblacional muestra cómo se distribuyen los valores de la variable de interés:\n\nggplot(casas_pob, aes(x = precio_miles)) + geom_histogram() +\n    geom_vline(xintercept = valor_poblacional)\n\n\n\n\nQue es muy diferente que las distribuciones de muestreo de nuestros dos estimadores:\n\ng_dist_muestreo"
  },
  {
    "objectID": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "href": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "title": "5  Remuestreo: el bootstrap",
    "section": "El mundo bootstrap",
    "text": "El mundo bootstrap\nEl problema que tenemos ahora es que normalmente sólo tenemos una muestra, así que no es posible calcular las distribuciones de muestreo como hicimos arriba. Sin embargo, podemos hacer lo siguiente:\n\nSi tuviéramos la distribución poblacional, simulamos muestras para aproximar la distribución de muestreo de nuestro estimador, y así entender su variabilidad.\nPero no tenemos la distribución poblacional\nSin embargo, podemos estimar la distribución poblacional con nuestros valores muestrales\n\nMundo bootstrap\n\nSi usamos la estimación del inciso anterior, entonces usando 1 podríamos tomar muestras de nuestros datos muestrales, como si fueran de la población, y usando el mismo tamaño de muestra. El muestreo lo hacemos con reemplazo, como la muestra original.\nA la distribución resultante le llamamos distribución bootstrap de la muestra\nUsamos la distribución bootstrap de la muestra para estimar la variabilidad en nuestra estimación con la muestra original.\n\nVeamos que sucede para un ejemplo concreto. Primero extraemos nuestra muestra:\n\nset.seed(2112)\nmuestra <- sample_n(casas_pob, 150, replace = T)\n\nEsta muestra nos da nuestro estimador de la distribución poblacional:\n\nbind_rows(muestra |> mutate(tipo = \"muestra\"),\n    casas_pob |> mutate(tipo = \"población\")) |> \nggplot(aes(sample = precio_miles, colour = tipo, group = tipo)) + \n    geom_qq(distribution = stats::qunif, alpha = 0.7, size = 2) + \n    scale_color_colorblind()\n\n\n\n\nY vemos que la aproximación es razonable, especialmente en las partes centrales de la distribución. Usamos nuestra muestra para estimar la población.\nPara evaluar ahora la variabilidad de nuestro estimador, podemos extraer un número grande de muestras con reemplazo de tamaño 150 de la muestra - estamos en el mundo Bootstrap!\n\nmediana_muestras <- map_dbl(1:5000, ~ muestra |>  \n    sample_n(150, replace = T) |>\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) \n\nY nuestra estimación de la distribución de muestreo es entonces:\n\nbootstrap <- tibble(mediana = mediana_muestras)\nggplot(bootstrap, aes(sample = mediana)) + geom_qq(distribution = stats::qunif)\n\n\n\n\nY podemos calcular ahora un intervalo de confianza del 90% simplemente calculando los cuantiles de esta distribución (no son los cuantiles de la muestra original!):\n\nlimites_ic <- quantile(mediana_muestras, c(0.05,  0.95)) |> round()\nlimites_ic\n\n 5% 95% \n154 173 \n\n\nPresentaríamos nuestro resultado como sigue: nuestra estimación puntual de la mediana es 165, con un intervalo de confianza del 90% de (154, 173)"
  },
  {
    "objectID": "remuestreo-bootstrap.html#experimento-de-simulación",
    "href": "remuestreo-bootstrap.html#experimento-de-simulación",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Experimento de simulación",
    "text": "Experimento de simulación\nEn nuestro ejemplo, podemos ver varias muestras (por ejemplo 20) de tamaño 100, y vemos cómo se ve la aproximación a la distribución de la población:\n\n\n\n\n\nPodemos calcular las distribuciones de remuestreo para cada muestra bootstrap, y compararlas con la distribución de muestreo real."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "href": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Cobertura nominal y cobertura real",
    "text": "Cobertura nominal y cobertura real\n¿Cómo sabemos que la cobertura nominal del 90% es cercana a la realidad? Sería muy malo que los intervalos fueran demasiado anchos (exageramos la variabilidad) o demasiado angostos (damos la idea de que nuestra estimación es más precisa de lo que realmente es). Que esto se cumpla depende de:\n\nCuál es la estadística de interés\nCómo es la población\nEl tamaño de muestra y otros aspectos del muestreo\n\nVarias observaciones útiles se pueden consultar en Hesterberg (2015) y en Efron y Tibshirani (1993) (por ejemplo, el bootstrap no funciona bien para estadísticas como el mínimo o el máximo). En estas referencias también pueden consultarse recomendaciones de cómo mejorar intervalos basados en boostrap - los que vimos se llaman intervalos de percentiles, pero hay más opciones simples que se desempeñan mejor en ciertos casos.\nY siempre podemos hacer ejercicios de simulación bajo ciertos supuestos acerca de la población para una estadística dada, y estimar empíricamente si la cobertura es adecuada. \n\nEjemplo\nConstruimos para nuestra población varias muestras bootstrap con sus respectivos intervalos de cuantiles. ¿Qué porcentaje de veces cubrimos al verdadero valor?\n\n\n\n\n#rep_remuestreo <- map(1:200, ~ muestras_boot(.x, B = 2000, fun_muestra = fun_muestra)) |> bind_rows()\nrep_remuestreo <- read_csv(\"./datos/bootstrap_reps.csv\")\n\nCon nuestras muestras, checamos ahora nuestros intervalos y su cobertura\n\nintervalos <- rep_remuestreo |> \n    group_by(n, rep) |> \n    summarise(inf =  quantile(mediana, 0.05), sup = quantile(mediana, 0.95)) |> \n    mutate(valor_poblacional = median(casas_pob$precio_miles))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\n\nggplot(intervalos, aes(x = rep, ymin = inf, ymax = sup)) + \n    geom_hline(yintercept = median(casas_pob$precio_miles), colour = \"salmon\") +\n    geom_linerange(alpha = 0.7) +\n    facet_wrap(~n) \n\n\n\n\nLa cobertura para nuestros intervalos es:\n\nintervalos |> mutate(cubre = valor_poblacional > inf & valor_poblacional < sup) |> \n     group_by(n) |> summarise(cobertura = mean(cubre), \n                               ee_cobertura = (sd(cubre) /sqrt(n())) |> round(3)) \n\n# A tibble: 2 × 3\n      n cobertura ee_cobertura\n  <dbl>     <dbl>        <dbl>\n1    50     0.895        0.022\n2   150     0.915        0.02 \n\n\nPara este número de repeticiones, estos números son consistentes con la cobertura nominal de 90%."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "href": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Ejemplo: estereogramas",
    "text": "Ejemplo: estereogramas\nEn este caso, queremos hacer inferencia sobre la diferencia de tiempo de reconocimiento de los grupos. Como discutimos antes, preferimos hacer comparaciones multiplicativas. En este caso particular, compararemos el cociente de las medias:\nPodemos adaptar el bootstrap en este caso para dos grupos: hacemos remuestreo de cada grupo, comparamos diferencias, y repetimos\n\nfusion <- read_table(\"./datos/fusion_time.txt\")\nmuestra_boot <- function(datos, grupo, medicion, fun_muestra, comparacion){\n    est_boot <- datos |> group_by({{ grupo }}) |> \n      sample_n(n(), replace = T) |> \n      summarise(est = fun_muestra( {{ medicion }})) |> \n      spread(nv.vv, est) |> \n      mutate(comp = {{ comparacion }}) |> \n      pull(comp)\n    est_boot\n}\nmuestra_boot(fusion, nv.vv, time, median, VV / NV) |> round(2)\n\n[1] 0.59\n\n\nLa distribución de remuestreo es:\n\nreps_boot <- map_dbl(1:2000, ~ muestra_boot(fusion, nv.vv, time, mean, VV / NV))\nggplot(tibble(cociente_boot = reps_boot), aes(sample = reps_boot)) +\n  geom_qq(distribution = stats::qunif) + xlab(\"f\") + ylab(\"Cociente\")\n\n\n\n\ny un intervalo de 90% sería:\n\nquantile(reps_boot, c(0.05, 0.95)) |> round(2)\n\n  5%  95% \n0.46 0.91 \n\n\nY esta sería una forma de presentar nuestros resultados: hay probabilidad considerable de que el efecto de este tratamiento sea marginal (una reducción de 10%), aunque lo más probables es que tenga un efecto consdierable (reducción alrededor de 60% del tiempo de fusión)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "href": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Ventajas y desventajas del bootstrap",
    "text": "Ventajas y desventajas del bootstrap\n\nEl bootstrap es una técnica versátil generalmente fácil de implementar (ventaja) - especialmente cuando a algún nivel podemos suponer que las muestras son idependientes e idénticamente distribuidas (desventaja).\n\nPor ejemplo: en muestreo estratificado, podemos hacer bootstrap sobre cada estrato por separado. En muestreo complejo, podemos hacer bootstrap de unidades primarias de muestreo, etc.\n\nRequiere más cómputo que fórmulas estándar (desventaja), pero tenemos flexibilidad (ventaja) para aplicar en estadísticas diferentes de manera muchas veces trivial (ventaja).\nEs una técnica estándar en el análisis de datos que se usa en un rango grande de aplicaciones (ventaja).\nEn el caso de muestras chicas y ciertas distribuciones poblacionales, los intervalos bootstrap de percentiles que vimos aquí pueden ser un poco angostos y no cumplir la cobertura nominal por ejemplo, si la muestra es de tamaño < 40. la cobertura puede ser de 90% en lugar de 95% en algunos casos (población normal, o de 80% en lugar de 95% en una poblacion exponencial), ver Hesterberg (2015)). Hay mejores opciones en estos casos (por ejempo, intervalos bootstrap-t, que se calculan fácilmente también).\nFinalmente, en casos donde tenemos la población total, o el supuesto de muestras aleatorias es dudoso, lo podemos utilizar más informalmente como un análisis de sensibilidad de nuestros resultados. Es una perturbación a los datos (que podemos combinar con otros tipos de perturbaciones) para juzgar qué tan fuertemente depende nuestro análisis de los datos que tenemos a mano."
  },
  {
    "objectID": "remuestreo-bootstrap.html#sesgo",
    "href": "remuestreo-bootstrap.html#sesgo",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Sesgo",
    "text": "Sesgo\nAlgunos estimadores comunes (por ejemplo, cociente de dos cantidades aleatorias) pueden sufrir de **sesgo* grande, especialmente en el caso de muestras chicas. Esto afecta la cobertura, pues es posible que nuestros intervalos no tengan “cobertura simétrica”, por ejemplo. Para muchos estimadores, y muestras no muy chicas, esté sesgo tiende a ser poco importante y no es necesario hacer correcciones.\nPodemos evaluar el sesgo comparando la media de nuestras replicaciones bootstrap con el valor muestral que obtuvimos (para estadísticas funcionales, ver Hesterberg (2015)). Si el tamaño del sesgo es chico comparado con la dispersión de la distribución bootstrap (por ejemplo, menos de 20% de la desviación estándar, Efron y Tibshirani (1993)), no es muy importante hacer correcciones.\nEn caso de que esta cantidad sea relativamente grande en relación a la dispersión de la distribución bootstrap, hay variantes los intervalos bootstrap de percentiles que mejoran esta situación (Efron y Tibshirani (1993))."
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Bootstrap y estimadores complejos: suavizadores",
    "text": "Bootstrap y estimadores complejos: suavizadores\nEl bootstrap es una técnica versátil. Por ejemplo, podemos usarlo para juzgar la variabilidad de un suavizador:\n\ngraf_casas <- function(data){\n    ggplot(data |> filter(calidad_gral < 7), \n        aes(x = area_habitable_sup_m2)) + \n        geom_point(aes(y = precio_m2_miles), alpha = 0.75) +\n        geom_smooth(aes(y = precio_m2_miles), method = \"loess\", span = 0.7, \n                se = FALSE, method.args = list(degree = 1, family = \"symmetric\"))     \n}\nset.seed(250)\ncasas_muestra <- sample_frac(casas, 0.2)\ngraf_casas(casas_muestra)\n\n\n\n\nPodemos hacer bootstrap para juzgar la estabilidad del suavizador:\n\nsuaviza_boot <- function(x, data){\n    # remuestreo\n    muestra_boot <- sample_n(data, nrow(data), replace = T)\n    ajuste <- loess(precio_m2_miles ~ area_habitable_sup_m2, data = muestra_boot, \n                    degree = 1, span = 0.7, family = \"symmetric\")\n    datos_grafica <- tibble(area_habitable_sup_m2 = seq(25, 250, 5))\n    ajustados <- predict(ajuste, newdata = datos_grafica)\n    datos_grafica |> mutate(ajustados = ajustados) |> \n        mutate(rep = x)\n}\nreps <- map(1:10, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n\n\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 1, colour = \"red\") \n\n\n\n\nDonde vemos que algunas cambios de pendiente del suavizador original no son muy interpretables (por ejemplo, para áreas chicas) y alta variabilidad en general en los extremos. Podemos hacer más iteraciones para calcular bandas de confianza:\n\nreps <- map(1:200, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 0.2, colour = \"red\")"
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "title": "5  Remuestreo: el bootstrap",
    "section": "Bootstrap y estimadores complejos: tablas de perfiles",
    "text": "Bootstrap y estimadores complejos: tablas de perfiles\nPodemos regresar al ejemplo de la primera sesión donde calculamos perfiles de los tomadores de distintos tés: en bolsa, suelto, o combinados:\n\n\n\n\n\n\n\n\n\n\n \n  \n    price \n    tea bag \n    tea bag+unpackaged \n    unpackaged \n    promedio \n  \n \n\n  \n    p_upscale \n    -0.71 \n    -0.28 \n    0.98 \n    28 \n  \n  \n    p_variable \n    -0.12 \n    0.44 \n    -0.31 \n    36 \n  \n  \n    p_cheap \n    0.3 \n    -0.53 \n    0.23 \n    2 \n  \n  \n    p_branded \n    0.62 \n    -0.16 \n    -0.45 \n    25 \n  \n  \n    p_private label \n    0.72 \n    -0.22 \n    -0.49 \n    5 \n  \n  \n    p_unknown \n    1.58 \n    -0.58 \n    -1 \n    3 \n  \n\n\n\n\n\n\n\n\n\n\nHacemos bootstrap sobre toda la muestra, y repetimos exactamente el mismo proceso de construción de perfiles:\n\nboot_perfiles <- map(1:1000, function(x){\n    te_boot <- te |> sample_n(nrow(te), replace = TRUE)\n    calcular_perfiles(te_boot) |> mutate(rep = x)\n}) |> bind_rows()\n\nAhora resumimos y graficamos, esta vez de manera distinta:\n\nresumen_perfiles <- boot_perfiles |> group_by(how, price) |> \n    summarise(perfil_media = mean(perfil), ymax = quantile(perfil, 0.9), ymin = quantile(perfil, 0.10)) \n\n`summarise()` has grouped output by 'how'. You can override using the `.groups`\nargument.\n\nresumen_bolsa <- resumen_perfiles |> ungroup() |> \n    filter(how == \"tea bag\") |> select(price, perfil_bolsa = perfil_media)\nresumen_perfiles <- resumen_perfiles |> left_join(resumen_bolsa) |> \n    ungroup() |> \n    mutate(price = fct_reorder(price, perfil_bolsa))\n\nJoining, by = \"price\"\n\nggplot(resumen_perfiles, aes(x = price, y = perfil_media, ymax = ymax, ymin = ymin)) + \n    geom_point(colour = \"red\") + geom_linerange() +\n    facet_wrap(~how) + coord_flip() +\n    geom_hline(yintercept = 0, colour = \"gray\") + ylab(\"Perfil\") + xlab(\"Precio\")\n\n\n\n\nNótese una deficiencia clara del bootstrap: para los que compran té suelto, en la muestra no existen personas que desconocen de dónde provienen su té (No sabe/No contestó). Esto produce un intervalo colapsado en 0 que no es razonable.\nPodemos remediar esto de varias maneras: quitando del análisis los que no sabe o no contestaron, agrupando en otra categoría, usando un modelo, o regularizar usando proporciones calculadas con conteos modificados: por ejemplo, agregando un caso de cada combinación (agregaría 18 personas “falsas” a una muestra de 290 personas).\n\n\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86."
  },
  {
    "objectID": "probabilidad.html",
    "href": "probabilidad.html",
    "title": "Introducción a teoría de probabilidades",
    "section": "",
    "text": "Introducción"
  },
  {
    "objectID": "modelo-prob.html",
    "href": "modelo-prob.html",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "",
    "text": "En esta parte comenzaremos a tratar el concepto de probabilidad de ocurrencia de eventos acerca de los que tenemos incertidumbre. Veremos que:"
  },
  {
    "objectID": "modelo-prob.html#equiprobabilidad-y-simetría",
    "href": "modelo-prob.html#equiprobabilidad-y-simetría",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.1 Equiprobabilidad y simetría",
    "text": "6.1 Equiprobabilidad y simetría\nHistóricamente, el concepto de probabilidad nació en el contexto de juegos de azar, y cómo definir apuestas ajustas o equitativas. Por ejemplo:\n\nSupongamos que nos cuesta 1 peso entrar a un juego de dados, y que ganamos si tiramos un 3. ¿Cuánto dinero deberíamos ganar si sale un 3 para que el costo de entrada sea justo?\n\nEl argumento iría como sigue: existen 6 posibles resultados, y como el dado se tira bien y está bien hecho, entonces sería lo mismo apostar a cualquier número. Entonces, si seis personas entran al juego apostando a distintos números, cada uno pagando 1 peso, sería justo que el ganador se llevara 6 pesos.\n\nEn este caso, definimos la probabilidad de obtener un 3 (o cualquier otro número) como 1/6, que es cociente entre la apuesta inicial entre la cantidad recibida si ganamos la apuesta justa.\n\nNótese que el criterio de “justicia” proviene de simetrías del experimento: si el dado no fuera simétrico, por ejemplo, esta sería una manera mala de definir probabilidades.\nEn general, una manera de difinir probabilidad es la siguiente:\n\n\n\nEspacios equiprobables\nSi un evento \\(A\\) puede ocurrir de \\(k\\) maneras de \\(n\\) posibles, y es indiferente apostar a cualquiera de los \\(n\\) posibles resultados, entonces\n\\[P(A) = \\frac{k}{n}\\]\n\n\nCon este enfoque podemos resolver diversos problemas de probabilidad, por ejemplo:"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dados",
    "href": "modelo-prob.html#ejemplo-dados",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dados",
    "text": "Ejemplo: dados\n¿Cuál es la probabilidad de que tiremos una suma de 9 con dos dados de seis lados?\nEn este caso, los resultados son pares \\((x, y)\\) donde \\(x\\) es el resultado del dado 1 y \\(y\\) es el resultado del dado 2. Existen 36 pares, y todos ellos son equivalentes. Para tirar un 9, tenemos que lograr alguna de las siguientes tiradas:\n\\[(3, 6), (4, 5), (5, 4), (6, 3)\\]\nDe modo que la probabilidad de tirar una suma de 9 , que escribimos como \\(S=9\\), es\n\\[P(S = 9) = 4/36 = 1/9\\]"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-cartas",
    "href": "modelo-prob.html#ejemplo-cartas",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "Ejemplo: cartas",
    "text": "Ejemplo: cartas\nSacamos dos cartas sucesivamente de una baraja de 52 cartas, donde 26 son negras y 26 rojas. ¿Cuál es la probabilidad de que la segunda carta que saquemos sea negra? Podemos denotar como \\(N_2\\) el evento que sucede cuando la segunda carta que sacamos es negra. La segunda carta que sacamos puede ser cualquiera de las 52, y somos indiferentes en apostar a cualqueira de las cartas para salir en segundo lugar, así que\n\\[P(N_2) = 26 / 52 = 1/2\\]\nEstos espacios equiprobables nos dan nuestros modelos probabilísticos más simples. Están basados en simetrías del espacio de resultados."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "href": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.2 Probabilidad y frecuencias relativas",
    "text": "6.2 Probabilidad y frecuencias relativas\nLa conexión entre experimentos reales y modelos de probabilidad, está en que si repetimos muchas veces el experimento, entonces las frecuencias relativas de ocurrencia de los eventos aproxima a las probabilidades teóricas. Por ejemplo, si tiramos muchos volados con una moneda bien balanceada, esperamos obtener alrededor de 1/2 de soles y 1/2 de águilas, y si tiramos un dado muchas veces esperamos obtener alrededor de 1/6 de las tiradas un 5, por ejemplo (bajo los modelos de resultados equiprobables correspondientes).\nEn realidad, esta es otra definición de probabilidad en términos de repeticiones de experimentos.\n\n\n\nProbabilidades y frecuencias\nSupongamos que repetimos una gran cantidad \\(n\\) de veces un experimento, y que registramos \\(k_n\\) = cuántas veces ocurre un evento \\(A\\). La probabilidad de que ocurra \\(A\\) es\n\\[\\lim_{n\\to\\infty} \\frac{k_n}{n} \\to P(A), \\]\nes decir, \\(P(A)\\) el la frecuencia al largo plazo de ocurrencia de \\(A\\).\n\n\nAunque podríamos hacer algunos experimentos físicos más reales, para este curso podemos hacer simulaciones de computadora del experimento que nos interesa."
  },
  {
    "objectID": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "href": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "Ejemplo: simulación de un dado",
    "text": "Ejemplo: simulación de un dado\nPrimero hacemos un dado. Podemos simular una tirada de dado como:\n\nsimular_dado <- function(caras = 1:6){\n   sample(caras, 1)\n}\nsimular_dado()\n\n[1] 3\n\n\nAhora simulamos una gran cantidad de tiradas de dado:\n\nset.seed(199213)\nn <- 5000\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nhead(sims_dado) \n\n# A tibble: 6 × 2\n  n_sim resultado\n  <int>     <int>\n1     1         3\n2     2         6\n3     3         3\n4     4         3\n5     5         6\n6     6         3\n\n\nEsta es una variable numérica, pero como toma valores enteros del uno al seis, podemos resumir con frecuencias, como si fuera categórica:\n\nsims_dado %>% \n   count(resultado) %>% \n   mutate(frec_relativa = n / sum(n))\n\n# A tibble: 6 × 3\n  resultado     n frec_relativa\n      <int> <int>         <dbl>\n1         1   806         0.161\n2         2   814         0.163\n3         3   857         0.171\n4         4   898         0.180\n5         5   856         0.171\n6         6   769         0.154\n\n\nY nuestro modelo teórico (resultados equiprobables) coincide razonablemente bien con las frecuencias observadas a largo plazo. Podemos ver cómo convergen las frecuencias relativas por ejemplo del resultado 1:\n\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")\n\n\n\n\nNótese que cuando hay pocas repeticiones podemos ver fluctuaciones considerablemente grandes de la frecuencia relativa observada de unos. Sin embargo, conforme aumentamos el tamaño de la meustra observada, esas fluctuaciones son más chicas.\nVeamos otra simulación:\n\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")"
  },
  {
    "objectID": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "href": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.3 Datos y modelos de probabilidad",
    "text": "6.3 Datos y modelos de probabilidad\n¿Cómo podemos usar modelos de probablidad para describir datos observados? La idea (simplificada) es la siguiente:\n\nHacemos una hipótesis acerca de cómo es el modelo de probabilidad asociado a un fenómeno\nObservamos una muestra de datos del fenómeno que nos interesa\nEvaluamos si las fluctuaciones observadas debidas a la información limitada que tenemos (una muestra) son consistentes con el modelo de probabilidad\n\nConsideremos el ejemplo de los dados. Supongamos que lanzamos el dado un número no muy grande de veces, y observamos:\n\nfrecs_obs <- tibble(resultado = 1:6,\n                    n = c(5, 7, 5, 10, 8, 5)) %>% \n   mutate(frec = n / sum(n))\nfrecs_obs %>% kable(digits = 2)\n\n\n\n \n  \n    resultado \n    n \n    frec \n  \n \n\n  \n    1 \n    5 \n    0.12 \n  \n  \n    2 \n    7 \n    0.17 \n  \n  \n    3 \n    5 \n    0.12 \n  \n  \n    4 \n    10 \n    0.25 \n  \n  \n    5 \n    8 \n    0.20 \n  \n  \n    6 \n    5 \n    0.12 \n  \n\n\n\n\n\nY nos preguntamos si este resultado podría ser observado bajo los supuestos de nuestro modelo de probabilidad, que en este caso, es el de resultados equiprobables. Podemos por ejemplo graficar los datos junto con simulaciones del modelo, en búsqueda de desajustes:\n\nset.seed(8834)\n# una vez\nsim_exp <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()))\n# 19 veces\nsims_exp <- map_df(1:19, function(x){\n         sims <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()) )\n         sims$rep <- x\n         sims\n         })\n\n\nfrec_sims <- sims_exp %>% \n   group_by(rep, resultado) %>% \n   summarise(n = n()) %>% \n   mutate(frec = n / sum(n))\n\n`summarise()` has grouped output by 'rep'. You can override using the `.groups`\nargument.\n\nobs_sims_tbl <- bind_rows(frec_sims, frecs_obs %>% mutate(rep = 20))\nggplot(obs_sims_tbl, aes(x = resultado, y = frec)) +\n   geom_col() +\n   facet_wrap(~rep)\n\n\n\n\nEn este caso, no vemos ninguna característica de los datos observados que no sea consistente las fluctuaciones esperadas para un tamaño de muestra de \\(n=40\\).\nPregunta: ¿qué defecto le ves a esta gráfica que tiene los datos en la posición 20? ¿Cómo podríamos hacer una evaluación más apropiada de la calidad del ajuste?\nObservación: como veremos, muchas veces proponemos modelos que tienen parámetros que deben ser estimados con la muestra. Este caso más común es más complejo que el explicado arriba, pero el proceso es similar."
  },
  {
    "objectID": "modelo-prob.html#espacios-no-equiprobables",
    "href": "modelo-prob.html#espacios-no-equiprobables",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.4 Espacios no equiprobables",
    "text": "6.4 Espacios no equiprobables\nDepende cómo planteemos nuestro experimento aleatorio, puede ser o no apropiado el modelo equiprobable. Veremos ahora algunos ejemplos donde construimos modelos no equiprobables"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dos-dados",
    "href": "modelo-prob.html#ejemplo-dos-dados",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dos dados",
    "text": "Ejemplo: dos dados\nSupongamos que nos interesa la suma de dos tiradas de dados. Comenzamos con un modelo equiprobable sobre los resultados de cada tirada, que denotamos como \\((x,y)\\). Cada resultado tiene probabilidad 1/36.\nSin embargo, sólo nos interesa la suma. Contando posibles resultados podemos dar la probabilidad de cada resultado:\n\n\n\nSuma\nResultados\nProb\n\n\n\n\n2\n(1,1)\n1/36\n\n\n3\n(1,2),(2,1)\n2/36\n\n\n4\n(1,3),(2,2),(3,1)\n3/36\n\n\n5\n(1,4),(2,3),(3,2),(4,1)\n4/36\n\n\n6\n(1,5),(2,4),(3,3),(4,2),(5,1)\n5/36\n\n\n7\n(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\n6/36\n\n\n8\n(2,6),(3,5),(4,4),(5,3),(6,2)\n5/35\n\n\n9\n(3,6),(4,5),(5,4),(6,3)\n4/36\n\n\n10\n(4,6),(5,5),(6,4)\n3/36\n\n\n11\n(5,6), (6,5)\n2/36\n\n\n12\n(6,6)\n1/36\n\n\n\nY entonces terminamos con la siguiente distribución no equiprobable sobre las posibles sumas:\n\nprobs_suma <- tibble(suma = 2:12) %>% \n   mutate(prob = (6 - abs(suma - 7)) / 36)\nprobs_suma %>% kable(digits = 3)\n\n\n\n \n  \n    suma \n    prob \n  \n \n\n  \n    2 \n    0.028 \n  \n  \n    3 \n    0.056 \n  \n  \n    4 \n    0.083 \n  \n  \n    5 \n    0.111 \n  \n  \n    6 \n    0.139 \n  \n  \n    7 \n    0.167 \n  \n  \n    8 \n    0.139 \n  \n  \n    9 \n    0.111 \n  \n  \n    10 \n    0.083 \n  \n  \n    11 \n    0.056 \n  \n  \n    12 \n    0.028 \n  \n\n\n\n\n\nEsta distribución la podemos graficar como sigue:\n\ng_teorica <- ggplot(probs_suma, aes(x = suma, y = prob)) +\n   geom_col() +\n   scale_x_continuous(breaks = 2:12)\ng_teorica\n\n\n\n\nPodríamos tirar dos dado un gran número de veces para verificar si este modelo da las probabilidades correctas (o más propiamente dicho, si estas observaciones son consistentes con el modelo de probabilidad mostrado arriba)."
  },
  {
    "objectID": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "href": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.5 Reglas básicas de probabilidad",
    "text": "6.5 Reglas básicas de probabilidad\nTanto modelos equiprobables como la interpretación frecuentista de la probabilidad resultan en un conjunto de reglas útiles para operar con probabilidades. Estas son reglas generales que aplican independientemente de la interpretación particular de la probabilidad que utilicemos.\n\nLa probabilidad del evento cierto es 1 Si \\(A\\) es el evento que cubre todos los posibles resultados, entonces \\(P(A) = 1\\).\n\nAsegúrate que puedes demostrar esto para un espacio equiprobable, por ejemplo.\n\nLa probabilidad de que un evento \\(A\\) no ocurra es \\(1- P(A)\\).\n\n¿Por qué es cierto esto en espacios equiprobables? ¿Y según la definición frecuentista? Usa esta regla y el ejemplo anterior para calcular la probabilidad de tirar una suma menor a 12 cuando tiramos dos dados.\nEsto normalmente lo escribimos como \\(P(A^c) = 1 - P(A)\\).\nDecimos que dos eventos \\(A\\) y \\(B\\) son disjuntos cuando no pueden ocurrir simultánemente, o en otras palabras, el conjunto de resultados donde ocurre \\(A\\) tiene intersección vacía con el conjunto de resultados donde ocurre \\(B\\).\n\nLa probabilidad de que ocurra un resultado dentro de un conjunto de eventos disjuntos es igual a la suma de las probabilidades de dichos eventos\n\nEsto lo podemos escribir como sigue: si \\(A_1, A_2, \\dots, A_n\\) son eventos disjuntos, entonces\n\\[P(A_1\\cup A_2\\cup \\cdots \\cup A_n) = P(A_1) + P(A_2) + \\cdots + P(A_n)\\]\nUtiliza esta regla para\n\nCalcula la probabilidad de tirar una suma mayor a 9 en dos tiradas de dados.\nCalcula la probabilidad de obtener un par al sacar dos cartas de una baraja usual de póquer.\nMás difícil: hay una caja con \\(100\\) pelotas, y una de ellas es dorada. Si sacamos al azar \\(20\\) pelotas, una a la vez, ¿cuál es la probabilidad de que nos salga la pelota dorada? ¿ y si sacamos 57 pelotas?\n\nSoluciones:\n\nPara tirar más de 9, podemos tirar 10, 11 o 12, así que \\(P(A) = P(A_{10}\\cup A_{11} \\cup A_{12})\\). Estos útlimos tres eventos son mutuamente excluyentes, así que por la regla de la suma,\n\n\\[P(A) = P(A_{10}) + P(A_{11}) + P(A_{12})\\] y consultando nuestra tabla de arriba,\n\\[P(A) = \\frac{3}{36} + \\frac{2}{36} + \\frac{1}{36} = 1/6\\]\n\nPodemos sacar \\(52(51)\\) pares distintos. Ahora calculamos la probabidad de sacar un par particular, por ejemplo de ases. Hay \\(4(3)=12\\) distintos pares de ases (en el orden que los sacamos). Lo mismo podemos decir de pares de 2, 3, etc. Así que sumando sobre cada una de estas posibilidades obtenemos:\n\n\\[P(A) = 13\\frac{4(3)}{52(51)} \\approx 0.0588\\]\nTambién podríamos calcular de la siguiente forma: sacamos la primera carta y la vemos. La siguiente carta puede ser una de 51 restantes, y solo con tres de ellas completamos un par, de forma que\n\\[P(A) = 3 / 51 \\approx 0.0588\\]\n\nSea \\(D\\) el evento donde en el proceso sacamos la pelota dorada, y sea \\(D_1\\) el evento donde sacamos la pelota dorada en nuestra primera extracción, \\(D_2\\) si la sacamos en la segunda, etc. Tenemos que\n\n\\[D =D_1 \\cup D_2 \\cup\\cdots \\cup D_{20}\\]\n(sacamos la dorada si y sólo si la sacamos en la extracción 1, 2 , etc. hasta 20) Además, solo hay una pelota dorada, de manera que \\(D_i\\cap D_j= \\emptyset\\), es decir, no pueden ocurrir juntos \\(D_1\\) y \\(D_2\\), \\(D_5\\) y \\(D_11\\), etc., así que por la regla de la suma\n\\[P(D) = P(D_1) + P(D_2) + \\dots + P(D_{20})\\]\nFinalmente, \\(P(D_1)= 1/100\\), pues hay 100 pelotas y todas tienen igual probabilidad de aparecen en la primera extracción. Pero también \\(P(D_2) = 1/100\\), pues todas las pelotas tienen la misma probabilidad de aparecer en la segunda extracción. Se sigue entonces que, haciendo la suma de arriba, que: \\(P(D) = 20(\\frac{1}{100}) = 0.20\\)"
  },
  {
    "objectID": "modelo-prob.html#simulación-y-probabilidad",
    "href": "modelo-prob.html#simulación-y-probabilidad",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.6 Simulación y probabilidad",
    "text": "6.6 Simulación y probabilidad\nUsando la interpetación frecuentista, también es posible resolver una variedad de problemas de probabilidad usando simulación. La idea es, si queremos aproximar la probabilidad \\(P(E)\\) de un evento es:\n\nDefinimos el espacio de resultados del experimento aleatorio.\nSimulamos el experimento aleatorio un número grande de veces.\nCalculamos para cuáles de esas simulaciones se cumple el evento \\(E\\)\nEstimamos la frecuencia relativa de ocurrencia de \\(E\\) a lo largo de todas las simulaciones.\n\n\nEjemplo: simulando dos dados\nPara dos dados, el espacio de resultados podemos escribirlo como los resultados conjunto de dos tiradas: \\((x,y)\\). Cada dado se tira de forma separada, y los resultados son equiprobables. Definimos entonces:\n\nsim_dados <- function(num_dados = 2, num_caras = 6){\n   resultado <- sample(1:num_caras, num_dados, replace = TRUE)\n   names(resultado) <- paste0(\"dado_\", 1:num_dados)\n   resultado\n}\nsim_dados()\n\ndado_1 dado_2 \n     5      3 \n\n\nAhora simulamos un número grande de veces el experimento:\n\nset.seed(2323)\nsims <- map_df(1:10000, ~ sim_dados())\nsims\n\n# A tibble: 10,000 × 2\n   dado_1 dado_2\n    <int>  <int>\n 1      5      6\n 2      3      3\n 3      2      5\n 4      3      5\n 5      6      4\n 6      2      2\n 7      3      6\n 8      6      5\n 9      3      4\n10      1      1\n# … with 9,990 more rows\n\n\nAhora que tenemos estas simulaciones, podemos estimar por ejemplo la probabilidad de tirar más de nueve con dos dados. Primero calculamos en qué simulaciones ocurre \\(E\\):\n\nsims_e <- \n   sims %>% \n   mutate(suma = dado_1 + dado_2) %>% \n   mutate(evento_E = (suma > 9)) \nsims_e\n\n# A tibble: 10,000 × 4\n   dado_1 dado_2  suma evento_E\n    <int>  <int> <int> <lgl>   \n 1      5      6    11 TRUE    \n 2      3      3     6 FALSE   \n 3      2      5     7 FALSE   \n 4      3      5     8 FALSE   \n 5      6      4    10 TRUE    \n 6      2      2     4 FALSE   \n 7      3      6     9 FALSE   \n 8      6      5    11 TRUE    \n 9      3      4     7 FALSE   \n10      1      1     2 FALSE   \n# … with 9,990 more rows\n\n\nY ahora calculamos la frecuencia relativa de ocurrencia de \\(E\\)\n\nsims_e %>% \n   summarise(frec_e = mean(evento_E))\n\n# A tibble: 1 × 1\n  frec_e\n   <dbl>\n1  0.169\n\n\nQue confirma a dos decimales el resultado que obtuvimos arriba usando la regla de la suma (o contando resultados).\n\n\nEjemplo: problema de pelotas\nExtraemos al azar\n\nset.seed(232)\npelotas <- c(\"dorada\", rep(\"otra\", 99))\nsim_extraccion <- function(){\n    sample(pelotas, 20, replace = FALSE)\n}\nsim_extraccion()\n\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nSimulamos el experimento aleatorio:\n\nsims_pelotas <- map(1:10000, ~ sim_extraccion())\nsims_pelotas[1:2]\n\n[[1]]\n [1] \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n [9] \"dorada\" \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n[17] \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n\n[[2]]\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nChecamos si ocurre o no el evento se extrajo la pelota dorada:\n\nevento_dorada <- map_lgl(sims_pelotas, ~ any(str_detect(.x, \"dorada\")))\nevento_dorada[1:10]                   \n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nY calculamos la frecuencia relativa de ocurrencia del evento:\n\nmean(evento_dorada)\n\n[1] 0.2007\n\n\nQue es consistente con el resultado que obtuvimos arriba haciendo cálculos.\n\n\n\nPara estimar probabilidades de ocurrencia de un evento usualmente es posible hacer simulaciones por computadora. Esto es especialmente importante cuando el experimento y evento que consideramos hace difícil (a veces imposible) hacer cálculos analíticos.\nEste tipo de métodos basados en simulación se llaman en general métodos de Monte Carlo.\n\n\n\n\n6.6.1 Ejemplo:\nSupongamos que ponemos 15 puntos igualmente espaciados en una circunferencia. Si escogemos tres puntos al azar, ¿cuál es la probabilidad de que formen un triángulo equilátero? Este problema se puede resolver contando los posibles resultados donde se forma un triángulo equilátero, pero no es trivial. Veamos cómo la haríamos simulando:\n\nsimular_3_puntos <- function(){\n   puntos_angulo <- 0:14 * (2  *pi / 15)\n   sample(puntos_angulo, 3)\n}\nsimular_3_puntos()\n\n[1] 3.769911 3.351032 1.675516\n\n\nCalcular dado el resultado requiere algo de consideración, pero finalmente terminamos con (¿qué otra manera se te ocurre?):\n\nes_equilatero <- function(tres_puntos){\n   tol <- 10^(-6)\n   puntos_ord <- sort(tres_puntos)\n   dif_1 <- puntos_ord[3] - puntos_ord[2]\n   dif_2 <- puntos_ord[2] - puntos_ord[1]\n   dif_3 <- 2*pi + puntos_ord[1] - puntos_ord[3]\n   if(abs(dif_1 - dif_2) < tol & abs(dif_2 - dif_3) < tol){\n      equilatero <- TRUE\n   } else {\n      equilatero <- FALSE\n   }\n   equilatero\n}\n\nSimulamos:\n\nsims_puntos <- map(1:50000, ~ simular_3_puntos())\n\nCalculamos la ocurrencia del evento de interés:\n\nevento <- map_lgl(sims_puntos, ~ es_equilatero(.x))\n\nY al frecuencia relativa es:\n\nmean(evento)\n\n[1] 0.01168\n\n\n¿Puedes resolver este problema contando los posibles triángulos, y cuáles de ellos son equiláteros?"
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva",
    "href": "modelo-prob.html#probabilidad-subjetiva",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.7 Probabilidad subjetiva",
    "text": "6.7 Probabilidad subjetiva\nExiste otra manera más flexible de interpretar la probabilidad que tiene que ver con un “grado de creencia” en que un evento va a ocurrir. Por ejemplo, el evento \\(A\\) podría ser “va a llover mañana”, y según mi conocimiento establezco que \\[P(A) = 0.25\\]\nEsta es una probablidad subjetiva pero no arbitraria. Para entender por qué es eso, pensemos en un juego de azar que consiste jugar una lotería de 100 boletos, donde podemos jugar con un número de boletos dado. Comparamos entonces dos alternativas:\n\nSi \\(A\\) ocurre, es decir, llueve mañana, entonces gano 1000 pesos.\nLa lotería de 100 boletos tiene un único premio de 1000 pesos.\n\nEntonces me puedo preguntar:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 5 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.05\\). Ahora puedo preguntarme:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 20 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.20\\) Sin embargo, probablemente si me pregunto si prefiero jugar 1 o jugar 2 con 90 boletos, prefería jugar la lotería, de modo yo creo que \\(P(A)<0.90.\\)\nCon estos ejercicios de preferencias sobre apuestas comparando con juegos de azar simples, puedo ver cuál es la probabilidad que yo le asigno a cualquier evento. Es posible demostrar que una probabilidad definida de esta manera cumple todas las reglas de probabilidad que mostramos arriba (si mis decisiones son racionales). Es posible definir modelos de probabilidad con este enfoque subjetivo, y es posible operar con ellos de manera usual con la maquinaria matemática.\nLo poderoso de este enfoque es que nos permite atacar problemas que 1) no está claro cómo podrían derivarse a partir de modelos equiprobables, y 2) no está claro que sea posible reproducir el experimento muchas veces para ver cual es la frecuencia relativa de ocurrencia del evento. Por ejemplo:\n\n¿Cuál es la probabilidad de que esta persona X contraiga una enfermedad particular?\n¿Cuál es la probabilidad de que un candidato Y gane una elección?\n¿Qué tan probable es que el próximo año el crecimiento del PIB sea mayor a 1%?\n\nEn todos estos casos importantes donde tenemos que tomar decisiones, la única alternativa real es pensar en términos de probabilidad subjetiva.\nEn la práctica, este tipo de enfoque se utiliza para poner probabilidades sobre cantidades o aspectos difíciles de medir directamente pero que son relativamente simples, y luego se utilizan modelos y reglas de probabilidad para producir otras probabilidades de interés más complicadas."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "href": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "title": "6  Básicos de probabilidad y simulación",
    "section": "6.8 Probabilidad subjetiva y calibración frecuentista",
    "text": "6.8 Probabilidad subjetiva y calibración frecuentista\nComo podemos imaginarnos, para muchas decisiones importantes, el enfoque subjetivo por sí solo puede no ser suficiente. Por ejemplo, imaginemos que trabajamos en INEGI queremos estimar el ingreso total de los hogares en México. Sería difícil proponer para esta tarea un enfoque puramente subjetivo.\nEn estos caso, podemos usar un enfoque donde utilizamos las probabilidades de manera subjetiva (por ejemplo elicitadas de un pánel de expertos), pero demostramos también que nuestro método tiene propiedades frecuentistas apropiadas: por ejemplo, que nuestros intervalos de 95% de estimación son realmente de 95%. Esto normalmente se cumple 1) Checando supuestos de nuestro modelo 2) Sometiendo nuestro método a distintos escenarios de estimación, y checando que se cumplen estimaciones frecuentistas apropiadas. En realidad, usualmente estos dos últimos pasos, 1 y 2, deben ser llevados a cabo no importa el enfoque de interpretación que utilicemos, pues cualquier método, frecuentista o subjetivo, tiene condiciones bajo las que puede fallar."
  },
  {
    "objectID": "prob-condicional.html",
    "href": "prob-condicional.html",
    "title": "7  Probabilidad condicional e independencia",
    "section": "",
    "text": "Uno los conceptos centrales de la probabilidad es el de probabilidad condicional:\nMuchos de los problemas de esta sección son de (Ross (1998))."
  },
  {
    "objectID": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "href": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.1 Probabilidad condicional en espacios equiprobables.",
    "text": "7.1 Probabilidad condicional en espacios equiprobables.\nSupongamos que en un experimento simétro de \\(n\\) posibles resultados, sabemos que ocurrió el evento \\(F\\), es decir, un conjunto de resultados fijo. ¿Cómo podemos calcular la probabilidad de que ocurra un evento \\(E\\) dado que sabemos que \\(F\\) ocurrió? Esta probabilidad se escribe\n\\[P(E|F)\\]\n\n7.1.1 Ejemplo: dos dados\nSupongamos que tiramos dos dados, y nos dicen que la suma de los dos datos es igual a 6. ¿Cuál es la probablidad condicional de haber tirado al menos un cinco dado que la suma es 6?\nSolución: los resultados equiprobables que resultan en un tiro de suma 6 son \\(F= \\{ (5,1),(4,2),(3,3),(2,4),(1,5)\\}\\), que son 5 posibles resultados. En solamente 2 de ellos tiramos un cinco. Como estos resultados son equiprobables, si \\(E\\) es el evento “tirar al menos un 5”,\n\\[P(E|F) = 2/5\\]\nPodemos formalizar de la siguiente manera: para calcular \\(P(E|F)\\) contamos todos los resultados de \\(F\\) donde también ocurre \\(E\\) y dividimos entre las maneras en que puede ocurrir \\(F\\):\n\nEn nuestro caso hay muchos resultados posibles donde tiramos al menos un 5, por ejemplo \\((5,1), (5,2), (5,6)\\) y así sucesivamente. Sin embargo, solo en 2 de ellos la suma es 5.\n\n\n\n\nEn un espacio de resultados equiprobables, si \\(E\\) y \\(F\\) son eventos, entonces\n\\[P(E|F) = \\frac{n(E\\cap F)}{n(F)},\\]\nes decir, dividimos el número de maneras en que pueden ocurrir \\(E\\) y \\(F\\) simultáneamente entre el número de maneras en que puede ocurrir \\(F\\).\n\n\nNótese que otra manera de ver esta definición es como sigue: una vez que sabemos que ocurrió \\(F\\), restringimos todo nuestro análisis a resultados dentro de \\(F\\), y proseguimos como si se tratara de una probabilidad usual.\n\n\n7.1.2 Ejemplo: dos volados\nSupongamos que tiramos dos volados. Cuál es la probabilidad condicional de que los dos volados sean sol (evento \\(E\\)) dado que 1) El primer volado es sol? 2) Alguno de los dos volados es sol, 3) Los dos volados son águilas?\nHay 2 resultados donde el primer volado es sol (enuméralos), así que la primer probabilidad es \\(P(E|F_1)=1/2\\). Explica por qué la segunda probabilidad condicional es igual a \\(P(E|F_2)=1/3\\). ¿Cuánto vale \\(P(E|F_3)\\)?\n\n\n7.1.3 Ejemplo: tres cartas\nSupongamos que extraemos tres cartas al azar de una baraja de 52 cartas (13 son corazones). Nos muestran que la segunda y la tercera carta son corazones. ¿Cuál es la probabilidad condicional de que la primera sea un corazón?\nSolución: como resultados para las primeras tres cartas seleccionadas tenemos \\((x_1,x_2,x_3)\\). Nos interesan solamente los resultados \\((x_1, corazon_1, corazon_2)\\). Existen \\(13*12*11 + 39*13*12 = 7800\\) resultados posibles (primero contamos los que tienen un corazón al principio, y luego los que no tienen un corazón al principio, de modo que la probabilidad que buscamos es\n\\[\\frac{13(12)(11)}{13(12)(11) + 39(13)(12)} = \\frac{11}{11 + 39} = 0.22\\]\nInterpreta la simplificación de arriba para describir una manera más simple de calcular esta probabilidad condicional."
  },
  {
    "objectID": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "href": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.2 Simulación y probabilidad condicional",
    "text": "7.2 Simulación y probabilidad condicional\nUna manera de aproximar probabilidades condicionales es simulando el experimento que nos interesa, y calculando frecuencias relativas solamente sobre la información que sabemos que ocurrió: es decir, filtramos las simulaciones escogiendo sólo las que son consistentes con la información dada.\n\nEjemplo: simulación tres cartas\n\nbaraja <- tibble(numero = 1:13) %>% \n   crossing(tibble(figura = c(\"C\", \"D\", \"T\", \"P\"))) %>% \n   mutate(carta = paste(numero, figura))\nnrow(baraja)\n\n[1] 52\n\nbaraja\n\n# A tibble: 52 × 3\n   numero figura carta\n    <int> <chr>  <chr>\n 1      1 C      1 C  \n 2      1 D      1 D  \n 3      1 P      1 P  \n 4      1 T      1 T  \n 5      2 C      2 C  \n 6      2 D      2 D  \n 7      2 P      2 P  \n 8      2 T      2 T  \n 9      3 C      3 C  \n10      3 D      3 D  \n# … with 42 more rows\n\n\n\ncartas <- baraja$carta\nexp_3_cartas <- function(cartas){\n   sample(cartas, 3) \n}\nset.seed(132185)\nexp_3_cartas(cartas)\n\n[1] \"7 C\"  \"8 P\"  \"13 T\"\n\n\nSimulamos el experimento\n\nsims <- map(1:20000, ~ exp_3_cartas(cartas))\nsims[1:5]\n\n[[1]]\n[1] \"4 C\"  \"8 T\"  \"12 D\"\n\n[[2]]\n[1] \"11 D\" \"6 P\"  \"7 C\" \n\n[[3]]\n[1] \"12 C\" \"1 D\"  \"5 C\" \n\n[[4]]\n[1] \"12 D\" \"9 D\"  \"13 T\"\n\n[[5]]\n[1] \"7 T\"  \"13 D\" \"13 T\"\n\n\nSin más información, la probabilidad de corazón en la primera extracción es:\n\nsims %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n[1] 0.2474\n\n\nQue debe estar alrededor de 1/4. Ahora condicionamos a que las cartas 2 y 3 son corazones:\n\nsims_F <- sims %>%\n   keep(~ str_detect(.x[2], \"C\") & str_detect(.x[3], \"C\"))\nlength(sims_F)\n\n[1] 1203\n\nsims_F[1:5]\n\n[[1]]\n[1] \"9 T\"  \"3 C\"  \"12 C\"\n\n[[2]]\n[1] \"5 D\"  \"2 C\"  \"13 C\"\n\n[[3]]\n[1] \"5 C\"  \"10 C\" \"1 C\" \n\n[[4]]\n[1] \"4 P\" \"6 C\" \"1 C\"\n\n[[5]]\n[1] \"8 C\"  \"13 C\" \"11 C\"\n\n\nY sobre estas simulaciones hacemos el mismo cálculo de arriba:\n\nsims_F  %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n[1] 0.2236076\n\n\nEsto nos da una aproximación de \\(P(E|F)\\). Nótese que si \\(F\\) es un evento con probabilidad baja, entonces será necesario correr más veces el experimento, pues el número de veces que ocurre \\(F\\) es relativamente bajo.\nPodemos definir también la probabilidad condicional en general, para cualquier probabilidad \\(P\\) no necesariamente resultante de un modelo equiprobable:\n\n\n\nLa **probabilidad condicional* del evento \\(E\\) dado que ocurrió el evento \\(F\\) se define como\n\\[P(E|F) = \\frac{P(E y F)}{P(F)}\\]"
  },
  {
    "objectID": "prob-condicional.html#regla-de-la-multiplicación",
    "href": "prob-condicional.html#regla-de-la-multiplicación",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.3 Regla de la multiplicación",
    "text": "7.3 Regla de la multiplicación\nA veces nos interesa calcular la probabilidad de que dos eventos ocurran, y conocemos \\(P(F)\\) y \\(P(E|F)\\). En ese caso podemos usar la definición de probabilidad condicional para escribir al regla del producto:\n\\[P(EF) = P(F)P(E|F)\\]\nPor ejemplo, si queremos calcular la probablidad de extraer dos corazones de una baraja usual, tenemos que \\(P(C_1) = 13/52 = 1/4\\). \\(P(C_2|C_1)\\) es fácil de calcular, pues si la primera carta que sacamos es un corazón, entonces para la segunda extracción hay 51 cartas, de las cuales 12 son corazones, de forma que \\(P(C_2|C_1) = 12/51\\). Usando la regla del producto, quedamos con\n\\[P(C_1C_2) = P(C_1)P(C_2|C_1) = \\frac{13}{52}\\frac{12}{51} \\approx 0.0588\\]\nPodemos generalizar esto a\n\n\n\nRegla del producto\n\\[P(E_1E_2E_3\\cdots E_n) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\cdots P(E_n|E_1\\cdots E_{n-1}\\]\n\n\n\nEjercicio\nSe divide al azar una baraja de 52 de cartas en 4 pilas iguales. Calcula la probabilidad de cada pila tenga exactamente un as.\nPodemos hacer el evento \\(E_1\\) que el as de corazones y el de diamantes están en diferentes pilas, \\(E_2\\) el evento de que el as de corazones, el de diamantes, y el de tréboles están en diferentes pilas, y finalmente \\(E_3\\) el evento de que todos los ases están en diferentes pilas. Nótese que buscamos la probabilidad \\(P(E_3)\\), pero será más fácil si escribimos:\n\\[P(E_3) = P(E_3E_2E_1) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\]\nPrimero, el as de corazones está en alguna de las pilas. La probabilidad de el as de diamantes no esté en esa pila es \\(P(E_1) = 1 - 12/51 = 39/51\\) (¿por qué?), pues la pila que tiene el as de corazones tiene otras 12 cartas de las 51 disponibles. La probabilidad de que el as de diamantes sea una de esas 12 cartas es entonces 12/51.\nSi se cumple \\(E_1\\), entonces el as de corazones y de diamantes están en pilas distintas. Entonces la probabilidad de que el as de tréboles caiga en una de esas dos pilas es \\(24/50\\), así que\n\\[P(E_2|E_1) = 1 -24/50 = 25/50\\]\nFinalmente, si el as de corazones, diamantes y tréboles están en distintas pilas, entonces la probabilidad de que el de espadas caiga en la una de esas pilas es \\(36/49\\), de modo que\n\\[P(E_3|E_2E_1) = 1 - 36/49 = 13/49\\].\nUsando la reglal producto,\n\\[P(E_1E_2E_3) = \\frac{39(26)(13)}{51(50)(49)} \\approx 0.105\\]"
  },
  {
    "objectID": "prob-condicional.html#nota-falacias-probabilísticas",
    "href": "prob-condicional.html#nota-falacias-probabilísticas",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.4 Nota: falacias probabilísticas",
    "text": "7.4 Nota: falacias probabilísticas\nUna de las razones por las que es importante usar la teoría de probabilidad para manipular probabilidades es que las personas, naturalmente, tienden a hacer “cálculos mentales probabilísticos” incorrectos, lo cual a fin de cuentas lleva a decisiones mal informadas. Existe amplia evidencia de esto, ver por ejemplo el trabajo de: Kahneman y Tversky\n“Before their work, economists had gotten far in their analyses of decision making under uncertainty by assuming that people correctly estimate probabilities of various outcomes or, at least, do not estimate these probabilities in a biased way […]. But Kahneman and Tversky found that this is not true: the vast majority of people misestimate probabilities in predictable ways.”\nAhora podemos descutir una de ellas, la falacia de conjunción. Consideremos la siguiente pregunta:\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\n\n¿Qué es más probable?\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement.\n\n¿Qué crees que tiende a escoger la mayoria de las personas como la más probable? ¿Qué regla de probabilidad puedes usar para demostrar que esto es una falacia?"
  },
  {
    "objectID": "prob-condicional.html#regla-de-probablilidad-total",
    "href": "prob-condicional.html#regla-de-probablilidad-total",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.5 Regla de probablilidad total",
    "text": "7.5 Regla de probablilidad total\nUna regla que usaremos varias veces es la regla de probabilidad total, que establece que\n\\[P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\\]\ndonde el evento \\(F^c\\) significa que \\(F\\) no ocurrió.\nEsta regla es útil en muchos casos para calcular probabilidades de un evento dependiendo de la ocurrencia o no de otro.\n\nEjemplo\nSacamos dos cartas de una bajara de 52 cartas. Vimos un ejemplo donde queríamos calcular la probabilidad de \\(N_2=\\) la segunda carta que sacamos es negra. Argumentamos usando espacio equiprobables que \\(P(N_2) = 0.5\\).\nSi \\(N_1 =\\) la primera carta es negra, entonces la ley de probabilidad total explica por qué pasa esto tomando en cuenta el resultado de la primera extracción:\n\nTenemos que \\(P(N_2|N_1) = 25/51\\) y \\(P(N_1) = 1/2\\)\nAdemás, \\(P(N_2|N_1^c) = 26/51\\) y \\(P(N_1^c) = 1/2,\\)\n\nde forma que\n\\[P(N_2) = (25/51)(1/2) + (26/51)(1/2) = \\frac{(25 + 26)}{51(2)} = /2\\]\nLa ley de probabilidades totales consiste de las reglas de ponderación usuales que conocemos.\n\n\nEjemplo\nEn un país hay 20% de adultos de 20 años o menos, y 80% de adultos de 21 años o más. Entre los adultos de 20 años o menos, el 90% solo usa plataformas digitales para ver televisión. Entre los adultos de 21 años o más, el 15% solo usa plataformas digitales para ver televisión. Si tomamos un adulto al azar de esta población, ¿cuál es la probabilidad de que solo use digital para ver TV?\nLa respuesta es\n\\[ 0.90(0.20) + 0.15(0.80) = 0.3\\]"
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.6 Ejercicio: dados y monedas",
    "text": "7.6 Ejercicio: dados y monedas\nSupongamos que tiramos un dado. Tiramos tantos volados como el número que salió en la tirada de dado, y registramos el número de soles. ¿Cuál es la probabilidad de que obtengamos cero soles?\nSea \\(X=\\) número que obtuvimos en la tirada de dado, y sea \\(Y=\\) número de soles obtenidos.\nCalcular directamente \\(P(Y=0)\\) puede hacerse de manera simple con la ley de probabilidad total, pues\n\n\\(P(Y=0|X=1) = 1/2\\), prob de ningún sol en 1 volado\n\\(P(Y=0|X = 2) = 1/4\\) prob de ningún sol en dos volados (suponiendo independencia de los volados)\n\\(P(Y=0|X=3) = 1/8\\), prob de ningún sol en tres volados.\n\ny así sucesivamente. Como \\(P(X=i)=1/6\\) para cualquier número del uno al seis, la probabilidad \\(P(Y=0)\\), usando probabilidad total, es\n\\[(1/6)(1/2) + (1/6)(1/2)^2 +(1/6)(1/2)^3 +(1/6)(1/2)^4 +(1/6)(1/2)^5 +(1/6)(1/2)^6\\]\nque es igual a\n\nprobs_x <- rep(1/6, 6)\nprobs_y_x = 0.5^(1:6)\nsum(probs_y_x * probs_x)\n\n[1] 0.1640625\n\n\nCheca usando simulación."
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.7 Ejercicio: dados y monedas (simulación)",
    "text": "7.7 Ejercicio: dados y monedas (simulación)\nEste es un experimento más interesante para simular:\n\nsimular_soles <- function(){\n   dado <- sample(1:6, 1)\n   soles <- sample(c(\"sol\", \"águila\"), dado, replace = TRUE)\n   num_soles = sum(as.numeric(soles == \"sol\"))\n   num_soles\n}\nset.seed(82332)\nsimular_soles()\n\n[1] 3\n\n\nSi hacemos 10 mil simulaciones:\n\nsims <- map_dbl(1:10000, ~ simular_soles())\nqplot(sims)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLa frecuencia relativa de cero soles es:\n\nsum(sims==0) / length(sims)\n\n[1] 0.1655\n\n\nCalcula también la probabilidad de obtener 2 soles o más en este experimento (puedes usar la simulación)."
  },
  {
    "objectID": "prob-condicional.html#regla-de-bayes",
    "href": "prob-condicional.html#regla-de-bayes",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.8 Regla de Bayes",
    "text": "7.8 Regla de Bayes\nLa regla de Bayes es una fórmula que se utiliza para invertir probabilidades condicionales:\n\\[P(E|F) = \\frac{P(F|E)P(E)}{P(F)},\\]\nque es una consecuencia fácil de la definición de probabilidad condicional. Es útil conocerla porque facilita resolver varios problemas de probabilidad que en un principio parecen difíciles.\n\nEjemplo\nSupongamos que tenemos una baraja de 8 cartas, 4 negras y 4 blancas. Extraemos sucesivamente dos cartas a azar. Sea \\(N_1=\\) la primera carta es negra y \\(B_2=\\) la segunda carta es blanca. ¿Cuál es la probabilidad condicional de que la primera carta haya sido negra si nos dicen que la segunda fue blanca?\nAunque ya resolvimos problemas como este, parece confuso en un principio. Sin embargo, podemos calcular:\n\\[P(N_1|B_2) = \\frac{P(B_2|N_1)P(N_1)}{P(B_2)}\\]\nSabemos que \\(P(B_2|N_1) = 4/7\\), y que \\(P(N_1)=1/2\\) y \\(P(B_2) = 1/2\\), de modo que\n\\[P(N_1|B_2) = 4/7 > 1/2\\]\nes decir, si sabemos que la segunda carta fue blanca, eso hace más probable que la primera carta haya sido negra.\n\n\nEjemplo (parte 1)\nSupongamos que una aseguradora cree que hay dos tipos de personas: unos con más riesgo de tener accidentes y otros con menos riesgo. Los datos muestran que una persona con riesgo alto tendrá un accidente en algún momento del año con probabilidad 0.04, y esta probabilidad baja a 0.01 para una persona de riesgo bajo. Si 10% de la población tiene riesgo alto, ¿cuál es la probabilidad de que un asegurado nuevo tenga un accidente un año después de comprar su póliza? (Nota: no sabemos si la persona nueva es de riesgo alto o bajo).\nPuedes resolver son simulación, o usar la ley de probabilidad total. Si \\(A\\) es el evento de tener un accidente, \\(R_A\\) es el evento de que la persona es de riesgo alto y \\(R_B\\) es el evento que la persona es de riesgo bajo, entonces\n\\[P(A) = P(A|R_A)P(R_A) + P(A|R_B)P(R_B)\\]\npues \\(R_A\\) y \\(R_B\\) cubren todas las posibilidades. Entonces\n\\[P(A) = 0.04(0.10) + (0.01)(0.90) = 0.004 + 0.009 = 0.013\\]\nEs decir, su probabilidad es de 1.3% de tener una accidente en el primer año.\n\n\nEjemplo (parte 2)\nAhora vemos que un cliente tuvo un accidente en su primer año. ¿Cuál es la probabilidad de que sea un cliente de riesgo alto?\nLa pregunta es de probabilidad condicional, porque ya tenemos información. Queremos calcular\n\\[P(R_A| A)\\]\nSi usamos la regla de Bayes obtenemos\n\\[P(R_A|A)= \\frac{P(A|R_A)P(R_A)}{P(A)}\\]\nSustituimos los datos que tenemos\n\\[P(R_A|A)= \\frac{0.04(0.10)}{P(A)}\\]\ny \\(P(A)\\) que calculamos en el ejercicio anterior:\n\\[P(R_A|A)= \\frac{0.04(0.10)}{0.013} \\approx 0.3077\\]\nDe manera que al principio la probabilidad no condicionada de ser de alto riesgo era de 10%, y cuando tiene un accidente esta probabilidad se triplica.\n\n\nEjemplo\nSupongamos que en un concurso de TV tenemos tres puertas y debemos escoger una. Atrás de una de ellas hay un premio, y no hay nada detrás de las otras dos. Escogemos una de las puertas.\nAhora el conductor (que sabe dónde está el premio), abre una puerta vacía, y nos pregunta si queremos cambiar o no de puerta. ¿Cuál es la mejor estrategia, cambiar o quedarnos con la que escogimos al principio?\nVeamos la estrategia de quedarnos con la puerta que escogimos. Sin perdida de generalidad, suponemos que escogemos la puerta 1.\nAhora observamos que el conductor abre la puerta 2.\nSea \\(E_1=\\) el premio está en la puerta 1, y \\(A_2=\\) el conductor abre la puerta 2, donde no hay premio. Por la regla de bayes:\n\\[P(E_1 | A_2) = \\frac{P(A_2 | E_1) P(E_1)}{P(A_2)}\\]\nSabemos que \\(P(E_1)= 1/3\\), y que \\(P(A_2|E_1)=1/2\\) (pues el conductor pudo abrir la puerta 2 o 3). Adicionalmente\n\\[P(A_2) = P(A_2|E_1)P(E_1) + P(A_2|E_1^c)P(E_1^c) = (1/2)(1/3) + (1/2) (2/3) = 1/2\\]\npues \\(P(A_2|E_1^c) = 1/2\\), es decir, si el premio no está en la puerta 1, la probabilidad de que abrir la puerta 2 es la probabilidad de que el premio esté en la puerta 2 dado que no está en la puerta 1\nSustituyendo,\n\\[P(E_1 | A_2) = \\frac{(1/2)(1/3)}{1/2} = 1/3\\]\nAsi que si cambiamos, la probabilidad de ganar es de 2/3.\nSimula para verificar tus resultados."
  },
  {
    "objectID": "prob-condicional.html#independencia",
    "href": "prob-condicional.html#independencia",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.9 Independencia",
    "text": "7.9 Independencia\nCuando tenemos dos eventos, y tenemos que \\(P(E|F) > P(E)\\) o \\(P(F|E) > P(F)\\) (checa que una implica la otra), decimos que los eventos tienen dependencia positiva: cuando sabemos que uno ocurre, la probabiidad del otro aumenta.\nEn algunos casos \\(P(E|F) = P(E)\\) y \\(P(F|E) = P(F)\\), lo cual sucede cuando \\(P(EF)=P(F)P(E)\\). En este caso decimos que los eventos \\(E\\) y \\(F\\) son independientes. Nótese que esto no quiere decir que no haya ninguna conexión entre \\(E\\) y \\(F\\) (puede ser que la ocurrencia de \\(F\\) cambie las maneras en que puede ocurrir \\(E\\)), sólo que la probabilidad de uno no cambia al condicionar al otro.\n\nEjemplos\n\nMuestra que si sabemos que sacar un corazón de una baraja de 52 cartas es independiente de sacar un as\nMuestra que en nuestro modelo equiprobable de dos volados, el resultado de un volado es independiente del resultado del otro.\nEl evento “la suma de la tirada de dos dados” es independiente del resultado del primer dado."
  },
  {
    "objectID": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "href": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.10 Independencia de más de dos eventos",
    "text": "7.10 Independencia de más de dos eventos\nNótese que cuando los eventos \\(E\\) y \\(F\\) son indpendientes, por definición\n\\[P(E \\,y\\, F) = P(E) P(F)\\]\nDecimos que los eventos \\(E\\), \\(F\\) y \\(G\\) son independientes cuando\n\n\\(P(E \\,y \\,F \\, y \\, G) = P(E)P(F)P(G)\\)\n\\(P(E \\,y \\,F ) = P(E)P(F)\\)\n\\(P(E \\,y \\, G) = P(E)P(G)\\)\n\\(P(F \\, y \\, G) = P(F)P(G)\\)\n\ny así sucesivamente para un número mayor de eventos: si los eventos son independientes, la probabilidad de que ocurran cualquier subconjunto de ellos es es el producto de la probabilidades de que cada uno de ellos ocurra.\nEn general, si \\(E_1, E_2, \\ldots, E_n\\) son eventos independientes, entonces\n\\[P(E_1\\, y \\, E_2 \\, y \\cdots \\,y E_n) = P(E_1)P(E_2)\\cdots P(E_n)\\]\n\nEjercicio\nHacemos un número indefinido de pruebas independientes, y cada una de ellas puede resultar en éxito con probabilidad \\(p\\) y fracaso con probabilidad \\(1-p\\). Calcula la probabilidad de que 1) al menos un éxito suceda en la primeras 20 pruebas. 2) todas las pruebas sean éxito y 3) exactamente 5 de las 2o pruebas sean éxito.\n\n\nEjemplo: número de seises\nConstruye un modelo para el número de seises en dos tiradas de dados. Escribimos \\(X\\)= número de seises que obtenemos en dos tiradas de dado.\nLos resultados posibles son 0, 1 y 2 seises. Para calcular la probablidad de tirar dos seises hacemos:\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2 | S_1) P(S_1).\\]\nahora, si suponemos que el primer tiro no afecta de ninguna manera el resultado del segundo tiro, entonces \\(P(S_2|S_1) = P(S_2)\\), y la fórmula es\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2) P(S_1) = (1/6)(1/6) = 1/36.\\]\nUsando el mismo argumento podemos calcular de la probabilidad de obtener ningún seis es\n\\[P(X=0) = P(S_2^c \\, y \\, S_1^c ) = P(S_2^c)P(S_1^c) = (5/6)(5/6) = 25/36\\]\nLa probabilidad restante se puede calcularse directamente, o notar que como 0, 1 y 2 son los únicos posibles resultados, entonces\n\\[P(X=1) = 1- P(X=0) - P(X=2) = 1 - 25/36 - 1/36 = 10/36\\]\nCheca tus resultados usando simulación.\nObservación: en muchos casos, la independencia se construye como un supuesto para construir modelos más complejos, cuando este supuesto es adecuado. Un ejemplo es cuando tomamos muestras de una población: si tomamos cada muestra independientemente de las otras, analizar los resultados es mucho más fácil que cuando hay esquemas complejos de dependencias entre los datos que xtraemos."
  },
  {
    "objectID": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "href": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "title": "7  Probabilidad condicional e independencia",
    "section": "7.11 Intro a estimación por máxima verosimilitud",
    "text": "7.11 Intro a estimación por máxima verosimilitud\nUno de los procedimientos más estándar para estimar cantidades desconocidas es el método de máxima verosimilitud. Los estimadores de máxima verosimilitud tienen propiedades convenientes, y dan en general resultados razonables siempre y cuando los supuestos sean razonables.\nMáxima verosimilitud es un proceso intuitivo, y consiste en aprender o estimar valores de parámetros desconocidos suponiendo para los datos su explicación más probable. Para esto, usando supuestos y modelos requeriremos calcular la probabilidad de un conjunto de observaciones.\n\nEjemplo 1\nSupongamos que una máquina produce dos tipos de bolsas de 25 galletas: la mitad de las veces produce una bolsa con 5 galletas de avena y 20 de chispas de chocolate, y la otra mitad produce galletas con 23 galletas de avena y 2 de chispas de chocolate.\nTomamos una bolsa, y no sabemos qué tipo de bolsa es (parámetro desconocido). Extraemos al azar una de las galletas, y es de chispas de chocolate (observación).\nPor máxima verosimilitud, inferimos que la bolsa que estamos considerando tiene 5 galletas de avena. Esto es porque es más probable observar una galleta de chispas en las bolsas que contienen 5 galletas de avena que en las bolsas que contienen 23 galletas de avena. Podemos cuantificar la probabilidad que “acertemos” en nuestra inferencia.\n\nCómo se aprecia en el ejemplo anterior, el esquema general es:\n\nExiste un proceso del que podemos obtener observaciones de algún sistema o población real.\nTenemos un modelo probabilístico que dice cómo se producen esas observaciones a partir del sistema o población real.\nUsualmente este modelo tiene algunas cantidades que no conocemos, que rigen el proceso y cómo se relaciona el proceso con las observaciones.\n\nNuestro propósito es:\n\nExtraemos observaciones del proceso:\n\n\\[x_1, x_2, \\ldots, x_n\\]\n\nQueremos aprender de los parámetros desconocidos del proceso para calcular cantidades de interés acerca del sistema o población real\n\nEn principio, los modelos que consideramos pueden ser complicados y tener varias partes o parámetros. Veamos primero un ejemplo clásico con un solo parámetro, y cómo lo resolveríamos usando máxima verosimilitud.\nNota: Cuando decimos muestra en general nos referimos a observaciones independientes obtenidas del mismo proceso (ver la sección anterior para ver qué significa que sea independientes). Este esquema es un supuesto que simplifica mucho los cálculos, como discutimos antes. Muchas veces este supuesto sale del diseño de la muestra o del estudio, pero en todo caso es importante considerar si es razonable o no para nuestro problema particular.\n\n\nEjemplo\nSupongamos que queremos saber qué proporción de registros de una base de datos tiene algún error menor de captura. No podemos revisar todos los registros, así que tomamos una muestra de 8 registros, escogiendo uno por uno al azar de manera independiente. Revisamos los 8 registros, y obtenemos los siguientes datos:\n\\[x_1 = 0, x_2 = 1, x_3 = 0, x_4 = 0, x_5 =1, x_6 =0, x_7 =0, x_8 =0\\]\ndonde 1 indica un error menor. Encontramos dos errores menores. ¿Cómo estimamos el número de registros con errores leves en la base de datos?\nYa sabemos una respuesta razonable para nuestro estimador puntual, que sería \\(\\hat{p}=2/8=0.25\\). Veamos cómo se obtendría por máxima verosimilitud.\nSegún el proceso con el que se construyó la muestra, debemos dar una probabilidad de observar los 2 errores en 8 registros. Supongamos que en realidad existe una proporción \\(p\\) de que un registro tenga un error. Entonces calculamos\nProbabilidad de observar la muestra:\n\\[P(X_1 = 0, X_2 = 1, X_3 = 0, X_4 = 0, X_5 =1, X_6 =0, X_7 =0, X_8 =0)\\]\nes igual a\n\\[P(X_1 = 0)P(X_2 = 1)P(X_3 = 0)P( X_4 = 0)P(X_5 =1)P(X_6 =0)P(X_7 =0)P(X_8 =0)\\]\npues la probabilidad de que cada observación sea 0 o 1 no depende de las observaciones restantes (la muestra se extrajo de manera independiente).\nEsta ultima cantidad tiene un parámetro que no conocemos: la proporcion \\(p\\) de registros con errores. Así que lo denotamos como una cantidad desconocida \\(p\\). Nótese entonces que \\(P(X_2=1) = p\\), \\(P(X_3=0) = 1-p\\) y así sucesivamente, así que la cantidad de arriba es igual a\n\\[p(1-p)p(1-p)(1-p)p(1-p)(1-p)(1-p) \\]\nque se simplifica a\n\\[ L(p) = p^2(1-p)^6\\]\nAhora la idea es encontrar la p que maximiza la probabilidad de lo que observamos. En este caso se puede hacer con cálculo, pero vamos a ver una gráfica de esta función y cómo resolverla de manera numérica.\n\nverosimilitud <- function(p){\n  p^2 * (1-p)^6\n}\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosimilitud))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.25, color = \"red\") +\n  xlab(\"p\")\n\n\n\n\nNótese que esta gráfica:\n\nDepende de los datos, que pensamos fijos.\nCuando cambiamos la \\(p\\), la probabilidad de observar la muestra cambia. Nos interesa ver las regiones donde la probabilidad es relativamente alta.\nEl máximo está en 0.25.\nAsí que el estimador de máxima verosimilitud es \\(\\hat{p} = 0.25\\)\n\n\nObsérvese que para hacer esto usamos:\n\nUn modelo teórico de cómo es la población con parámetros y\nInformación de como se extrajo la muestra,\n\ny resolvimos el problema de estimación convirtiéndolo en uno de optimización.\nProbamos esta idea con un proceso más complejo:\n\n\n7.11.1 Ejemplo 2\nSupongamos que una máquina puede estar funcionando correctamente o no en cada corrida. Cada corrida se producen 500 productos, y se muestrean 10 para detectar defectos. Cuando la máquina funciona correctamente, la tasa de defectos es de 3%. Cuando la máquina no está funcionando correctamente la tasa de defectos es de 10%\nSupongamos que escogemos al azar 11 corridas, y obervamos los siguientes datos de número de defectuosos:\n\\[1, 0, 0, 3 ,0, 0, 0, 2, 1, 0, 0\\]\nLa pregunta es: ¿qué porcentaje del tiempo la máquina está funcionando correctamente?\nPrimero pensemos en una corrida. La probabilidad de observar una sucesión particular de \\(r\\) defectos es\n\\[0.03^r(0.97)^{(10-r)}\\]\ncuando la máquina está funcionando correctamente.\nSi la máquina está fallando, la misma probabilidad es\n\\[0.2^r(0.8)^{(10-r)}\\]\nAhora supongamos que la máquina trabaja correctamente en una proporción \\(p\\) de las corridas. Entonces la probabilidad de observar \\(r\\) fallas se calcula promediando (probabilidad total) sobre las probabilidades de que la máquina esté funcionando bien o no:\n\\[0.03^r(0.97)^{(10-r)}p + 0.2^r(0.8)^{(10-r)}(1-p)\\]\nY esta es nuestra función de verosimilitud para una observación.\nSuponemos que las \\(r_1,r_2, \\ldots, r_{11}\\) observaciones son independientes (por ejemplo, después de cada corrida la máquina se prepara de una manera estándar, y es como si el proceso comenzara otra vez). Entonces tenemos que multiplicar estas probabilidades para cada observación \\(r_1\\):\n\ncalc_verosim <- function(r){\n  q_func <- 0.03^r*(0.97)^(10-r)\n  q_falla <- 0.2^r*(0.8)^(10-r)\n  function(p){\n    #nota: esta no es la mejor manera de calcularlo, hay \n    # que usar logaritmos.\n    prod(p*q_func + (1-p)*q_falla)\n  }\n}\nverosim <- calc_verosim(c(1, 0, 0, 3, 0, 0, 0, 2, 1, 0, 0))\nverosim(0.1)\n\n[1] 2.692087e-14\n\n\n\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosim))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.8, color = \"red\") +\n  xlab(\"prop funcionado\")\n\n\n\n\nY nuestra estimación puntual sería de alrededor de 80%.\n\n\nAspectos numéricos\nCuando calculamos la verosimilitud arriba, nótese que estamos multiplicando números que pueden ser muy chicos (por ejemplo \\(p^6\\), etc). Esto puede producir desbordes numéricos fácilmente. Por ejemplo para un tamaño de muestra de 1000, podríamos tener que calcular\n\np <- 0.1\nproba <- (p ^ 800)*(1-p)^200\nproba\n\n[1] 0\n\n\nEn estos casos, es mejor hacer los cálculos en la escala logarítmica. El logaritmo convierte productos en sumas, y baja exponentes multiplicando. Si calculamos en escala logaritmica la cantidad de arriba, no tenemos problema:\n\nlog_proba <- 800 * log(p) + 200 * log(1-p)\nlog_proba\n\n[1] -1863.14\n\n\nAhora notemos que\n\nMaximizar la verosimilitud es lo mismo que maximizar la log-verosimilitud, pues el logaritmo es una función creciente. Si \\(x_{max}\\) es el máximo de \\(f\\), tenemos que \\(f(x_{max})>f(x)\\) para cualquier \\(x\\), entonces tomando logaritmo, \\[log(f(x_{max}))>log(f(x))\\] para cualquier \\(x\\), pues el logaritmo respeta la desigualdad por ser creciente.\nUsualmente usamos la log verosimilitud para encontrar estimador de máxima verosimilitud\nHay razónes teóricas y de interpretación por las que también es conveniente hacer esto.\n\n\n\n\n\nRoss, Sheldon M. 1998. A First Course in Probability. Fifth. Upper Saddle River, N.J.: Prentice Hall."
  },
  {
    "objectID": "modelos-continuos.html",
    "href": "modelos-continuos.html",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "",
    "text": "En la parte anterior consideramos un númeroo fijo de resultados numéricos de experimentos aleatorios, por ejemplo, cuando \\(X\\) el resultado de una tirada de dado. En este caso, un modelo de probabilidad para \\(X\\) asigna una probabilidad dada a cada posible resultado, por ejemplo\n\\[P(X=1) = 1/6\\]\ne igualmente \\(P(X=2)=\\cdots = P(X=6) = 1/6\\). En muchos casos, la cantidad \\(X\\) que nos interesa puede tomar valores numéricos arbitrarios, y en este esquema no está claro cómo asignaríamos probabilidades."
  },
  {
    "objectID": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "href": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.1 Modelo equiprobable o uniforme",
    "text": "8.1 Modelo equiprobable o uniforme\nLos modelos más simple para una medición continua \\(X\\) son los modelos uniforme.\nPara nuestra ruleta, por ejemplo, \\(X\\) puede tomar valores en el intervalo \\([0, 360)\\). Si la ruleta es justa, entonces la probabilidad de que la flecha caiga en cualquier sector \\([a,b]\\) debe ser igual. Una manera de lograr esto usar como probabilidad la proporción de la longitud de \\([a,b]\\) con respecto al total de \\([0, 360)\\):\n\\(P(X\\in [a,b]) = \\frac{b-a}{360}.\\)\n\nDiscute por qué esta asignación de probabilidades satisface las tres reglas básicas de probabilidad (axiomas) que presentamos anteriormente.\nEste es el equivalente continuos para espacios equiprobables con un número finito de resultados.\n\n::: {.cell type=‘comentario’}\n\nSupongamos que una variable aleatoria puede tomar valores en el intervalo \\([L,U]\\). La variable aleatoria es uniforme en \\([L,U]\\) cuando\n\\[P(X \\in [a,b]) = \\frac{b-a}{L-U}\\]\n</div>\\EndKnitrBlock{comentario}\n:::\n\n8.1.1 Ejemplo: ruleta sesgada\nAhora supongamos que nuestra ruleta no está del todo balanceada. Por ejemplo, podría ser estuviera colgada en una pared, y al girar la flecha es un poco más probable que la flecha apunte hacia el piso en lugar de hacia el cielo.\nEn este caso, si la dirección hacia arriba es 90 grados y hacia abajo es 270 grados, quisiéramos por ejemplo que\n\\[P(260 < X <280) < P(80 < X < 100)\\]\nY nótese que debe ser posible asignar probabilidades a cualquier sector de la ruleta con el nuevo modelo que propongamos. ¿Cómo podríamos modificar nuestra asignación de probabilidades?\nUna de las maneras más fáciles es pensando que nuestra probabilidad se obtiene integrando una funcion constante:\n\nSi \\([a,b]\\) es un sector de la ruleta con \\(a<b\\), podríamos poner\n\n\\[P(X\\in [a,b]) = \\int_a^b \\frac{1}{360} \\,dx = \\frac{b-a}{360}\\]\nDe forma que si \\(f(x)= 1/360\\) para valores \\(0 \\leq x < 360\\), nuestra probabilidad se escribe como la integral\n\\[P(X\\in [a,b]) = \\int_a^b f(x) \\,dx \\]\n\nEn este caso, probabilidad es área bajo la curva \\(f(x)=1/360\\) que se calcula integrando sobre el intervalo de interés\n\nPara generalizar la idea es la siguiente:\n\nUsamos la fórmula anterior, pero modificamos o perturbamos la función \\(f(x) = 1/360\\) para que \\(f\\) sea un poco más alta alrededor de 270 grados (abajo), y un poco más baja alrededor de 90 grados (arriba).\nLo único que necesitamos es que \\(f(x)\\) no puede tomar valores negativos (por que si no obtendríamos probabilidades negativas en algunos sectores), y la integral sobre la ruleta completa debe ser uno:\n\n\\[P(X\\in [0, 360]) = \\int_0^{360} f(x)\\,dx = 1\\]\nPodríamos utilizar por ejemplo:\n\nf_dens <- function(x){\n    x_rad <- 2 * pi * x / 360\n    (1/360) +  0.0002 * cos(x_rad - 3 * pi  / 2)\n}\ngraf_1_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"uniforme\",\n                   f = 1 / 360) \ngraf_2_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"colgada\") %>% \n    mutate(f = f_dens(angulo))\ngraf_tbl <- bind_rows(graf_1_tbl, graf_2_tbl)\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1)\n\n\n\n\nEl cálculo se hace ahora con área bajo la curva. Para calcular la probabilidad\n\\[P(X\\in [50, 130]),\\]\nintegramos la función \\(f\\) correspondiente, que corresponde a calcular área bajo la curva:\n\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1) +\n    geom_area(aes(x = ifelse(angulo > 50 & angulo < 130, angulo, 0)), \n              fill=\"salmon\", alpha = 0.5)\n\n\n\n\nY ahora vemos que para la versión perturbada, más de la probabilidad se concentra alrededor de 270 grados que alrededor de 90. Por las propiedades de la integral, todas las propiedades usuales de probabilidad se cumplen."
  },
  {
    "objectID": "modelos-continuos.html#funciones-de-densidad",
    "href": "modelos-continuos.html#funciones-de-densidad",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.2 Funciones de densidad",
    "text": "8.2 Funciones de densidad\nCuando trabajamos con mediciones de tipo continuo, es más conveniente definir asignaciones de probabilidad utilizando funciones de densidad de probabilidad:\n\n\n\nUna función \\(f(x)\\) no negativa cuya integral es igual a 1 es una función de densidad de probabilidad. Las probabilidades asociadas se calculan integrando:\n\\[P(X\\in [a,b]) = \\int_a^b f(x)\\,dx\\]\nEn este caso decimos que \\(f\\) es la función de densidad de probabilidad asociada a la variable aleatoria \\(X\\). A este tipo de variables aleatorias les llamamos continuas."
  },
  {
    "objectID": "modelos-continuos.html#ejemplo-densidad-triangular",
    "href": "modelos-continuos.html#ejemplo-densidad-triangular",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.3 Ejemplo: densidad triangular",
    "text": "8.3 Ejemplo: densidad triangular\nSupongamos que tenemos una variable aleatoria que tiene mediana 2, y puede tomar valores entre 0 y 4. Podríamos definir una densidad como sigue: Si \\(x\\) está entre 0 y 2, entonces\n\\[f(x) = \\frac{x}{4}\\]\ny si \\(x\\) está entre 2 y 4, entonces\n\\[f(x) = 1 - \\frac{x}{4}\\]\n\ndens_triangular <- function(x){\n    (x > 0) * (x < 4) * ifelse(x < 2, x/4, 1 - x/4)\n}\ntriangular_tbl <- tibble(x = seq(-1, 5, 0.001)) %>% \n    mutate(f = dens_triangular(x)) \nggplot(triangular_tbl, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\nEjemplo\nSupongamos que una variable \\(X\\) tiene mediana 2 y rango de 0 a 4, con densidad triangular. ¿Cuál es la probabilidad \\(P(X>1)\\)?\nSolución: Por reglas usuales de probabilidad, \\(P(X>1) = P(1<X<2) + P(X\\geq2)\\). Tenemos que \\(P(X\\geq 2) = 0.5\\). Ahora usamos la fórmula de la densidad triangular para obtener\n\\[P(1<X<2) = \\int_{1}^{2} f(x)\\,dx = \\int_1^2 \\frac{x}{4}\\,dx =\n\\left [\\frac{x^2}{8}\\right ]_1^2 = 1/2 - 1/8 = 3/8 = 0.375\\]\nde modo que\n\\[P(X<1) = 0.375 +0.500 = 0.875\\]\nEn general, podemos dar una fórmula para una densidad triangular en el intervalo \\([A,B]\\) con mediana en \\((A + B)/2\\). ¿Cómo sería la fórmula?"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "href": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.4 Cuantiles de variables aleatorias",
    "text": "8.4 Cuantiles de variables aleatorias\nAntes vimos la definición de cuantiles para datos numéricos. Podemos definirlos también para variables aleatorias numéricas:\n::: {.cell type=‘comentario’}\n\nSea \\(p\\in (0,1)\\). El cuantil-\\(p\\) de la variable \\(X\\) con función de densidad \\(f(x)\\) es el valor \\(x(p)\\) tal que\n\\[\\int_{-\\infty}^{x(p)} f(x)\\,dx = p\\]\n</div>\\EndKnitrBlock{comentario}\n:::\nObservación: nótese que usamos como límite inferior \\(-\\infty\\) para indicar que integramos \\(f\\) sobre toda la densidad que esté a la izquierda de \\(x(p)\\).\n\nEjemplo: densidad triangular\nSupongamos que \\(X\\) tiene la densidad triangular mostrada arriba. Calcula el cuartil inferior y superior (es decir, los cuantiles 0.25 y 0.75). Para el cuartil superior, por ejemplo, buscamos al \\(x(0.75)\\) de la siguiente gráfica:\n\nsource(\"R/triangular.R\")\nggplot(triangular_tbl, aes(x = x, y = f)) +\n        geom_line() +\n    geom_area(aes(x = ifelse(x > 0 & x < qtri(0.75, 0, 4), x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0, 0.7)) +\n    annotate(\"text\", x = qtri(0.75, 0, 4), y = 0.03, label = \"x(0.75)\") +\n    annotate(\"point\", x = qtri(0.75, 0, 4), y = 0.0) \n\n\n\n\nComenzaremos por el cuartil inferior. Buscamos una \\(x(0.25)\\) tal que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = 0.25\\]\nSabemos que \\(x(0.25)< 2\\), pues la integral hasta 2 es 0.5, así que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = \\int_0^{x(0.25)} x/4 \\,dx = \\left [ x^2/8\\right]_0^{x(0.25)} = (x(0.25))^2/8\\]\nSi queremos que este valor sea igual a 0.25, entonces despejando obtenemos\n\\[x(0.25) = \\sqrt{0.25(8)} = \\sqrt{2}\\approx 1.4142\\]\nAhora podríamos calcular la otra integral, pero por simetría podemos concluir que\n\\[x(0.75) = 2 + (2 - 1.4142) \\approx 2.5858\\]\ny concluimos que los cuartiles inferiores y superiores son aproximadamente 1.41 y 2.59\n\n\nEjercicio: densidad uniforme\nCalcula la mediana, y los percentiles 0.10 y 0.90 de una variable uniforme en \\([0, 10]\\)."
  },
  {
    "objectID": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "href": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.5 Comparando cuantiles teóricos y empíricos",
    "text": "8.5 Comparando cuantiles teóricos y empíricos\nLos cuantiles que vimos en la parte de descriptivos para datos numéricos se llaman usualmente cuantiles empíricos. Estos cuantiles podemos compararlos con cuantiles teóricos para ver qué tan similares son, y si el modelo teórico describe adecuadamente los datos.\n\n8.5.1 Ejemplo: distribución uniforme\nSimularemos 500 datos uniformes en \\([0, 10]\\):\n\nx_sim_u <- runif(500, 0, 10)\n\nPodríamos calcular algunos cuantiles empíricos:\n\nquantile(x_sim_u, c(0.10, 0.50, 0.90))\n\n      10%       50%       90% \n0.7228508 4.9042604 8.9673423 \n\n\nPor el ejercicio anterior sabemos cuáles son los cuantiles teóricos correspondientes a una uniforme en \\([0,10]\\). Podemos calcularlos también como sigue:\n\nqunif(c(0.10, 0.5, 0.90), 0, 10)\n\n[1] 1 5 9\n\n\nY vemos que son muy similares los cuantiles empíricos y teóricos. Una mejor manera de considerar esta similitud es graficando todos los cuantiles empíricos y comparándolos con los teóricos:\n\nggplot(tibble(x = x_sim_u), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = stats::qunif, dparams = list(min = 0, max = 10)) +\n    xlab(\"Cuantiles Teóricos U(0,10)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\nY vemos que la forma de las dos distribuciones es muy similar: los cuantiles empíricos son muy similares a los teóricos. Existen algunas fluctuaciones debidas al muestreo aleatorio.\n\n\n8.5.2 Ejemplo: distribución triangular\nRepetimos para la distribución triangular. Los cuantiles que calculamos arriba son:\n\nqtri(c(0.25, 0.75), a = 0, b = 4)\n\n[1] 1.414214 2.585786\n\n\n\nx_sim_tri <- rtri(500, 0, 4)\nggplot(tibble(x = x_sim_tri), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = qtri, dparams = list(a = 0, b = 4)) +\n    xlab(\"Cuantiles Teóricos triangular(0,4)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\nNótese que otra vez, los cuantiles teóricos se alinean bien con los teóricos."
  },
  {
    "objectID": "modelos-continuos.html#histogramas-y-densidades",
    "href": "modelos-continuos.html#histogramas-y-densidades",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.6 Histogramas y densidades",
    "text": "8.6 Histogramas y densidades\nPara el análisis de datos usual, las gráficas cuantil-cuantil tienden a ser útiles para entender si unos datos se comportan según alguna densidad teórica. Sin embargo, muchas veces se usan histogramas, como en las siguientes gráficas:\n\nhist_1 <- ggplot(tibble(x = x_sim_u),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 10, 0.01)) %>% \n                  mutate(f = dunif(x, 0, 10)),\n              aes(x = x, y = f), colour = \"red\")\nhist_2 <- ggplot(tibble(x = x_sim_tri),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.25, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 4, 0.01)) %>% \n                  mutate(f = dtri(x, 0, 4)),\n              aes(x = x, y = f), colour = \"red\")\nhist_1 + hist_2\n\n\n\n\nNótese la escala vertical de estos histogramas, que no es simplemente el conteo de casos que caen en cada intervalo del histograma. Para poder comparar los conteos con las densidades correspondientes, es necesario observar lo siguiente:\nSi \\(I = [a,b]\\) es un intervalo del histograma, según la densidad (teórica), la probabilidad de que un dato \\(x\\) caiga en \\(I\\) es\n\\[P(x\\in I) = \\int_{a}^b f(x)\\,dx \\approx f(a) (b-a)\\]\nLa última aproximación se debe a que en un intervalo chico \\([a,b]\\), el área bajo la curva de \\(f(x)\\) es aproximadamente igual a la base (el ancho del intervalo) por la altura en un punto de la curva (\\(f(a)\\), aunque también podríamos usar \\(f(\\frac{a+b}{2})\\) por ejemplo).\nSi tenemos \\(n\\) observaciones, esperamos entonces que caigan \\(nP(X\\in I)\\) en el intervalo \\(I\\), de forma que si \\(n([a,b])\\) es el número de observaciones que caen en \\(I = [a,b]\\), esperamos\n\\[\\frac{n([a,b])}{n} \\approx f(a)(b-a),\\]\ny despejando obtenemos\n\\[f(a)\\approx \\frac{n([a, b])}{n(b-a)}.\\]\nEsto implica que para aproximar la densidad, es necesario dividir las frecuencias relativas entre el ancho de los intervalos correspondientes, y de ahí la escala vertical de las gráficas de arriba.\nObservación: las gráficas de cuantiles son generalmente más prácticas para evaluar el ajuste a un modelo teórico, aunque son menos comunes."
  },
  {
    "objectID": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "href": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.7 Más descriptivos: media y desviación estándar",
    "text": "8.7 Más descriptivos: media y desviación estándar\nPodemos utilizar cuantiles para describir modelos teóricos y también conjuntos de datos (por ejemplo, mediana para centralidad y diferencia entre cuantiles 0.9 y 0.10 para dispersión), y funcionan bien en general. Sin embargo, para modelos teóricos y conjuntos de datos particulares muchas veces es más conveniente usar medidas como la media y desviación estándar.\n\n8.7.1 Media teórica y empírica\nSabemos que la media de un conjunto de datos \\(x_1,x_2,\\ldots,x_n\\) está dada por\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots x_n}{n}.\\]\nAhora consideremos qué pasa con esta cantidad cuando las \\(x_i's\\) son observaciones independientes de una distribución con densidad teórica \\(f(x)\\). Utilizaremos simulaciones de la distribución triangular que vimos arriba\n\ndatos <- tibble(x = rtri(5000, 0, 4))\nggplot(datos, aes(x = x)) + stat_bin(breaks = seq(0, 4, 0.25))\n\n\n\n\nPodemos aproximar la media de estos datos promediando los valores iniciales de los intervalos de corte ponderado por el número de casos. Lo hacemos así:\n\nparticion <- seq(0, 4, 0.25)\niniciales <- head(particion, -1) # quitar último\nagrupados_cubeta <- datos %>% \n  mutate(inicial = cut(x, breaks = particion, labels = iniciales)) %>%  \n  mutate(inicial = as.numeric(as.character(inicial))) %>% \n  group_by(inicial) %>%\n  count() %>% \n  ungroup() \nagrupados_cubeta\n\n# A tibble: 16 × 2\n   inicial     n\n     <dbl> <int>\n 1    0       37\n 2    0.25   121\n 3    0.5    185\n 4    0.75   281\n 5    1      337\n 6    1.25   429\n 7    1.5    523\n 8    1.75   581\n 9    2      602\n10    2.25   484\n11    2.5    446\n12    2.75   341\n13    3      272\n14    3.25   198\n15    3.5    123\n16    3.75    40\n\n\nY calculamos la aproximación a la media como sigue:\n\nagrupados_cubeta %>% \n  summarise(media_aprox = sum(inicial * n) / sum(n))\n\n# A tibble: 1 × 1\n  media_aprox\n        <dbl>\n1        1.88\n\n\nApliquemos esta idea cuando tenemos una densidad \\(f(x)\\). Dividimos el rango de la densidad en cubetas, y aproximamos la densidad por rangos, por ejemplo:\n\nparticion <- seq(0, 4, 0.25)\nvalor <- dtri(particion, 0, 4)\napprox_tbl <- tibble(x = particion, densidad = valor)\ndensidad_tri <- tibble(x = seq(0, 4, 0.01)) %>% \n  mutate(densidad = dtri(x, 0, 4))\nggplot(densidad_tri) + \n  ylab('f(x)') + \n  geom_line(aes(x = x, y= densidad), alpha = 0.8) +\n  geom_step(data = approx_tbl, aes(x = x, y = densidad), colour = \"red\") +\n  theme_minimal() \n\n\n\n\nY repetimos el mismo proceso: ponderamos los valores iniciales por la altura de la densidad:\n\napprox_tbl %>% \n  summarise(media_approx = sum(x * densidad) / sum(densidad))\n\n# A tibble: 1 × 1\n  media_approx\n         <dbl>\n1            2\n\n\nY esta es una aproximación a la media de esta distribución.\nNótese que la cantidad que estamos calculando es\n\\[\\sum_i x_i f(x_i) \\Delta\\]\ndonde \\(\\Delta\\) es igual a\n\napprox_tbl %>% summarise(suma_densidad = 1 / sum(densidad))\n\n# A tibble: 1 × 1\n  suma_densidad\n          <dbl>\n1          0.25\n\n\nque es el ancho del intervalo de las particiones. Recordamos por cálculo que esta es una la aproximación a la siguiente integral:\n\\[\\sum_i x_i f(x_i) \\Delta \\approx  \\int xf(x)\\,dx\\]\nDe modo que para pasar de media de los datos \\(\\bar{x}\\) a la media de \\(\\mu_f\\) de una distribución la equivalencia es:\n$$\n{x} = x_i xf(x) = \n$$ donde\n\nUsamos la densidad en lugar de frecuencias relativas para ponderar\nUsamos integral en lugar de suma\n\nEs decir, cuando tenemos una densidad teórica continua, es necesario integrar en lugar de sumar para calcular su media.\n\n\n8.7.2 Varianza y desviación estándar\nOtra cantidad importante es la varianza de una muestra. Es una medida de dispersión, y se calcula como\n$$\n^2 = _{i=1}^n (x_i - {x})^2\n$$\nNótese que cuando los datos están altamente concentrados alrededor del valor de la media, la varianza es chica, y cuando hay más dispersión alrededor de la media, la varianza es grande.\nLa desviación estándar es la raíz cuadrada de esta cantidad:\n$$\n = \n$$ que tiene la ventaja de que está en las mismas unidades que la variable original.\nSiguiendo el mismo patrón que arriba, tenemos que integrar en lugar de sumar, y ponderar por la densidad en lugar de la frecuencia: El equivalente en una distribución teórica es también una integral, y la varianza está definida por\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x) \\,dx\\]\nLa desviación estándar \\(\\sigma\\) es la raíz cuadrada de esta cantidad.\n\\[\\sigma = \\sqrt{\\int (x - \\mu)^2 f(x)\\,dx}\\]\nEn resumen la densidad indica la frecuencia relativa de datos que esperamos observar alrededor de cada punto, y\n\nResúmenes de una distribución teórica (como cuantiles, media, varianza, etc.) se calculan integrando ponderado por la densidad.\nResúmenes de una distribución empírica se calculan sumando ponderando por la frecuencia relativa.\n\n\n\nNotación\nPara la media de una variable aleatoria \\(X\\) con densidad \\(f\\), utilizamos la notación siguiente:\n\\[\\mu = \\int x f(x)\\,dx = E(X)\\]\ny decimos también que \\(E(X)\\) es el valor esperado de \\(X\\). Igualmente, la varianza podemos escribirla como el valor esperado de la variable \\((X-\\mu)^2\\):\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x)  = E\\left ( (X-\\mu)^2 \\right)\\]\nAunque esto requiere de un teorema simple (el teorema del estadístico inconsciente) que establece que para cualquier función \\(h\\), si \\(Y=h(X)\\) y \\(f\\) es la función de densidad de \\(X\\), entonces\n\\[E(Y) = E(h(X)) = \\int h(x) f(x) \\,dx\\]\n\n\n8.7.3 ¿Cuándo usar media y desviación estándar?\nAlgunos modelos probabilísticos son más fáciles de tratar analíticamente usando media y varianza. En esos casos, conviene usar estas medidas. Esto es especialmente cierto cuando las distribuciones son simétricas y no tienen colas muy largas.\nEn cuanto a datos observados, conviene usar media y varianza cuando pretendemos modelarlos con densidades como las del párrafo anterior. En este caso, la interpretación de estos valores se hace a través de la forma de la densidad. Es importante checar (por ejemplo usando gráficas de cuantiles) que esas densidades describen apropiadamente a los datos.\n\n\nEjercicio\n\nCalcula media y desviación estándar para una densidad triangular (0,4) y otra (0,10)\nCalcula media y desviación estándar para una densidad uniforme en (0,4) y otra uniforme en (0, 10)\nContrasta tus resultados. ¿Las medias ocurren en el lugar que esperabas? ¿Qué distribuciones presentan más dispersión de acuerdo a la desviacion estándar?"
  },
  {
    "objectID": "modelos-continuos.html#la-distribución-normal",
    "href": "modelos-continuos.html#la-distribución-normal",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.8 La distribución normal",
    "text": "8.8 La distribución normal\nLa distribución normal es una que aparece naturalmente en al teoría de probabilidad.\n\n8.8.1 Promedio de variables\nConsideremos que tiramos 40 dados justos de 6 lados, y consideramos su promedio \\(\\bar{X}\\) como resultado de nuestro experimento aleatorio. ¿Cómo se ve la distribución de probabilidades de esta variable \\(\\bar{X}\\)?\nComenzamos haciendo simulacion:\n\nsimular_bolsa <- function(num_dados = 40){\n    dados <- sample(1:6, num_dados, replace = TRUE)\n    media <- mean(dados)\n    media\n}\nsimular_bolsa()\n\n[1] 3.45\n\n\nVeamos cómo se ven los resultados si hacemos este experimento un número grande de veces:\n\nset.seed(23)\nsims_dados <- map_dbl(1:10000, ~ simular_bolsa())\nggplot(tibble(resultado = sims_dados), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.1)\n\n\n\n\nY notamos una forma de “campana” interesante. Esto se explica porque típicamente tendremos tiros bajos y altos, de modo que muchos resultados de este experimento se concentran alrededor de la media de un dado (3.5). Además, existen fluctuaciones aleatorios, y a veces tenemos un poco más de tiros altos o de tiros bajos, de forma que existe dispersión alrededor de 3.5.\nSin embargo, estas desviaciones de 3.5 no pueden ser muy grandes: por ejemplo, para tener un promedio de 1, todas las tiradas de los 40 dados tendrían que dar 1, y eso es muy poco probable. Igualmente, para que el promedio fuera cercano a 6, la gran mayoría de los 40 dados deberían de dar 6, lo cual otra vez es muy poco probable. Esto explica al menos la forma general de la forma de las colas derecha e izquierda de esta distribución.\nLos dados podrían ser diferentes (por ejemplo, un poco cargados a 1 o 6, o más cargados a valores centrales), y los argumentos de arriba también se cumplirían. Lo que es sorprendente es que, independientemente de cómo sean las particularidades de los dados, la forma analítica de esta distribución que acabamos de mostrar es la misma.\nEsta forma está descrita por la densidad normal estándar:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\]\ncuya gráfica presentamos a continuación:\n\ntibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = (1/sqrt(2*pi)) * exp(-x^2 / 2)) %>% \nggplot(aes(x = x, y = f)) + geom_line()\n\n\n\n\nA una variable \\(Z\\) que tiene esta densidad le llamamos una variable con distribución normal estándar. Comparemos cuantiles en nuestro ejemplo:\n\nggplot(tibble(resultado = sims_dados),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 40 dados\")\n\n\n\n\nY notamos que los cuantiles no corresponden, pero el espaciamiento entre los cuantiles de los datos y los teóricos de la normal estándar es el mismo. Quiere decir que estas dos distribuciones tienen la misma forma, aunque estén escaladas y centradas en distintos valores.\nProbemos con promedios de 20 observaciones triangulares en \\((0,1)\\) por ejemplo. El resultado es el mismo:\n\nset.seed(23)\nsims_tri <- map_dbl(1:10000, ~ mean(rtri(20, 0 ,1)))\nggplot(tibble(resultado = sims_tri), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\nggplot(tibble(resultado = sims_tri),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 20 triangulares (0,1)\")\n\n\n\n\nOtra vez, la forma general es la misma, aún cuando los datos están centrados y escalados de manera distinta."
  },
  {
    "objectID": "modelos-continuos.html#la-densidad-normal-estándar",
    "href": "modelos-continuos.html#la-densidad-normal-estándar",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.9 La densidad normal estándar",
    "text": "8.9 La densidad normal estándar\nComo expicamos, la densidad normal estándar está dada por\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}},\\]\ncuya gráfica es como sigue:\n\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\nLas probabilidades asociadas a una normal estándar se calculan integrando esta curva (que tiene que hacerse de forma numérica). Por ejemplo, para calcular\n\\[P(Z < 1.5),\\]\npodemos usar\n\npnorm(1.5)\n\n[1] 0.9331928\n\n\nQue es el área bajo la curva mostrada abajo:\n\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.005)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line() +\n    geom_area(aes(x = ifelse(x > -3.5 & x < 1.5, x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0,0.4)) +\n    scale_x_continuous(breaks = seq(-3.5, 3.5, 0.5))\n\n\n\n\nEsta es la forma de la densidad estándar. Podemos centrar esta campana en otro valor \\(\\mu\\) y aumentar la dispersión por un factor \\(\\sigma\\). Si \\(Z\\) es una variable normal estándar, la variable\n\\[X = \\mu + \\sigma Z\\]\nes una variable normal con parámetros \\((\\mu, \\sigma)\\), o de manera más compacta, decimos que \\(X\\) es \\(N(\\mu, \\sigma)\\). La distribución normal estándar es \\(N(0,1)\\).\nPor ejemplo, si escogemos \\(\\mu=5\\) y \\(\\sigma = 0.5\\), obtenemos:\n\nnormal_graf <- tibble(x = seq(3, 7, 0.01)) %>% \n    mutate(f = dnorm(x, 5, 0.5))\nggplot(normal_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\nPodemos mostrar juntas estas dos distribuciones:\n\ndensidad_normal <- tibble(x = seq(3, 7, 0.1)) %>% \n  mutate(densidad = dnorm(x, 5, 0.5))\ndensidad_normal_estandar <- tibble(x = seq(-3, 3, 0.1)) %>% \n  mutate(densidad = dnorm(x))\ng_2 <- ggplot(densidad_normal_estandar, aes(x = x, y = densidad)) + geom_line()\ng_3 <- g_2 + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_1 <- ggplot(densidad_normal, aes(x = x, y = densidad)) + geom_line() + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_3 + g_1\n\n\n\n\nSe puede demostrar que:\n\n\n\nDistribución normal\n\nLa distribución normal estándar \\(N(0,1)\\) tiene media 0 y desviación estándar 1\nLa distribución normal \\(N(\\mu,\\sigma)\\) tiene media \\(\\mu\\) y desviación estándar \\(\\sigma\\)"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "href": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.10 Cuantiles y concentración de la densidad normal",
    "text": "8.10 Cuantiles y concentración de la densidad normal\nCon un poco de cálculo podemos ver qué tan fuertemente se concentra la densidad alrededor de la media para una distribución normal. La regla es la siguiente:\n\n68% de la densidad se concentra en el intervalo \\([\\mu-\\sigma, \\mu+\\sigma]\\)\n95% de la densidad se concentra en el intervalo \\([\\mu-2\\sigma, \\mu+2\\sigma]\\)\n99.7% de la densidad se concentra en el intervalo \\([\\mu-3\\sigma, \\mu+3\\sigma]\\)\n\n\ngrafica_concentracion <- function(mu, sigma, z){\n  x <- seq(mu - 3.1 * sigma, mu + 3.1 * sigma, 0.01)\n  valor <- dnorm(x, mu, sigma)\n  datos <- tibble(x = x, `f(x)`=valor)\n  texto <- round(100*(pnorm(z) - pnorm(-z)), 1)\n  texto <- paste0(texto, \"%\")\n  ggplot(datos, aes(x = x, y = `f(x)`)) +\n    geom_area(data = filter(datos, x < mu + z*sigma, x > mu - z*sigma), \n      fill = \"salmon\") +\n        geom_line() +\n    annotate(\"text\", x = mu, y = 0.1, label = texto) +\n    scale_x_continuous(breaks = mu + sigma*seq(-3, 3, 1)) +\n    theme_minimal() + ylab(\"\") \n}\ng_68 <- grafica_concentracion(10, 2, 1)\ng_95 <- grafica_concentracion(10, 2, 2)\ng_997 <- grafica_concentracion(10, 2, 3)\npaneles <- g_68 + g_95 + g_997\npaneles + plot_annotation(title = \"Concentración alrededor de la media (normal)\")\n\n\n\n\nNota: esto aplica para cualquier densidad normal, independientemente de los parámetros.\nObsérvese que esto nos da una interpretación natural de la desviación estándar de una distribución normal en términos de percentiles de los datos, y la manera usual con la que entendemos la desviación estándar de la distribución normal."
  },
  {
    "objectID": "modelos-continuos.html#el-teorema-central-del-límite",
    "href": "modelos-continuos.html#el-teorema-central-del-límite",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.11 El teorema central del límite",
    "text": "8.11 El teorema central del límite\nUna de las razones por las que el modelo normal es tan importante es el siguiente resultado:\n::: {.cell type=‘comentario’}\n\nTeorema central del límite\nSi \\(X_1,X_2,\\ldots, X_n\\) son variables aleatorias independientes con media \\(\\mu\\) y desviación estándar \\(\\sigma\\) con una densidad \\(f(x)\\):\n\n\\(S_n = X_1 + X_2 + \\cdots X_n\\) es aproximadamente normal cuando \\(n\\) es suficientemente grande\n\n:::\nMuchas cantidades de interés en la estadística se pueden definir como sumas o promedios de un número grande de variables aleatorias (por ejempo, cuando queremos estimar el total de ingreso de los hogares, estatura promedio en una población, etc.) Los percentiles de una muestra grande también cumplen un teorema central del límite de este tipo.\nLa aproximación del teorema central del límite mejora cuando \\(n\\) es más grande. Aunque una regla de dedo dice que \\(n\\geq 30\\) es suficiente para muchas distribuciones, puede ser que sea necesaria usar una \\(n\\) más grande.\n\nEsto nos permite, por ejemplo, considerar nuestro primera técnica de estimación por intervalos:\n\nObservamos una muestra grande \\(x_1,\\ldots, x_n\\) de datos de una población (no necesariamente con distribución normal). Supongamos que buscamos estimar la media \\(\\mu\\) de la población con un intervalo.\nEstimamos la media con\n\n\\[\\bar{x} = \\frac{1}{n}(x_1+\\cdots + x_n) = \\frac{1}{n}\\sum_i x_i,\\]\n\nPor el teorema del límite central, {x} es aproximadamente normal, y su media es \\(\\mu\\). Esto implica que\n\n\\[P(\\mu - 2\\sigma  \\leq \\bar{x} \\leq\\mu + 2\\sigma)\\approx 0.95\\]\nDespejando \\(\\mu\\) obtenemos\n\\[P(\\bar{x} - 2\\sigma  \\leq \\mu  \\leq\\bar{x} + 2\\sigma)\\approx 0.95\\]\nFinalmente, no conocemos \\(\\sigma\\), pero la estimamos con\n\\[\\hat{\\sigma}^2 = \\frac{1}{n}((x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2) = \\frac{1}{n}\\sum_i(x_i - \\bar{x})^2\\]\nY aproximamos sustituyendo nuestra estimación:\n\\[P(\\bar{x} - 2\\hat{\\sigma } \\leq \\mu  \\leq\\bar{x} + 2\\hat{\\sigma})\\approx 0.95\\]\nEsto nos da un intervalo (llamado el intervalo de Wald) con 95% de confianza para la media poblacional:\n\\[[\\bar{x} - 2\\hat{\\sigma }  , \\bar{x} + 2\\hat{\\sigma}]\\]\nNotas:\n\nPor otras razones técnicas, a veces se usa \\(s^2 = \\frac{1}{n-1}\\sum_i (x_i-\\bar{x})^2\\) en lugar de \\(\\hat{\\sigma}^2\\). Si la muestra es grande esto no es importante.\nEstos intervalos tienen cobertura nominal de 95%, sin embargo, puede variar dependiendo del tamaño de muestra y la forma de la distribución teórica (poblacional). Existen métodos como el bootstrap donde podemos checar qué tan razonable es hacer esta aproximación. También se puede hacer simulación modelando la distribución \\(f(x)\\)."
  },
  {
    "objectID": "modelos-continuos.html#otras-densidades-comunes",
    "href": "modelos-continuos.html#otras-densidades-comunes",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.12 Otras densidades comunes",
    "text": "8.12 Otras densidades comunes\nComo vimos arriba, consideraciones teóricas hacen razonable suponer que una variable aleatoria tiene cierta distribución: por ejemplo, si una variable aleatoria es suma de muchas perturbaciones independientes, la suma o promedio resultante puede modelarse como una distribución normal.\nOtras consideraciones teóricas sugieren otro tipo formas útiles de densidades. Un primer ejemplo es la distribución exponencial.\n\n8.12.1 Variables aleatorias exponenciales\nSupongamos que estamos modelando tiempos a la ocurrencia \\(X\\) de un evento (por ejemplo en análisis de supervivencia en estudios clínicos). Esta es una variable que toma valores en los números positivos. ¿Cómo podría ser su forma?\nConsideremos por ejemplo que el tiempo de espera no tienen memoria. Es decir: si hay esperamos un periodo de \\(t\\) días por ejemplo, la distribución del tiempo restante que tenemos que esperar no depende de \\(s\\). En términos de probabilidad, podríamos escribir:\n\\[P(X > s + t | X > t) = P(X > s)\\]\nEsto se lee: dado que el evento ocurre en más de \\(t\\) días, la probabilidad de que tarde al menos otros \\(s\\) días no dependen de \\(t\\). A la función \\(S(t) = P(X>t)\\) muchas veces se le llama la función de supervivencia.\nEste es un modelo base útil, que después puede extenderse a procesos donde los eventos ocurren aceleradamente (envejecen), o donde los eventos dan evidencia de robustez (los que han sobrevivido hasta cierto tiempo se espera que duren más que lo que inicialmente), o quizá una combinación de los dos dependiendo de el valor de \\(t\\).\nCon el supuesto de falta de memoria, la ecuación de arriba se cumple, y entonces (¿por qué?):\n\\[P(X > s + t) = P(X > s) P(X > t),\\]\nasí que\n\\[\\frac{1}{s}(P(X > s+ t) - P(X>t)) = \\frac{1}{s}(P(X>s) - 1)P(X>t).\\]\nQue podemos reescribir usando la densidad \\(f(x)\\) como\n\\[\\frac{1}{s}\\int_t^{t+s} f(x)\\,dx = P(X>t)\\frac{1}{s}\\int_0^s f(x)\\,dx\\]\nConforme \\(s\\) se hace más chica, el lado izquierdo converge a \\(f(t)\\). El lado derecho, por otra parte, converge a \\(f(0)\\), y obtenemos\n\\[f(t) = f(0)\\int_t^\\infty f(x)\\,dx,\\]\ny ahora derivamos de ambos lados para obtener\n\\[f'(t) = -f(0)f(t)\\]\nLa única función que satisface esta propiedad (su derivada es proporcional a ella misma) es\n\\[f(t) = \\lambda e^{-\\lambda t}\\]\nPuedes checar que efectivamente esta densidad cumple que la ecuación anterior. A esta densidad le llamamos la densidad exponencial con tasa \\(\\lambda\\).\n\nlambda <- 1\nsim_exp <- rexp(1000, rate = lambda)\nggplot(tibble(x = sim_exp), aes(x = x)) +\n    geom_histogram(boundary = 0)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEjercicio: demuestra que una variable aleatoria exponencial con tasa \\(\\lambda\\) tiene media \\(E(X) = 1/\\lambda\\) y $Var(X) = 1 / ^2\nEl parámetro \\(\\lambda\\) se llama tasa por la interpretación de tiempo de espera que mostramos arriba. Supongamos que \\(\\lambda = 10\\). Eso quiere decir que esperamos observar el evento en \\(1/\\lambda = 1/10\\) minutos, por ejemplo, o lo que es lo mismo, a una tasa de \\(\\lambda = 10\\) eventos por minuto.\nNótese finalmente que todas las variables exponenciales son escalamientos de una variable exponencial con \\(\\lambda = 1\\). Esto es porque si \\(Y=kX\\), y \\(X\\) es exponencial con \\(\\lambda= 1\\) entonces:\n\\[P(Y>t)=P(kX > t) = P(X > t/k) = \\int_{t/k}^\\infty e^{-x}\\,dx = e^{-t/k}\\]\nDe modo que derivando, encontramos que la densidad de \\(Y\\) es \\(\\frac{1}{k} e^{-t/k}\\), que es una exponencial con tasa \\(\\lambda = 1/k\\).\n\n\n8.12.2 Ejemplo: exponencial\nSupongamos que un tipo de focos tienen tiempos de vida exponencial con una vida media de 10 años. ¿Cuál es la probabilidad de que un foco dure más de 15 años? Si tenemos un foco que ya vivió 10 años, cuál es la probabilidad de que dure otros 15 años.\nTenemos que la vida de un foco es una variable \\(X\\) exponencial con parámetro \\(\\lambda = 1/10\\). La probabilidad de que dure más de 15 años es entonces\n\\[P(X>15) = \\int_{15}^\\infty \\frac{1}{10} e^{x/10}\\,dx\\]\nPodemos calcular a mano, o usar rutinas usuales de R:\n\npexp(15, rate = 1/10, lower.tail = FALSE)\n\n[1] 0.2231302\n\n\n\n\n8.12.3 Variables aleatorias gamma\nEsta es otra familia que extiende la familia de distribuciones exponenciales. La forma analítica de una densidad gamma con parámetro de forma \\(k>0\\) y tasa \\(\\lambda\\)\n\\[f(x) = C x^{k-1} e^{-\\lambda x}\\]\ndonde la constante \\(C\\) de normalización depende de \\(k\\) y \\(\\lambda\\).\nAbajo vemos datos simulados de densidades Gamma con distintas combinaciones de parámetros:\n\nparams_tbl <- crossing(k = c(1, 2, 5, 10), lambda = c(1/4, 1/2))\nsims_tbl <- params_tbl %>% \n    mutate(sims = map2(k, lambda, \n                       ~ rgamma(10000, shape = .x, rate = .y))) %>% \n    unnest(cols = sims)\n\n\nggplot(sims_tbl, aes(x = sims)) + \n    geom_histogram(boundary = 0, bins = 50) +\n    facet_grid(k~lambda)\n\n\n\n\nNótese que cada columna es un rescalamiento de la otra, pero las densidades de los renglones tienen distinta forma. Puedes ver aquí parámetros como esperanza, varianza de esta estas distribuciones, junto con otras propiedades y aplicaciones."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "href": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.13 Modelos conjuntos de probabilidad",
    "text": "8.13 Modelos conjuntos de probabilidad\nUsualmente no nos interesa una sola variable aleatoria, sino varias. Nos interesa entender cómo están relacionadas o cómo depende una de otra.\nPor ejemplo, ¿cuál es la mediana de peso para un infante de 2 meses, y qué tan diferente es de la mediana de peso para un infante de 5 meses? ¿qué relación hay entre temperatura y presión atmosférica? ¿qué relación hay entre creencias religiosas y afiliación política? En todos estos casos quisiéramos describir de distintas formas cómo ser relaciona una cantidad aleatoria con otra.\nAl principio de este curso, vimos algunas técnicas descriptivas para mostrar y explorar estas relaciones. Por ejemplo:\n\n¿Cómo cambian las preferencias de forma de tomar té dependiendo del tipo de té una persona acostumbra tomar? ¿Qué tan probable es que alguien que toma té negro use azúcar vs alguien que toma té verde? (relación entre dos variables categóricas o discretas)\n¿Cómo cambian los precios medios de las casas dependiendo del vecindario donde se ubican? (describir la dependencia de una variable numérica si sabemos el valor de una variable categórica)\n¿Cómo cambia la mediana y los cuartiles de peso de un infante dependiendo de los meses desde que nació (describir cómo depende una variable numérica de otra variable numérica)\n\nEn esta parte veremos una introducción cómo se formalizan estos conceptos en modelos probabilísticos."
  },
  {
    "objectID": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "href": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.14 Estaturas: modelando relaciones de dependencia",
    "text": "8.14 Estaturas: modelando relaciones de dependencia\nSupongamos que \\(X\\) es la edad de una persona entre 4 y 15 años. y \\(Y\\) es su estatura. La relación entre \\(X\\) y \\(Y\\) no es determinística, pues existe variación en el crecimiento para las personas por distintas razones. Por esta razón, no tiene mucho sentido dar una relación como \\(Y = 80 + 5.5 X\\), por ejemplo, pues esta relación no se cumple para prácticamente ninguna persona.\nTiene más sentido, sin embargo, decir cómo es la distribución condicional de \\(Y\\) dado que conocemos \\(X\\). Por ejemplo, podríamos hacer la hipótesis de que la mediana de estatura para una persona de edad \\(X\\) está dada por\n\\[med(Y|X) = 80 + 5.5 X\\]\nNótese que escribimos la mediana condicional de \\(Y\\) dado que conocemos el valor de \\(X\\). También podríamos escribir la media condicional de \\(Y\\) dada \\(X\\) como\n\\[E(Y|X) = 80 + 5.5 X\\]\nY estas dos cantidades tienen sentido.\nEstas cantidades claramente no determinan la variabilidad que hay de la estatura cuando conocemos \\(X\\). Podríamos entonces también especificar por ejemplo la desviación estándar condicional:\n\\[\\sigma(Y|X) = 3\\sqrt{X} \\]\nGeneralmente estas relaciones se estiman empíricamente con datos observados, pero para este ejemplo utilizaremos estos modelos fijos.\nSimulamos algunos datos con estas propiedades:\n\nedades <- runif(800, 2, 15) # edad distribuida uniforme, o grupos de edad del mismo tamaño\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rnorm(n(), mean = media, sd = desv_est))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point()\n\n\n\n\n\nObsérvese cómo en efecto la estatura esperada aumenta con la edad (condicional a la edad), y que la dispersión de estatura aumenta conforme la edad aumenta.\n\nNótese que si usamos un suavizador, podemos estimar la media condicional de nuestro modelo, que en este caso está cercana a la fórmula que establecimos en nuestro modelo:\n\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nPodemos estimar cuantiles también como vimos en secciones anteriores:\n\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 20, quantiles = c(0.10, 0.5, 0.90)) \n\n\n\n\nY observamos igualmente que la dispersión para el grupo de 15 años es cercana al doble que la dispersión para el grupo de 4 años."
  },
  {
    "objectID": "modelos-continuos.html#distribuciones-condicionales",
    "href": "modelos-continuos.html#distribuciones-condicionales",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.15 Distribuciones condicionales",
    "text": "8.15 Distribuciones condicionales\nTodo el trabajo de arriba de modelación teórica consiste entonces en definir distribuciones condicionales. En el ejemplo anterior,\n\nDimos una distribución para \\(X\\), que en este caso la tomamos uniforme en [4, 15], pues suponemos que esta es la estructura de nuestra población (hay el aproximadamente el mismo número de personas de cada edad)\nDimos una distribución para \\(Y\\) condicionada a \\(X\\). En este caso, establecimos que \\(Y|X\\) es normal con media \\(80 + 5.5X\\) y desviación estándar \\(3 * sqrt(X)\\).\n\nEstas dos partes dan un modelo conjunto para \\(X\\) y \\(Y\\): sabemos que está completamente determinado pues pudimos simular del modelo. Otra manera de entender esto es que cualquier probabilidad que involucra a \\(X\\) y \\(Y\\) puede ser calculada con la regla del producto. Aunque no entraremos en detalles, la densidad conjunta de \\(X\\) y \\(Y\\) puede definirse en este caso como\n\\[f(x,y) = f_X(x)f_{Y|X}(y|x)\\]\nSabemos que \\(f_X(x) = 1/(15- 3)\\) para \\(x\\) entre 4 y 15 años, y la forma de \\(f_{Y|X}(y|x)\\) sabemos que es normal con media y varianza conocida (en términos de x). Esta conjunta puede ser integrada sobre cualquier región (al menos en teoría) para calcular la probabilidad de interés"
  },
  {
    "objectID": "modelos-continuos.html#estaturas-variación-gamma",
    "href": "modelos-continuos.html#estaturas-variación-gamma",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.16 Estaturas: variación gamma",
    "text": "8.16 Estaturas: variación gamma\nPodemos juntar estos bloques de densidades condicionales para construir otro tipo de modelos. Por ejemplo, supondremos que la estatura dada la edad es una distribución gamma \\((k,\\lambda)\\) y la misma media y desviación estándar que vimos arriba. Como la media de una gamma de este tipo es \\(\\mu = k/\\lambda\\) y su desviación estándar es \\(\\sigma = \\sqrt{k}/\\lambda\\), podemos despejar \\(k\\) y \\(lambda\\) y hacer:\n\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\n\n\nEste modelo es muy similar al normal. Sin embargo, podríamos intentar otras variaciones si cambiamos la magnitud de la desviación estándar en relación a la media, por ejemplo:\n\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 30 + 10*edad, desv_est = 7 * (edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(medicion_y = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = medicion_y)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\n\n\nEste caso claramente no serviría para modelar estaturas, pero podemos ver cómo introdujimos asimetría considerable en las distribuciones condicionales de y dado x una vez que especificamos la media y la varianza condicionales."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "href": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.17 Modelos conjuntos para factor categórico",
    "text": "8.17 Modelos conjuntos para factor categórico\nSupongamos ahora que la variable \\(Y\\) es numérica y la variable \\(X\\) es categórica. En este caso, un modelo conjunto está definido por las probabilidades \\(P(X=x)\\) junto con densidades condicionales \\(Y|X\\).\n\n8.17.1 Ejemplo: cuentas y propinas\nSupongamos que \\(X\\) es la hora del día (comida y cena) y que \\(Y\\) es el tamaño de la cuenta.\nPodríamos establecer por ejemplo, que \\(Y|X=comida\\) es Gamma con media 10 dólares y desviacion estándar 10 dólares. Sin embargo, \\(Y|X=cena\\) es Gamma con media 25 dólares y desviación estándar 20 dólares. Simulamos y analizamos:\n\ndatos_tbl <- \n   tibble(hora = sample(c(\"comida\", \"cena\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(hora == \"comida\", 10, 25),\n          desv_est = ifelse(hora == \"comida\", 10, 20)) %>% \n   mutate(cuenta = rgamma(n(), shape = (media / desv_est)^2,\n                               rate = media / (desv_est^2)))\nggplot(datos_tbl, aes(x = hora, y = cuenta)) +\n   geom_boxplot()\n\n\n\n\nY esta gráfica busca mostrar una estimación de las distribuciones condicionales de cuenta dado el turno donde el cliente consumió."
  },
  {
    "objectID": "modelos-continuos.html#modelos-multivariados",
    "href": "modelos-continuos.html#modelos-multivariados",
    "title": "8  Modelos probabilísticos para variables continuas",
    "section": "8.18 Modelos multivariados",
    "text": "8.18 Modelos multivariados\nAhora consideremos que queremos construir un modelo conjunto para tres variables \\(X, Y\\) y \\(Z\\). La forma en que procedemos dependerá del problema particular, pero podemos usar la regla del producto como guía. Por ejemplo, podríamos dar una distribución para \\(Z\\), luego una densidad condicional de Y dado \\(Z\\), y finalmente una condicional de \\(Y\\) dada tanto \\(X\\) como \\(Z\\).\n\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    Z -> X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\nEn este caso, cada variable aleatorio es nodo, y representa una distribución condicional dado sus antecesores. Esta gráfica por ejemplo, muestra una manera de escribir con la regla del producto un modelo conjunto, pero podríamos cambiar de posición los nodos dependiendo de nuestro conocimiento y el problema que queremos resolver.\nEn algunos casos, es posible simplificar la construcción del modelo eliminando algunas aristas. Supongamos por ejemplo que\n\nX es la edad de la persona\nZ es “M” of “F”\nY es su estatura\n\nEn este caso, no es necesario especificar la condicional de \\(X\\) dado \\(Z\\), pues estas dos son variables independientes. Pondríamos entonces simplemente\n\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\nY solo necesitamos especificar las distribuciones de \\(X\\), de \\(Z\\), y la condicional de \\(Y\\) dado \\(Z\\). Siguiendo nuestro ejemplo anterior, consideraremos a\n\n\\(X\\) como uniforme en \\([4,15]\\) (que es nuestro rango de edad de interés)\n\\(Z\\) es \\(M\\) con probabilidad 0.5 y \\(F\\) con probabilidad 0.5\n\nY podríamos especificar ahora: la condicional de \\(Y\\) (estatura) es normal con los siguientes parámetros:\n\nSi \\(X\\) es la edad y \\(Z=\"F\"\\), entonces la media es \\(70 + 6.5 X\\)\nSi \\(X\\) es la edad y \\(Z=\"M\"\\), entonces la media es \\(80 + 4.5 X\\)\nLa desviación estándar sólo depende de \\(X\\), y es igual a \\(4\\sqrt{X}\\).\n\nSimulamos ahora de este modelo probabilístico:\n\ndatos_tbl <- tibble(x = runif(1000, 4, 15)) %>%\n   # independientemente simulamos M o F\n   mutate(z = sample(c(\"m\", \"f\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(z==\"f\", 70 + 6.5 * x, 80 + 4.5 * x)) %>% \n   mutate(desv_est = 4 * sqrt(x)) %>% \n   mutate(estatura = rnorm(n(), media, desv_est))\ndatos_tbl %>% head(20) %>% kable()\n\n\n\n \n  \n    x \n    z \n    media \n    desv_est \n    estatura \n  \n \n\n  \n    13.917312 \n    m \n    142.62790 \n    14.922366 \n    106.17567 \n  \n  \n    9.655767 \n    m \n    123.45095 \n    12.429492 \n    118.95931 \n  \n  \n    4.178553 \n    m \n    98.80349 \n    8.176603 \n    102.90009 \n  \n  \n    4.085706 \n    m \n    98.38568 \n    8.085252 \n    99.27880 \n  \n  \n    7.510502 \n    f \n    118.81826 \n    10.962118 \n    120.93123 \n  \n  \n    9.506507 \n    m \n    122.77928 \n    12.333049 \n    131.72703 \n  \n  \n    8.311886 \n    f \n    124.02726 \n    11.532137 \n    127.80079 \n  \n  \n    10.336809 \n    f \n    137.18926 \n    12.860363 \n    138.18963 \n  \n  \n    8.578672 \n    m \n    118.60402 \n    11.715748 \n    133.69638 \n  \n  \n    11.500391 \n    f \n    144.75254 \n    13.564891 \n    121.21221 \n  \n  \n    6.668017 \n    f \n    113.34211 \n    10.329002 \n    123.89877 \n  \n  \n    12.935783 \n    f \n    154.08259 \n    14.386540 \n    135.22746 \n  \n  \n    14.904618 \n    m \n    147.07078 \n    15.442600 \n    128.11113 \n  \n  \n    11.579196 \n    m \n    132.10638 \n    13.611287 \n    143.13437 \n  \n  \n    14.921303 \n    m \n    147.14586 \n    15.451241 \n    162.54670 \n  \n  \n    8.863920 \n    m \n    119.88764 \n    11.908934 \n    114.67713 \n  \n  \n    8.487825 \n    m \n    118.19521 \n    11.653549 \n    105.22789 \n  \n  \n    4.221525 \n    m \n    98.99686 \n    8.218540 \n    94.52403 \n  \n  \n    6.803164 \n    m \n    110.61424 \n    10.433150 \n    108.54392 \n  \n  \n    14.458625 \n    m \n    145.06381 \n    15.209799 \n    145.96106 \n  \n\n\n\n\n\nY hacemos algunas gráficas descriptivas:\n\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point()\n\n\n\n\n\nDiscute qué otras cosas podrías cambiar en este modelo probabilístico para que fuera más flexible o más simple. ¿Cómo ajustarías un modelo así a datos reales?\n\n\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point() +\n   facet_wrap(~z) +\n   geom_quantile(method = \"rqss\", lambda = 10,\n                 quantiles = c(0.10, 0.5, 0.9))\n\nWarning: Computation failed in `stat_quantile()`:\n\nComputation failed in `stat_quantile()`:\n\n\n\n\n\n\n\n\nEn la modelación probabilística generalmente usamos estos mecanismos (dependencia condicional, independencia) y estos bloques (distribuciones de probabilidad dadas en términos de parámetros) para obtener estimaciones de parámetros de interés.\nLas decisiones de cómo usar estos mecanismos y bloques se desprenden de conocimiento de dominio, alcances del análisis, y siempre están sujetos a revisión dependiendo del tipo de desajustes que presenten frente a los datos reales."
  },
  {
    "objectID": "inferencia-modelos.html",
    "href": "inferencia-modelos.html",
    "title": "9  Inferencia con modelos probabilísticos",
    "section": "",
    "text": "Intro"
  },
  {
    "objectID": "inferencia-bayesiana.html",
    "href": "inferencia-bayesiana.html",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Introducción"
  },
  {
    "objectID": "modelos-bayesianos.html",
    "href": "modelos-bayesianos.html",
    "title": "10  Inferencia bayesiana",
    "section": "",
    "text": "Para esta sección seguiremos principalmente Kruschke (2015). Adicionalmente puedes ver la sección correspondiente de Chihara y Hesterberg (2018).\nEn las secciones anteriores estudiamos el método de máxima verosimilitud y métodos de remuestreo. Esto lo hemos hecho para estimar parámetros, y cuantificar la incertidumbre qué tenemos acerca de valores poblacionales. La inferencia bayesiana tiene objetivos similares.\nEl concepto probabilístico básico que utilizamos para construir estos modelos y la inferencia es el de probabilidad condicional: la probabilidad de que ocurran ciertos eventos dada la información disponible del fenómeno que nos interesa."
  },
  {
    "objectID": "modelos-bayesianos.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "href": "modelos-bayesianos.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "title": "10  Inferencia bayesiana",
    "section": "Un primer ejemplo completo de inferencia bayesiana",
    "text": "Un primer ejemplo completo de inferencia bayesiana\nConsideremos el siguiente problema: Nos dan una moneda, y solo sabemos que la moneda puede tener probabilidad \\(3/5\\) de tirar sol (está cargada a sol) o puede ser una moneda cargada a águila, con probabilidad \\(2/5\\) de tirar sol.\nVamos a lanzar la moneda dos veces y observamos su resultado (águila o sol). Queremos decir algo acerca de qué tan probable es que hayamos tirado la moneda cargada a sol o la moneda cargada a águila.\nEn este caso, tenemos dos variables: \\(X\\), que cuenta el número de soles obtenidos en el experimento aleatorio, y \\(\\theta\\), que da la probabilidad de que un volado resulte en sol (por ejemplo, si la moneda es justa entonces \\(\\theta = 0.5\\)).\n¿Qué cantidades podríamos usar para evaluar qué moneda es la que estamos usando? Si hacemos el experimento, y tiramos la moneda 2 veces, podríamos considerar la probabilidad\n\\[P(\\theta = 0.4 | X = x)\\]\ndonde \\(x\\) es el número de soles que obtuvimos en el experimento. Esta es la probabilidad condicional de que estemos tirando la moneda con probabilidad de sol 2/5 dado que observamos \\(x\\) soles. Por ejemplo, si tiramos 2 soles, deberíamos calcular\n\\[P(\\theta=0.4|X=2).\\]\n¿Cómo calculamos esta probabilidad? ¿Qué sentido tiene?\nUsando reglas de probabildad (regla de Bayes en particular), podríamos calcular\n\\[P(\\theta=0.4|X=2) = \\frac{P(X=2 | \\theta = 0.4) P(\\theta =0.4)}{P(X=2)}\\]\nNota que en el numerador uno de los factores, \\(P(X=2 | \\theta = 0.4),\\) es la verosimilitud. Así que primero necesitamos la verosimilitud:\n\\[P(X=2|\\theta = 0.4) = (0.4)^2 = 0.16.\\]\nLa novedad es que ahora tenemos que considerar la probabilidad \\(P(\\theta = 0.4)\\). Esta cantidad no la habíamos encontrado antes. Tenemos que pensar entonces que este parámetro es una cantidad aleatoria, y puede tomar dos valores \\(\\theta=0.4\\) ó \\(\\theta = 0.6\\).\nConsiderar esta cantidad como aleatoria requiere pensar, en este caso, en cómo se escogió la moneda, o qué sabemos acerca de las monedas que se usan para este experimento. Supongamos que en este caso, nos dicen que la moneda se escoge al azar de una bolsa donde hay una proporción similar de los dos tipos de moneda (0.4 ó 0.6). Es decir el espacio parametral es \\(\\Theta = \\{0.4, 0.6\\},\\) y las probabilidades asociadas a cada posibilidad son las mismas. Es decir, tenemos\n\\[P(\\theta = 0.4) = P(\\theta = 0.6) =0.5,\\]\nque representa la probabilidad de escoger de manera aleatoria la moneda con una carga en particular.\nAhora queremos calcular \\(P(X=2)\\), pero con el trabajo que hicimos esto es fácil. Pues requiere usar reglas de probabilidad usuales para hacerlo. Podemos utilizar probabilidad total \\[\\begin{align}\nP(X) &= \\sum_{\\theta \\in \\Theta} P(X, \\theta)\\\\\n&= \\sum_{\\theta \\in \\Theta} P(X\\, |\\, \\theta) P(\\theta),\n\\end{align}\\] lo cual en nuestro ejemplo se traduce en escribir\n\\[ P(X=2) = P(X=2|\\theta = 0.4)P(\\theta = 0.4) + P(X=2|\\theta=0.6)P(\\theta =0.6),\\]\npor lo que obtenemos\n\\[P(X=2) = 0.16(0.5) + 0.36(0.5) = 0.26.\\]\nFinalmente la probabilidad de haber escogido la moneda con carga \\(2/5\\) dado que observamos dos soles en el lanzamiento es\n\\[P(\\theta=0.4|X=2) = \\frac{0.16(0.5)}{0.26} \\approx  0.31.\\]\nEs decir, la probabilidad posterior de que estemos tirando la moneda \\(2/5\\) baja de 0.5 (nuestra información inicial) a 0.31.\nEste es un ejemplo completo, aunque muy simple, de inferencia bayesiana. La estrategia de inferencia bayesiana implica tomar decisiones basadas en las probabilidades posteriores.\n\n\n\n\n\n\nTip\n\n\n\n¿Cuál sería la estimación de máxima verosimilitud para este problema? ¿Cómo cuantificaríamos la incertidumbre en la estimación de máxima verosimilitud?\n\n\nFinalmente, podríamos hacer predicciones usando la posterior predictiva. Si \\({X}_{nv}\\) es una nueva tirada adicional de la moneda que estamos usando, nos interesaría saber:\n\\[P({X}_{nv}=\\mathsf{sol}\\, | \\, X=2)\\]\nNotemos que un volado adicional es un resultado binario. Por lo que podemos calcular observando que \\(P({X}_{nv}|X=2, \\theta)\\) es una variable Bernoulli con probabilidad \\(\\theta\\), que puede valer 0.4 ó 0.6. Como tenemos las probabilidades posteriores \\(P(\\theta|X=2)\\) podemos usar probabilidad total, condicionado en \\(X=2\\): \\[\\begin{align*}\nP({X}_{nv}=\\mathsf{sol}\\, | \\, X=2) & = \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}, \\theta \\, | \\, X=2) & \\text{(probabilidad total)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta , X=2) P(\\theta \\, | \\, X=2) & \\text{(probabilidad condicional)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta ) P(\\theta \\, | \\, X=2), & \\text{(independencia condicional)}\n\\end{align*}\\]\nlo que nos da el siguiente cálculo\n\\[P(X_{nv}=\\mathsf{sol}\\, |\\, \\theta=0.4) \\,  P(\\theta=0.4|X=2) \\,  +\\, P(X_{nv}=\\mathsf{sol}|\\theta = 0.6) \\, P(\\theta =0.6|X=2)\\]\nEs decir, promediamos ponderando con las probabilidades posteriores. Por lo tanto obtenemos\n\\[P(X_{nv} = \\mathsf{sol}|X=2) =  0.4 ( 0.31) + 0.6 (0.69) = 0.538.\\]\n\nObservación 0\nNótese que en contraste con máxima verosimilitud, en este ejemplo cuantificamos con probabilidad condicional la incertidumbre de los parámetros que no conocemos. En máxima verosimilitud esta probabilidad no tiene mucho sentido, pues nunca consideramos el parámetro desconocido como una cantidad aleatoria.\n\n\nObservación 1\nNótese el factor \\(P(X=2)\\) en la probabilidad posterior puede entenderse como un factor de normalización. Notemos que los denominadores en la distribución posterior son\n\\[P(X=2 | \\theta = 0.4) P(\\theta =0.4) = 0.16(0.5) = 0.08,\\]\ny\n\\[P(X=2 | \\theta = 0.6) P(\\theta =0.6) = 0.36(0.5) = 0.18.\\]\nLas probabilidades posteriores son proporcionales a estas dos cantidades, y como deben sumar uno, entonces normalizando estos dos números (dividiendo entre su suma) obtenemos las probabilidades.\n\n\nObservación 2\nLa nomenclatura que usamos es la siguiente:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, o modelo de los datos.\nA \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior.\n\nPara utilizar inferencia bayesiana, hay que hacer supuestos para definir las primeras dos partes del modelo. La parte de iniciales o previas está ausente de enfoques como máxima verosimlitud usual.\n\n\nObservación 3\n¿Cómo decidimos las probabilidades iniciales, por ejemplo \\(P(\\theta=0.4)\\) ?\nQuizá es un supuesto y no tenemos razón para pensar que se hace de otra manera. O quizá conocemos el mecanismo concreto con el que se selecciona la moneda. Discutiremos esto más adelante.\n\n\nObservación 4\n¿Cómo decidimos el modelo de los datos? Aquí típicamente también tenemos que hacer algunos supuestos, aunque algunos de estos pueden estar basados en el diseño del estudio, por ejemplo. Igual que cuando usamos máxima verosimilitud, es necesario checar que nuestro modelo ajusta razonablemente a los datos.\n\n\nEjercicio\nCambia distintos parámetros del número de soles observados, las probabilidades de sol de las monedas, y las probabilidades iniciales de selección de las monedas.\n\nn_volados <- 2\n# posible valores del parámetro desconocido\ntheta = c(0.4, 0.6)\n# probabilidades iniciales\nprobs_inicial <- tibble(moneda = c(1, 2),\n                        theta = theta,\n                        prob_inicial = c(0.5, 0.5))\nprobs_inicial\n\n# A tibble: 2 × 3\n  moneda theta prob_inicial\n   <dbl> <dbl>        <dbl>\n1      1   0.4          0.5\n2      2   0.6          0.5\n\n# verosimilitud\ncrear_verosim <- function(no_soles){\n    verosim <- function(theta){\n      # prob de observar no_soles en 2 volados con probabilidad de sol theta\n      dbinom(no_soles, 2, theta)\n    }\n    verosim\n}\n# evaluar verosimilitud\nverosim <- crear_verosim(2)\n# ahora usamos regla de bayes para hacer tabla de probabilidades\ntabla_inferencia <- probs_inicial %>%\n  mutate(verosimilitud = map_dbl(theta, verosim)) %>%\n  mutate(inicial_x_verosim = prob_inicial * verosimilitud) %>%\n  # normalizar\n  mutate(prob_posterior = inicial_x_verosim / sum(inicial_x_verosim))\n\ntabla_inferencia %>%\n  mutate(moneda_obs = moneda) %>%\n  select(moneda_obs, theta, prob_inicial, verosimilitud, prob_posterior)\n\n# A tibble: 2 × 5\n  moneda_obs theta prob_inicial verosimilitud prob_posterior\n       <dbl> <dbl>        <dbl>         <dbl>          <dbl>\n1          1   0.4          0.5          0.16          0.308\n2          2   0.6          0.5          0.36          0.692\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n¿Qué pasa cuando el número de soles es 0? ¿Cómo cambian las probabilidades posteriores de cada moneda?\nIncrementa el número de volados, por ejemplo a 10. ¿Qué pasa si observaste 8 soles, por ejemplo? ¿Y si observaste 0?\n¿Qué pasa si cambias las probabilidades iniciales (por ejemplo incrementas la probabilidad inicial de la moneda 1 a 0.9)?\n\n\n\nJustifica las siguientes aseveraciones (para este ejemplo):\n\n\n\n\n\n\nTip\n\n\n\n\nLas probabilidades posteriores o finales son una especie de punto intermedio entre verosimilitud y probablidades iniciales.\nSi tenemos pocas observaciones, las probabilidades posteriores son similares a las iniciales.\nCuando tenemos muchos datos, las probabilidades posteriores están más concentradas, y no es tan importante la inicial.\nSi la inicial está muy concentrada en algún valor, la posterior requiere de muchas observaciones para que se pueda concentrar en otros valores diferentes a los de la inicial.\n\n\n\nAhora resumimos los elementos básicos de la inferencia bayesiana, que son relativamente simples:\n\n\n\n\n\n\nInferencia bayesiana.\n\n\n\nCon la notación de arriba:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, proceso generador de datos o modelo de los datos.\nEl factor \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior\n\nHacemos inferencia usando la ecuación\n\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\\]\nque también escribimos:\n\\[P(\\theta | X) \\propto P(X | \\theta) P(\\theta)\\]\ndonde \\(\\propto\\) significa “proporcional a”. No ponemos \\(P(X)\\) pues como vimos arriba, es una constante de normalización.\n\n\nEn estadística Bayesiana, las probablidades posteriores \\(P(\\theta|X)\\) dan toda la información que necesitamos para hacer inferencia. ¿Cuándo damos probablidad alta a un parámetro particular \\(\\theta\\)? Cuando su verosimilitud es alta y/o cuando su probabilidad inicial es alta. De este modo, la posterior combina la información inicial que tenemos acerca de los parámetros con la información en la muestra acerca de los parámetros (verosimilitud). Podemos ilustrar como sigue:"
  },
  {
    "objectID": "modelos-bayesianos.html#ejemplo-estimando-una-proporción",
    "href": "modelos-bayesianos.html#ejemplo-estimando-una-proporción",
    "title": "10  Inferencia bayesiana",
    "section": "Ejemplo: estimando una proporción",
    "text": "Ejemplo: estimando una proporción\nRegresamos ahora a nuestro problema de estimar una proporción \\(\\theta\\) de una población dada usando una muestra iid \\(X_1,X_2,\\ldots, X_n\\) de variables Bernoulli. Ya sabemos calcular la verosimilitud (el modelo de los datos):\n\\[P(X_1=x_1,X_2 =x_2,\\ldots, X_n=x_n|\\theta) = \\theta^k(1-\\theta)^{n-k},\\]\ndonde \\(k = x_1 + x_2 +\\cdots + x_k\\) es el número de éxitos que observamos.\nAhora necesitamos una distribución inicial o previa \\(P(\\theta)\\). Aunque esta distribución puede tener cualquier forma, supongamos que nuestro conocimiento actual podemos resumirlo con una distribución \\(\\mathsf{Beta}(3, 3)\\):\n\\[P(\\theta) \\propto \\theta^2(1-\\theta)^2.\\]\nLa constante de normalización es 1/30, pero no la requerimos. Podemos simular para examinar su forma:\n\nsim_inicial <- tibble(theta = rbeta(10000, 3, 3))\nggplot(sim_inicial) + geom_histogram(aes(x = theta, y = ..density..), bins = 15)\n\n\n\n\nDe modo que nuestra información inicial es que la proporción puede tomar cualquier valor entre 0 y 1, pero es probable que tome un valor no tan lejano de 0.5. Por ejemplo, con probabilidad 0.95 creemos que \\(\\theta\\) está en el intervalo\n\nquantile(sim_inicial$theta, c(0.025, 0.975)) %>% round(2)\n\n 2.5% 97.5% \n 0.15  0.85 \n\n\nEs difícil justificar en abstracto por qué escogeriamos una inicial con esta forma. Aunque esto los detallaremos más adelante, puedes pensar, por el momento, que alguien observó algunos casos de esta población, y quizá vio tres éxitos y tres fracasos. Esto sugeriría que es poco probable que la probablidad \\(\\theta\\) sea muy cercana a 0 o muy cercana a 1.\nAhora podemos construir nuestra posterior. Tenemos que\n\\[P(\\theta| X_1=x_1, \\ldots, X_n=x_n) \\propto P(X_1 = x_1,\\ldots X_n=x_n | \\theta)P(\\theta) = \\theta^{k+2}(1-\\theta)^{n-k + 2}\\]\ndonde la constante de normalización no depende de \\(\\theta\\). Como \\(\\theta\\) es un parámetro continuo, la expresión de la derecha nos debe dar una densidad posterior.\nSupongamos entonces que hicimos la prueba con \\(n = 30\\) (número de prueba) y observamos 19 éxitos. Tendríamos entonces\n\\[P(\\theta | S_n = 19) \\propto \\theta^{19 + 2} (1-\\theta)^{30 - 19 +2} = \\theta^{21}(1-\\theta)^{13}\\]\nLa cantidad de la derecha, una vez que normalizemos por el número \\(P(X=19)\\), nos dará una densidad posterior (tal cual, esta expresion no integra a 1). Podemos obtenerla usando cálculo, pero recordamos que una distribución \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) tiene como fórmula\n\\[\\frac{1}{B(a,b)} \\theta^{a-1}(1-\\theta)^{b-1}\\]\nConcluimos entonces que la posterior tiene una distribución \\(\\mathsf{Beta}(22, 14)\\). Podemos simular de la posterior usando código estándar para ver cómo luce:\n\nsim_inicial <- sim_inicial %>% mutate(dist = \"inicial\")\nsim_posterior <- tibble(theta = rbeta(10000, 22, 14)) %>% mutate(dist = \"posterior\")\nsims <- bind_rows(sim_inicial, sim_posterior)\nggplot(sims, aes(x = theta, fill = dist)) +\n  geom_histogram(aes(x = theta), bins = 30, alpha = 0.5, position = \"identity\")\n\n\n\n\nLa posterior nos dice cuáles son las posibilidades de dónde puede estar el parámetro \\(\\theta\\). Nótese que ahora excluye prácticamente valores más chicos que 0.25 o mayores que 0.9. Esta distribución posterior es el objeto con el que hacemos inferencia: nos dice dónde es creíble que esté el parámetro \\(\\theta\\).\nPodemos resumir de varias maneras. Por ejemplo, si queremos un estimador puntual usamos la media posterior:\n\nsims %>% group_by(dist) %>%\n  summarise(theta_hat = mean(theta) %>% round(3))\n\n# A tibble: 2 × 2\n  dist      theta_hat\n  <chr>         <dbl>\n1 inicial       0.5  \n2 posterior     0.612\n\n\nNota que el estimador de máxima verosimilitud es \\(\\hat{p} = 19/30 = 0.63\\), que es ligeramente diferente de la media posterior. ¿Por qué?\nY podemos construir intervalos de percentiles, que en esta situación suelen llamarse intervalos de credibilidad, por ejemplo:\n\nf <- c(0.025, 0.975)\nsims %>% group_by(dist) %>%\n  summarise(cuantiles = quantile(theta, f) %>% round(2), f = f) %>%\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n`summarise()` has grouped output by 'dist'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   dist [2]\n  dist      `0.025` `0.975`\n  <chr>       <dbl>   <dbl>\n1 inicial      0.15    0.85\n2 posterior    0.45    0.76\n\n\nEl segundo renglón nos da un intervalo posterior para \\(\\theta\\) de credibilidad 95%. En inferencia bayesiana esto sustituye a los intervalos de confianza.\n\nEl intervalo de la inicial expresa nuestras creencias a priori acerca de \\(\\theta\\). Este intervalo es muy amplio (va de 0.15 a 0.85)\nEl intervalo de la posterior actualiza nuestras creencias acerca de \\(\\theta\\) una vez que observamos los datos, y es considerablemente más angosto y por lo tanto informativo.\n\nObservaciones:\n\nNótese que escogimos una forma analítica fácil para la inicial, pues resultó así que la posterior es una distribución beta. No siempre es así, y veremos qué hacer cuando nuestra inicial no es de un tipo “conveniente”.\nComo tenemos la forma analítica de la posterior, es posible hacer los cálculos de la media posterior, por ejemplo, integrando la densidad posterior a mano. Esto generalmente no es factible, y en este ejemplo preferimos hacer una aproximación numérica. En este caso particular es posible usando cálculo, y sabemos que la media de una \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) es \\(a/(a+b)\\), de modo que nuestra media posterior es\n\n\\[\\hat{\\mu} = (19 + 2)/(30 + 4) = 21/34 = 0.617 \\]\nque podemos interpretar como sigue: para calcular la media posterior, a nuestras \\(n\\) pruebas iniciales agregamos 4 pruebas adicionales fijas, con 2 éxitos y 2 fracasos, y calculamos la proporción usual de éxitos.\n\n\n\n\n\n\nTip\n\n\n\nRepite el análisis considerando en general \\(n\\) pruebas, con \\(k\\) éxitos. Utiliza la misma distribución inicial.\n\n\n\nLo mismo aplica para el intervalo de 95% (¿cómo se calcularía integrando?). También puedes usar la aproximación de R, por ejemplo:\n\n\nqbeta(0.025, shape1 = 22, shape2 = 14) %>% round(2)\n\n[1] 0.45\n\nqbeta(0.975, shape1 = 22, shape2 = 14) %>% round(2)\n\n[1] 0.76"
  },
  {
    "objectID": "modelos-bayesianos.html#ejemplo-observaciones-uniformes",
    "href": "modelos-bayesianos.html#ejemplo-observaciones-uniformes",
    "title": "10  Inferencia bayesiana",
    "section": "Ejemplo: observaciones uniformes",
    "text": "Ejemplo: observaciones uniformes\nAhora regresamos al problema de estimación del máximo de una distribución uniforme. En este caso, consideraremos un problema más concreto. Supongamos que hay una lotería (tipo tradicional) en México y no sabemos cuántos números hay. Obtendremos una muestra iid de \\(n\\) números, ya haremos una aproximación continua, suponiendo que\n\\[X_i \\sim U[0,\\theta]\\]\nLa verosimilitud es entonces\n\\[P(X_1,\\ldots, X_n|\\theta) = \\theta^{-n},\\]\ncuando \\(\\theta\\) es mayor que todas las \\(X_i\\), y cero en otro caso. Necesitaremos una inicial \\(P(\\theta)\\).\nPor la forma que tiene la verosimilitud, podemos intentar una distribución Pareto, que tiene la forma\n\\[P(\\theta) = \\frac{\\alpha \\theta_0^\\alpha}{\\theta^{\\alpha + 1}}\\]\ncon soporte en \\([\\theta_0,\\infty]\\). Tenemos que escoger entonces el mínimo \\(\\theta_0\\) y el parámetro \\(\\alpha\\). En primer lugar, como sabemos que es una lotería nacional, creemos que no puede haber menos de unos 300 mil números, así que \\(\\theta_0 = 300\\). La función acumulada de la pareto es \\(1- (300/\\theta)^\\alpha\\), así que el cuantil 99% es\n\nalpha <- 1.1\n(300/(0.01)^(1/alpha))\n\n[1] 19738\n\n\nes decir, alrededor de 20 millones de números. Creemos que es un poco probable que el número de boletos sea mayor a esta cota. Nótese ahora que la posterior cumple (multiplicando verosimilitud por inicial):\n\\[P(\\theta|X_1,\\ldots, X_n |\\theta) \\propto \\theta^{-(n + 2.1)}\\]\npara \\(\\theta\\) mayor que el máximo de las \\(X_n\\)’s y 300, y cero en otro caso. Esta distribución es pareto con \\(\\theta_0' = \\max\\{300, X_1,\\ldots, X_n\\}\\) y \\(\\alpha = n + 1.1\\)\nUna vez planteado nuestro modelo, veamos los datos. Obtuvimos la siguiente muestra de números:\n\nloteria_tbl <- read_csv(\"datos/nums_loteria_avion.csv\", col_names = c(\"id\", \"numero\")) %>%\n  mutate(numero = as.integer(numero))\n\nRows: 99 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): numero\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(334)\nmuestra_loteria <- sample_n(loteria_tbl, 25) %>%\n  mutate(numero = numero/1000)\nmuestra_loteria %>% as.data.frame %>% head\n\n  id   numero\n1 87  348.341\n2  5 5851.982\n3 40 1891.786\n4 51 1815.455\n5 14 5732.907\n6 48 3158.414\n\n\nPodemos simular de una Pareto como sigue:\n\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n\nSimulamos de la inicial:\n\nsims_pareto_inicial <- tibble(\n  theta = rpareto(20000, 300, 1.1 ),\n  dist = \"inicial\")\n\nY con los datos de la muestra, simulamos de la posterior:\n\nsims_pareto_posterior <- tibble(\n  theta = rpareto(20000,\n                  max(c(300, muestra_loteria$numero)),\n                  nrow(muestra_loteria) + 1.1),\n  dist = \"posterior\")\nsims_theta <- bind_rows(sims_pareto_inicial, sims_pareto_posterior)\nggplot(sims_theta) +\n  geom_histogram(aes(x = theta, fill = dist),\n                 bins = 70, alpha = 0.5, position = \"identity\",\n                 boundary = max(muestra_loteria$numero))  +\n  xlim(0, 15000) + scale_y_sqrt() +\n  geom_rug(data = muestra_loteria, aes(x = numero))\n\nWarning: Removed 273 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 4 rows containing missing values (geom_bar).\n\n\n\n\n\nNótese que cortamos algunos valores de la inicial en la cola derecha: un defecto de esta distribución inicial, con una cola tan larga a la derecha, es que pone cierto peso en valores que son poco creíbles y la vuelve poco apropiada para este problema. Regresamos más adelante a este problema.\nSi obtenemos percentiles, obtenemos el intervalo\n\nf <- c(0.025, 0.5, 0.975)\nsims_theta %>% group_by(dist) %>%\n  summarise(cuantiles = quantile(theta, f) %>% round(2), f = f) %>%\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n`summarise()` has grouped output by 'dist'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 4\n# Groups:   dist [2]\n  dist      `0.025` `0.5` `0.975`\n  <chr>       <dbl> <dbl>   <dbl>\n1 inicial      307.  569.   8449.\n2 posterior   5858. 6010.   6732.\n\n\nEstimamos entre 5.8 millones y 6.7 millones de boletos. El máximo en la muestra es de\n\nmax(muestra_loteria$numero)\n\n[1] 5851.982\n\n\nEscoger la distribución pareto como inicial es conveniente y nos permitió resolver el problema sin dificultad, pero por su forma vemos que no necesariamente es apropiada para el problema por lo que señalamos arriba. Nos gustaría, por ejemplo, poner una inicial como la siguiente\n\nqplot(rgamma(2000, 5, 0.001), geom=\"histogram\", bins = 20) +\n  scale_x_continuous(breaks = seq(1000, 15000, by = 2000))\n\n\n\n\nSin embargo, los cálculos no son tan simples en este caso, pues la posterior no tiene un forma reconocible. Tendremos que usar otras estrategias de simulación para ejemplos como este (Monte Carlo por medio de Cadenas de Markov, que veremos más adelante)."
  },
  {
    "objectID": "modelos-bayesianos.html#probabilidad-a-priori",
    "href": "modelos-bayesianos.html#probabilidad-a-priori",
    "title": "10  Inferencia bayesiana",
    "section": "Probabilidad a priori",
    "text": "Probabilidad a priori\nLa inferencia bayesiana es conceptualmente simple: siempre hay que calcular la posterior a partir de verosimilitud (modelo de datos) y distribución inicial o a priori. Sin embargo, una crítica usual que se hace de la inferencia bayesiana es precisamente que hay que tener esa información inicial, y que distintos analistas llegan a distintos resultados si tienen información inicial distinta.\nEso realmente no es un defecto, es una ventaja de la inferencia bayesiana. Los datos y los problemas que queremos resolver no viven en un vacío donde podemos creer que la estatura de las personas, por ejemplo, puede variar de 0 a mil kilómetros, el número de boletos de una lotería puede ir de 2 o 3 boletos o también quizá 500 millones de boletos, o la proporción de personas infectadas de una enfermedad puede ser de unos cuantos hasta miles de millones.\n\nEn todos estos casos tenemos cierta información inicial que podemos usar para informar nuestras estimaciones. Esta información debe usarse.\nAntes de tener datos, las probabilidades iniciales deben ser examinadas en términos del conocimiento de expertos.\nLas probabilidades iniciales son supuestos que hacemos acerca del problema de interés, y también están sujetas a críticas y confrontación con datos."
  },
  {
    "objectID": "modelos-bayesianos.html#análisis-conjugado",
    "href": "modelos-bayesianos.html#análisis-conjugado",
    "title": "10  Inferencia bayesiana",
    "section": "Análisis conjugado",
    "text": "Análisis conjugado\nLos dos ejemplos que hemos visto arriba son ejemplos de análisis conjugado:\n\n(Beta-bernoulli) Si las observaciones \\(X_i\\) son \\(\\mathsf{Bernoulli}(p)\\) (\\(n\\) fija) queremos estimar \\(p\\), y tomamos como distribución inicial para \\(p\\) una \\(\\mathsf{Beta}(a,b)\\), entonces la posterior para \\(p\\) cuando \\(S_n=k\\) es \\(\\mathsf{Beta}(k + a, n - k + b)\\), donde \\(S_n = X_1 + X_2 +\\cdots +X_n\\).\n\nY más en general:\n\n(Beta-binomial) Si las observaciones \\(X_i, i=1,2,\\ldots, m\\) son \\(\\mathsf{Binomial}(n_i, p)\\) (\\(n_i\\)’s fijas) independientes, queremos estimar \\(p\\), y tomamos como distribución inicial para \\(p\\) una \\(\\mathsf{Beta}(a,b)\\), entonces la posterior para \\(p\\) cuando \\(S_m=k\\) es \\(\\mathsf{Beta}(k + a, n - k + b)\\), donde \\(S_m = X_1 + X_2 +\\cdots +X_m\\) y \\(n= n_1+n_2+\\cdots+n_m\\)\n\nTambién aplicamos:\n\n(Uniforme-Pareto) Si el modelo de datos \\(X_i\\) es uniforme \\(\\mathsf{U}[0,\\theta]\\) (\\(n\\) fija), queremos estimar \\(\\theta\\), y tomamos como distribución inicial para \\(\\theta\\) una Pareto \\((\\theta_0, \\alpha)\\), entonces la posterior para \\(p\\) si el máximo de las \\(X_i\\)’s es igual a \\(M\\) es Pareto con parámetros \\((\\max\\{\\theta_0, M\\}, \\alpha + n)\\).\n\nNótese que en estos casos, dada una forma de la verosimilitud, tenemos una familia conocida de iniciales tales que las posteriores están en la misma familia. Estos modelos son convenientes porque podemos hacer simulaciones de la posterior de manera fácil, o usar sus propiedades teóricas.\nOtro ejemplo típico es el modelo normal-normal:\n\n(Normal-normal) Si \\(X_i\\sim \\mathsf{N}(\\mu,\\sigma)\\), con \\(\\sigma\\) conocida, y tomamos como distribución inicial para \\(\\mu \\sim \\mathsf{N}(\\mu_0,\\sigma_0)\\), y definimos la precisión \\(\\tau\\) como el inverso de la varianza \\(\\sigma^2\\), entonces la posterior de \\(\\mu\\) es Normal con media \\((1-\\lambda) \\mu_0 + \\lambda\\overline{x}\\), y precisión \\(\\tau_0 + n\\tau\\), donde \\(\\lambda = \\frac{n\\tau}{\\tau_0 + n\\tau}\\)\n\nMás útil es el siguiente modelo:\n\n(Normal-Gamma inverso) Sean \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma)\\). Queremos estimar \\(\\mu\\) y \\(\\sigma\\). Tomamos como distribuciones iniciales (dadas por 4 parámetros: \\(\\mu_0, n_0, \\alpha,\\beta\\)):\n\n\\(\\tau = \\frac{1}{\\sigma^2} \\sim \\mathsf{Gamma}(\\alpha,\\beta)\\)\n\\(\\mu|\\sigma\\) es normal con media \\(\\mu_0\\) y varianza \\(\\sigma^2 / {n_0}\\) , y\n\\(p(\\mu, \\sigma) = p(\\mu|\\sigma)p(\\sigma)\\)\n\nEntonces la posterior es:\n\n\\(\\tau|x\\) es \\(\\mathsf{Gamma}(\\alpha', \\beta')\\), con \\(\\alpha' = \\alpha + n/2\\), \\(\\beta' = \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(x_{i} - \\bar{x})^2 + \\frac{nn_0}{n+n_0}\\frac{({\\bar{x}}-\\mu_{0})^2}{2}\\)\n\\(\\mu|\\sigma,x\\) es normal con media \\(\\mu' = \\frac{n_0\\mu_{0}+n{\\bar{x}}}{n_0 +n}\\) y varianza $ ^2/({n_0 +n})$.\n\\(p(\\mu,\\sigma|x) = p(\\mu|x,\\sigma)p(\\sigma|x)\\)\n\n\nObservaciones\n\nNótese que este último ejemplo tienen más de un parámetro. En estos casos, el objeto de interés es la posterior conjunta de los parámetros \\(p(\\theta_1,\\theta_2,\\cdots, \\theta_p|x)\\). Este último ejemplo es relativamente simple pues por la selección de iniciales, para simular de la conjunta de \\(\\mu\\) y \\(\\tau\\) podemos simular primero \\(\\tau\\) (o \\(\\sigma\\)), y después usar este valor para simular de \\(\\mu\\): el par de valores resultantes son una simulación de la conjunta.\nLos parámetros \\(a,b\\) para la inicial de \\(\\tau\\) pueden interpretarse como sigue: \\(\\sqrt{b/a}\\) es un valor “típico” a priori para la varianza poblacional, y \\(a\\) indica qué tan seguros estamos de este valor típico.\nNótese que para que funcionen las fórmulas de la manera más simple, escogimos una dependencia a priori entre la media y la precisión: \\(\\tau = \\sigma^{-2}\\) indica la escala de variabilidad que hay en la población, la incial de la media tiene varianza \\(\\sigma^2/n_0\\). Si la escala de variabilidad de la población es más grande, tenemos más incertidumbre acerca de la localización de la media.\nAunque esto tiene sentido en algunas aplicaciones, y por convenviencia usamos esta familia conjugada, muchas veces es preferible otro tipo de especificaciones para las iniciales: por ejemplo, la media normal y la desviación estándar uniforme, o media normal, con iniciales independientes. Sin embargo, estos casos no son tratables con análisis conjugado (veremos más adelante cómo tratarlos con MCMC).\n\n\nEjemplo\nSupongamos que queremos estimar la estatura de los cantantes de tesitura tenor con una muestra iid de tenores de Estados Unidos. Usaremos el modelo normal de forma que \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma^2)\\).\nUna vez decidido el modelo, tenemos que poner distribución inicial para los parámetros \\((\\mu, \\sigma^2)\\).\nComenzamos con \\(\\sigma^2\\). Como está el modelo, esta inicial debe estar dada para la precisión \\(\\tau\\), pero podemos simular para ver cómo se ve nuestra inicial para la desviación estándar. En la población general la desviación estándar es alrededor de 7 centímetros\n\n# Comenzamos seleccionando un valor que creemos típico para la desviación estándar\nsigma_0 <- 7\n# seleccionamos un valor para a, por ejemplo: si es más chico sigma tendrá más\n# disperisón\na <- 3\n# ponemos 8 = sqrt(b/a) -> b = a * 64\nb <- a * sigma_0^2\nc(a = a, b = b)\n\n  a   b \n  3 147 \n\n\nAhora simulamos para calcular cuantiles\n\ntau <- rgamma(1000, a, b)\nquantile(tau, c(0.05, 0.95))\n\n         5%         95% \n0.005781607 0.042170161 \n\nsigma <- 1 / sqrt(tau)\nmean(sigma)\n\n[1] 8.002706\n\nquantile(sigma, c(0.05, 0.95))\n\n       5%       95% \n 4.869653 13.151520 \n\n\nQue es dispersión considerable: con poca probabilidad la desviación estándar es menor a 4 centímetros, y también creemos que es poco creíble la desviación estándar sea de más de 13 centímetros.\nComenzamos con \\(\\mu\\). Sabemos, por ejemplo, que con alta probabilidad la media debe ser algún número entre 1.60 y 1.80. Podemos investigar: la media nacional en estados unidos está alrededor de 1.75, y el percentil 90% es 1.82. Esto es variabilidad en la población: debe ser muy poco probable, por ejemplo, que la media de tenores sea 1.82 Quizá los cantantes tienden a ser un poco más altos o bajos que la población general, así que podríamos agregar algo de dispersión.\nPodemos establecer parámetros y simular de la marginal a partir de las fórmulas de arriba para entender cómo se ve la inicial de \\(\\mu\\):\n\nmu_0 <- 175 # valor medio de inicial\nn_0 <- 5 # cuánta concentración en la inicial\ntau <- rgamma(1000, a,b)\nsigma <- 1/sqrt(tau)\nmu <- map_dbl(sigma, ~ rnorm(1, mu_0, .x / sqrt(n_0)))\nquantile(mu, c(0.05, 0.5, 0.95))\n\n      5%      50%      95% \n168.7275 174.8412 180.7905 \n\n\nQue consideramos un rango en el que con alta probabilidad debe estar la media poblacional de los cantantes.\nPodemos checar nuestros supuestos simulando posibles muestras usando sólo nuestra información previa:\n\nsimular_normal_invgamma <- function(n, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular media\n  tau <- rgamma(1, a, b)\n  sigma <- 1 / sqrt(tau)\n  mu <- rnorm(1, mu_0, sigma/sqrt(n_0))\n  # simular sigma\n  rnorm(n, mu, sigma)\n}\nset.seed(3461)\nsims_tbl <- tibble(rep = 1:20) %>%\n  mutate(estatura = map(rep, ~ simular_normal_invgamma(500, c(mu_0, n_0, a, b)))) %>%\n  unnest(cols = c(estatura))\nggplot(sims_tbl, aes(x = estatura)) + geom_histogram() +\n  facet_wrap(~ rep) +\n  geom_vline(xintercept = c(150, 180), colour = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPusimos líneas de referencia en 150 y 180. Vemos que nuestras iniciales no producen simulaciones totalmente fuera del contexto, y parecen cubrir apropiadamente el espacio de posiblidades para estaturas de los tenores. Quizá hay algunas realizaciones poco creíbles, pero no extremadamente. En este punto, podemos regresar y ajustar la inicial para \\(\\sigma\\), que parece tomar valores demasiado grandes (produciendo por ejemplo una simulación con estatura de 220 y 140, que deberían ser menos probables).\nAhora podemos usar los datos para calcular nuestras posteriores.\n\nset.seed(3413)\ncantantes <- lattice::singer %>%\n  mutate(estatura_cm = round(2.54 * height)) %>%\n  filter(str_detect(voice.part, \"Tenor\")) %>%\n  sample_n(20)\ncantantes\n\n    height voice.part estatura_cm\n139     70    Tenor 1         178\n150     68    Tenor 2         173\n140     65    Tenor 1         165\n132     66    Tenor 1         168\n152     69    Tenor 2         175\n141     72    Tenor 1         183\n161     71    Tenor 2         180\n156     71    Tenor 2         180\n158     71    Tenor 2         180\n164     69    Tenor 2         175\n147     68    Tenor 1         173\n130     72    Tenor 1         183\n162     71    Tenor 2         180\n134     74    Tenor 1         188\n170     69    Tenor 2         175\n167     68    Tenor 2         173\n149     64    Tenor 1         163\n143     68    Tenor 1         173\n157     69    Tenor 2         175\n153     71    Tenor 2         180\n\n\nLos cálculos son un poco tediosos, pero podemos construir una función apropiada:\n\ncalcular_pars_posterior <- function(x, pars_inicial){\n  # iniciales\n  mu_0 <- pars_inicial[1]\n  n_0 <- pars_inicial[2]\n  a_0 <- pars_inicial[3]\n  b_0 <- pars_inicial[4]\n  # muestra\n  n <- length(x)\n  media <- mean(x)\n  S2 <- sum((x - media)^2)\n  # sigma post\n  a_1 <- a_0 + 0.5 * n\n  b_1 <- b_0 + 0.5 * S2 + 0.5 * (n * n_0) / (n + n_0) * (media - mu_0)^2\n  # posterior mu\n  mu_1 <- (n_0 * mu_0 + n * media) / (n + n_0)\n  n_1 <- n + n_0\n  c(mu_1, n_1, a_1, b_1)\n}\npars_posterior <- calcular_pars_posterior(cantantes$estatura_cm, c(mu_0, n_0, a, b))\npars_posterior\n\n[1] 175.8  25.0  13.0 509.0\n\n\n¿Cómo se ve nuestra posterior comparada con la inicial? Podemos hacer simulaciones:\n\nsim_params <- function(m, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular sigmas\n  sims <- tibble(tau = rgamma(m, a, b)) %>%\n    mutate(sigma = 1 / sqrt(tau))\n  # simular mu\n  sims <- sims %>% mutate(mu = rnorm(m, mu_0, sigma / sqrt(n_0)))\n  sims\n}\nsims_inicial <- sim_params(5000, c(mu_0, n_0, a, b)) %>%\n  mutate(dist = \"inicial\")\nsims_posterior <- sim_params(5000, pars_posterior) %>%\n  mutate(dist = \"posterior\")\nsims <- bind_rows(sims_inicial, sims_posterior)\nggplot(sims, aes(x = mu, y = sigma, colour = dist)) +\n  geom_point()\n\n\n\n\nY vemos que nuestra posterior es consistente con la información inicial que usamos, hemos aprendido considerablemente de la muestra. La posterior se ve como sigue. Hemos marcado también las medias posteriores de cada parámetro: media y desviación estándar.\n\nmedias_post <- sims %>% filter(dist == \"posterior\") %>%\n  select(-dist) %>%\n  summarise(across(everything(), mean))\nggplot(sims %>% filter(dist == \"posterior\"),\n    aes(x = mu, y = sigma)) +\n  geom_point(colour = \"#00BFC4\") +\n  geom_point(data = medias_post, size = 5, colour = \"black\") +\n  coord_equal()\n\n\n\n\nPodemos construir intervalos creíbles del 90% para estos dos parámetros, por ejemplo haciendo intervalos de percentiles:\n\nf <- c(0.05, 0.5, 0.95)\nsims %>%\n  pivot_longer(cols = mu:sigma, names_to = \"parametro\") %>%\n  group_by(dist, parametro) %>%\n  summarise(cuantil = quantile(value, f) %>% round(1), f= f) %>%\n  pivot_wider(names_from = f, values_from = cuantil)\n\n`summarise()` has grouped output by 'dist', 'parametro'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   dist, parametro [4]\n  dist      parametro `0.05` `0.5` `0.95`\n  <chr>     <chr>      <dbl> <dbl>  <dbl>\n1 inicial   mu         169.  175.   181. \n2 inicial   sigma        4.8   7.4   13.3\n3 posterior mu         174.  176.   178. \n4 posterior sigma        5.1   6.3    8.2\n\n\nComo comparación, los estimadores de máxima verosimlitud son\n\nmedia_mv <- mean(cantantes$estatura_cm)\nsigma_mv <- mean((cantantes$estatura_cm - media_mv)^2) %>% sqrt\nc(media_mv, sigma_mv)\n\n[1] 176   6\n\n\nAhora solo resta checar que el modelo es razonable. Veremos más adelante cómo hacer esto, usando la distribución predictiva posterior."
  },
  {
    "objectID": "modelos-bayesianos.html#pasos-de-un-análisis-de-datos-bayesiano",
    "href": "modelos-bayesianos.html#pasos-de-un-análisis-de-datos-bayesiano",
    "title": "10  Inferencia bayesiana",
    "section": "Pasos de un análisis de datos bayesiano",
    "text": "Pasos de un análisis de datos bayesiano\n\n\n\n\n\n\nTip\n\n\n\nComo vimos en los ejemplos, en general un análisis de datos bayesiano sigue los siguientes pasos:\n\nIdentificar los datos releventes a nuestra pregunta de investigación, el tipo de datos que vamos a describir, que variables queremos estimar.\nDefinir el modelo descriptivo para los datos. La forma matemática y los parámetros deben ser apropiados para los objetivos del análisis.\nEspecificar la distribución inicial de los parámetros.\nUtilizar inferencia bayesiana para reubicar la credibilidad a lo largo de los posibles valores de los parámetros.\nVerificar que la distribución posterior replique los datos de manera razonable, de no ser el caso considerar otros modelos descriptivos para los datos.\n\n\n\n\nElicitando probablidades subjetivas (opcional)\nNo siempre es fácil elicitar probabilidades subjetivas de manera que capturemos el verdadero conocimiento de dominio que tenemos. Una manera clásica de hacerlo es con apuestas\nConsidera una pregunta sencilla que puede afectar a un viajero: ¿Qué tanto crees que habrá una tormenta que ocasionará el cierre de la autopista México-Acapulco en el puente del \\(20\\) de noviembre? Como respuesta debes dar un número entre \\(0\\) y \\(1\\) que refleje tus creencias. Una manera de seleccionar dicho número es calibrar las creencias en relación a otros eventos cuyas probabilidades son claras.\nComo evento de comparación considera una experimento donde hay canicas en una urna: \\(5\\) rojas y \\(5\\) blancas. Seleccionamos una canica al azar. Usaremos esta urna como comparación para considerar la tormenta en la autopista. Ahora, considera el siguiente par de apuestas de las cuales puedes elegir una:\n\nA. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre.\nB. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(5\\) canicas rojas y \\(5\\) blancas.\n\nSi prefieres la apuesta B, quiere decir que consideras que la probabilidad de tormenta es menor a \\(0.5\\), por lo que al menos sabes que tu creencia subjetiva de una la probabilidad de tormenta es menor a \\(0.5\\). Podemos continuar con el proceso para tener una mejor estimación de la creencia subjetiva.\n\nA. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre.\nC. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(1\\) canica roja y \\(9\\) blancas.\n\nSi ahora seleccionas la apuesta \\(A\\), esto querría decir que consideras que la probabilidad de que ocurra una tormenta es mayor a \\(0.10\\). Si consideramos ambas comparaciones tenemos que tu probabilidad subjetiva se ubica entre \\(0.1\\) y \\(0.5\\)."
  },
  {
    "objectID": "modelos-bayesianos.html#verificación-predictiva-posterior",
    "href": "modelos-bayesianos.html#verificación-predictiva-posterior",
    "title": "10  Inferencia bayesiana",
    "section": "Verificación predictiva posterior",
    "text": "Verificación predictiva posterior\nUna vez que ajustamos un modelo bayesiano, podemos simular nuevas observaciones a partir del modelo. Esto tiene dos utilidades:\n\nHacer predicciones acerca de datos no observados.\nConfirmar que nuevas producidas simuladas con el modelo son similares a las que de hecho observamos. Esto nos permite confirmar la calidad del ajuste del modelo, y se llama verificación predictiva posterior.\n\nSupongamos que tenemos la posterior \\(p(\\theta | x)\\). Podemos generar una nueva replicación de los datos como sigue:\nLa distribución predictiva posterior genera nuevas observaciones a partir de la información observada. La denotamos como \\(p(\\tilde{x}|x)\\).\nPara simular de ella:\n\nMuestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\).\nSimulamos del modelo de las observaciones \\(\\tilde{x} \\sim p(\\tilde{x}|\\tilde{\\theta})\\).\nRepetimos el proceso hasta obtener una muestra grande.\nUsamos este método para producir, por ejemplo, intervalos de predicción para nuevos datos.\n\nSi queremos una replicación de las observaciones de la predictiva posterior,\n\nMuestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\).\nSimulamos del modelo de las observaciones \\(\\tilde{x}_1, \\tilde{x}_2,\\ldots, \\tilde{x}_n \\sim p(\\tilde{x}|\\tilde{\\theta})\\), done \\(n\\) es el tamaño de muestra de la muestra original \\(x\\).\nUsamos este método para producir conjuntos de datos simulados que comparamos con los observados para verificar nuestro modelo.\n\n\nEjemplo: estaturas de tenores\nEn este ejemplo, usaremos la posterior predictiva para checar nuestro modelo. Vamos a crear varias muestras, del mismo tamaño que la original, según nuestra predictiva posterior, y compararemos estas muestras con la observada.\nY ahora simulamos otra muestra\n\nmuestra_sim <- simular_normal_invgamma(20, pars_posterior)\nmuestra_sim %>% round(0)\n\n [1] 167 181 184 181 167 167 172 170 177 172 169 174 182 184 176 171 175 176 168\n[20] 181\n\n\nPodemos simular varias muestras y hacer una prueba de lineup:\n\nlibrary(nullabor)\nsims_obs <- tibble(.n = 1:19) %>%\n  mutate(estatura_cm = map(.n, ~ simular_normal_invgamma(20, pars_posterior))) %>%\n  unnest(estatura_cm)\nset.seed(9921)\npos <- sample(1:20, 1)\nlineup_tbl <- lineup(true = cantantes %>% select(estatura_cm),\n                     samples = sims_obs, pos = pos)\nggplot(lineup_tbl, aes(x = estatura_cm)) + geom_histogram(binwidth = 2.5) +\n  facet_wrap(~.sample)\n\n\n\n\nCon este tipo de gráficas podemos checar desajustes potenciales de nuestro modelo.\n\n¿Puedes encontrar los datos verdaderos? ¿Cuántos seleccionaron los datos correctos?\nPrueba hacer pruebas con una gráfica de cuantiles. ¿Qué problema ves y cómo lo resolverías?\n\n\n\nEjemplo: modelo Poisson\nSupongamos que pensamos el modelo para las observaciones es Poisson con parámetro \\(\\lambda\\). Pondremos como inicial para \\(\\lambda\\) una exponencial con media 10.\nNótese que la posterior está dada por\n\\[p(\\lambda|x_1,\\ldots, x_n) \\propto e^{-n\\lambda}\\lambda^{\\sum_i x_i} e^{-0.1\\lambda} = \\lambda^{n\\overline{x}}e^{-\\lambda(n + 0.1)}\\]\nque es una distribución gamma con parámetros \\((n\\overline{x} + 1, n+0.1)\\)\nAhora supongamos que observamos la siguiente muestra, ajustamos nuestro modelo y hacemos replicaciones posteriores de los datos observados:\n\nx <- rnbinom(250, mu = 20, size = 3)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) %>%\n  bind_rows() %>%\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\nY vemos claramente que nuestro modelo no explica apropiadamente la variación de los datos observados. Contrasta con:\n\nset.seed(223)\nx <- rpois(250, 15)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) %>%\n  bind_rows() %>%\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\nY verificamos que en este caso el ajuste del modelo es apropiado."
  },
  {
    "objectID": "modelos-bayesianos.html#predicción",
    "href": "modelos-bayesianos.html#predicción",
    "title": "10  Inferencia bayesiana",
    "section": "Predicción",
    "text": "Predicción\nCuando queremos hacer predicciones particulares acerca de datos que observemos en el futuro, también podemos usar la posterior predictiva. En este caso, tenemos que considerar\n\nLa variabilidad que produce la incertidumbre en la estimación de los parámetros\nLa variabilidad de las observaciones dados los parámetros.\n\nEs decir, tenemos que simular sobre todos las combinaciones factibles de los parámetros.\n\nEjemplo: cantantes\nSi un nuevo tenor llega a un coro, ¿cómo hacemos una predicción de su estatura? Como siempre, quisiéramos obtener un intervalo que exprese nuestra incertidumbre acerca del valor que vamos a observar. Entonces haríamos:\n\nsims_posterior <- sim_params(50000, pars_posterior) %>%\n  mutate(y_pred = rnorm(n(), mu, sigma))\nsims_posterior %>% head\n\n# A tibble: 6 × 4\n     tau sigma    mu y_pred\n   <dbl> <dbl> <dbl>  <dbl>\n1 0.0286  5.91  175.   181.\n2 0.0200  7.07  177.   178.\n3 0.0257  6.23  176.   170.\n4 0.0344  5.39  176.   174.\n5 0.0297  5.80  175.   169.\n6 0.0282  5.96  177.   170.\n\n\n\nf <- c(0.025, 0.5, 0.975)\nsims_posterior %>% summarise(f = f, y_pred = quantile(y_pred, f))\n\n# A tibble: 3 × 2\n      f y_pred\n  <dbl>  <dbl>\n1 0.025   163.\n2 0.5     176.\n3 0.975   189.\n\n\nY con esto obtenemos el intervalo (163, 189), al 95%, para una nueva observación. Nótese que este intervalo no puede construirse con una simulación particular de la posterior de los parámetros, pues sería demasiado corto.\nEs posible demostrar que en este caso, la posterior predictiva tiene una forma conocida:\n\nLa posterior predictiva para el modelo normal-gamma inverso es una distribución \\(t\\) con \\(2\\alpha'\\) grados de libertad, centrada en \\(\\mu'\\), y con escala \\(s^2 = \\frac{\\beta'}{\\alpha'}\\frac{n + n_0 + 1}{n +n_0}\\)\n\n\nmu_post <- pars_posterior[1]\nn_post <- pars_posterior[2]\nalpha_post <- pars_posterior[3]\nbeta_post <- pars_posterior[4]\ns <- sqrt(beta_post/alpha_post) * sqrt((n_post + 1)/n_post)\nqt(c(0.025, 0.5, 0.975), 2 * alpha_post) * s + mu_post\n\n[1] 162.6832 175.8000 188.9168\n\n\n\n\nEjemplo: posterior predictiva de Pareto-Uniforme.\nLa posterior predictiva del modelo Pareto-Uniforme no tiene un nombre estándar, pero podemos aproximarla usando simulación. Usando los mismos datos del ejercicio de la lotería, haríamos:\n\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n# Simulamos de la posterior de los parámetros\nlim_inf_post <- max(c(300, muestra_loteria$numero))\nk_posterior <- nrow(muestra_loteria) + 1.1\nsims_pareto_posterior <- tibble(\n  theta = rpareto(100000, lim_inf_post, k_posterior))\n# Simulamos una observación para cada una de las anteriores:\nsims_post_pred <- sims_pareto_posterior %>%\n  mutate(x_pred = map_dbl(theta, ~ runif(1, 0, .x)))\n# Graficamos\nggplot(sims_post_pred, aes(x = x_pred)) +\n  geom_histogram(binwidth = 50) +\n  geom_vline(xintercept = lim_inf_post, colour = \"red\")\n\n\n\n\nQue es una mezcla de una uniforme con una Pareto.\n\n\n\n\nChihara, Laura M., y Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and R. 2.ª ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nKruschke, John. 2015. Doing Bayesian Data Analysis (Second Edition). Academic Press."
  }
]