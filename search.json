[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pensamiento estadístico",
    "section": "",
    "text": "En casi todas las soluciones basadas en datos, los científicos de datos ejercen el pensamiento estadístico al diseñar estrategias de recopilación de datos, obtener evidencia para la toma de decisiones y construir modelos para predecir tendencias futuras.\nEste curso busca explicar con los principios básicos de la estadística, y su papel en el análisis de datos. Nuestro punto de vista es uno de fundamentos, con menos énfasis en recetas o técnicas particulares.\nEn particular, estudiaremos básicos de probabilidad, métodos basados en remuestreo, y en la parte final la filosofía de la inferencia bayesiana y las técnicas de modelado bayesiano utilizando estudios de casos ilustrativos.\n\n\n\n\n\nLicencia Creative Commons\n\n\nEste trabajo está bajo una licencia: Attribution-NonCommercial 4.0 International\nEres libre de adaptarlo para propósitos no comerciales otorgando el crédito correspondiente."
  },
  {
    "objectID": "est-cd-1.html",
    "href": "est-cd-1.html",
    "title": "Estadística y ciencia de datos",
    "section": "",
    "text": "El término “ciencia de datos” surgió recientemente, y hay discusión acerca de qué tan apropiado es el término, si es correcto llamarla “ciencia”, y en general, en cómo definirla exactamente. En este curso tomamos el punto de vista de que:\nDesde este punto de vista, el estándar de validez más importante en la ciencia de datos (Tukey (1962)) es su funcionamiento en la práctica, y no la adherencia a argumentos teóricos, matemáticos o estadísticos.\nIgualmente puede ser difícil definir qué es la estadística (algunos la ven como una parte o rama de las matemáticas, en un extremo, y otros la consideran algo más cercano al análisis de datos). En cualquier caso:"
  },
  {
    "objectID": "est-cd-1.html#preguntas-y-datos",
    "href": "est-cd-1.html#preguntas-y-datos",
    "title": "Estadística y ciencia de datos",
    "section": "Preguntas y datos",
    "text": "Preguntas y datos\nCuando observamos un conjunto de datos, independientemente de su tamaño, el paso inicial más importante es entender bajo qué proceso se generan los datos.\n\nA grandes rasgos, cuanto más sepamos de este proceso, mejor podemos contestar preguntas de interés.\nEn muchos casos, tendremos que hacer algunos supuestos de cómo se generan estos datos para dar respuestas (condicionales a esos supuestos).\n\n\nEjemplo: nacimientos\nComenzamos con un ejemplo de análisis exploratorio. Consideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016. Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño, pueden representarse de manera razonablemente efectiva en una visualización de serie de tiempo\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\nnacimientos <- read_rds(\"datos/nacimientos/natalidad.rds\") |>\n   ungroup() |> \n   filter(year(fecha) == 2016, month(fecha) <= 3)\n\n\nExaminamos partes del contenido de la tabla:\n\n\nCódigo\ntab_1 <- nacimientos |> \n   select(fecha, n) |> \n   slice_head(n = 5)\ntab_2 <- nacimientos |> \n   select(fecha, n) |> \n   slice_tail(n = 5)\nkable(list(tab_1, tab_2)) |> kable_styling()\n\n\n\n\n\n  \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-01-01 \n    3952 \n  \n  \n    2016-01-02 \n    4858 \n  \n  \n    2016-01-03 \n    4665 \n  \n  \n    2016-01-04 \n    5948 \n  \n  \n    2016-01-05 \n    6087 \n  \n\n\n\n \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-03-27 \n    4112 \n  \n  \n    2016-03-28 \n    5805 \n  \n  \n    2016-03-29 \n    5957 \n  \n  \n    2016-03-30 \n    5766 \n  \n  \n    2016-03-31 \n    5497 \n  \n\n\n\n \n  \n\n\n\n\n\nEn un examen rápido de estos números no vemos nada fuera de orden. Los datos tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar de manera simple como sigue:\n\n\nCódigo\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\n\nEsta es una descripción de los datos, que quizá no es muy compacta pero muestra varios aspectos importantes. En este caso notamos algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:\n\n\nCódigo\ndomingos_tbl <- nacimientos |> \n   filter(weekdays(fecha) == \"Sunday\")\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = \"salmon\") +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\n\nObservamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente menos nacimentos. ¿Por qué crees que sea esto?\nAdicionalmente a estos patrones observamos otros aspectos interesantes:\n\nEl primero de enero hay considerablemente menos nacimientos de los que esperaríamos para un viernes. ¿Por qué?\nEl primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial este primero de marzo?\n¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?\n¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?\n\nLa confirmación de estas hipótesis, dependiendo de su forma, puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados con otros días de la semana) hasta muy compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes, procesos en el registro civil, etc.) En todo caso, una descripción correcta de estos datos requiere conocer tanto hechos generales como conocimiento detallado de prácticas relacionadas con la natalidad y el registro de nacimientos.\n\nEl análisis exploratorio y descripción de los datos requiere también conocimiento de dominio: ¿qué cosas intervienen en el proceso que genera estos datos?\n\n\n\nEjemplo (cálculos renales)\nEste es un estudio real acerca de tratamientos para cálculos renales (Julious y Mullee (1994)). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.\nLa tabla original tiene 700 renglones (cada renglón es un paciente)\n\n\nCódigo\ncalculos <- read_csv(\"./datos/kidney_stone_data.csv\")\nnames(calculos) <- c(\"tratamiento\", \"tamaño\", \"éxito\")\ncalculos <- calculos |> \n   mutate(tamaño = ifelse(tamaño == \"large\", \"grandes\", \"chicos\")) |> \n   mutate(resultado = ifelse(éxito == 1, \"mejora\", \"sin_mejora\")) |> \n   select(tratamiento, tamaño, resultado)\nnrow(calculos)\n\n\n[1] 700\n\n\ny se ve como sigue (muestreamos algunos renglones):\n\n\nCódigo\ncalculos |> \n   sample_n(20) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n  \n \n\n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    grandes \n    sin_mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    chicos \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n\n\n\n\n\nAunque estos datos contienen información de 700 pacientes, los datos pueden resumirse sin pérdida de información contando como sigue:\n\n\nCódigo\ncalculos_agregada <- calculos |> \n   group_by(tratamiento, tamaño, resultado) |> \n   count()\ncalculos_agregada |> kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n    n \n  \n \n\n  \n    A \n    chicos \n    mejora \n    81 \n  \n  \n    A \n    chicos \n    sin_mejora \n    6 \n  \n  \n    A \n    grandes \n    mejora \n    192 \n  \n  \n    A \n    grandes \n    sin_mejora \n    71 \n  \n  \n    B \n    chicos \n    mejora \n    234 \n  \n  \n    B \n    chicos \n    sin_mejora \n    36 \n  \n  \n    B \n    grandes \n    mejora \n    55 \n  \n  \n    B \n    grandes \n    sin_mejora \n    25 \n  \n\n\n\n\n\nEste resumen no es muy informativo, pero al menos vemos qué valores aparecen en cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:\n\n\nCódigo\ncalculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, tamaño, total, prop_mejora) |> \n   arrange(tamaño) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    chicos \n    87 \n    0.93 \n  \n  \n    B \n    chicos \n    270 \n    0.87 \n  \n  \n    A \n    grandes \n    263 \n    0.73 \n  \n  \n    B \n    grandes \n    80 \n    0.69 \n  \n\n\n\n\n\nEsta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Pero es apropiada para empezar a contestar la pregunta:\n\n¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?\n\nSupongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:\n\n\nCódigo\ncalculos |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\ny parece ser que el tratamiento \\(B\\) es mejor que el \\(A\\). Esta es una paradoja (un ejemplo de la paradoja de Simpson) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar \\(B\\)? ¿Si sabe debería recetar \\(A\\)? Esta discusión parece no tener mucho sentido.\nPodemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:\n\n\nCódigo\ncalculos |> group_by(tratamiento, tamaño) |> count() |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    n \n  \n \n\n  \n    A \n    chicos \n    87 \n  \n  \n    A \n    grandes \n    263 \n  \n  \n    B \n    chicos \n    270 \n  \n  \n    B \n    grandes \n    80 \n  \n\n\n\n\n\nNuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento \\(A\\) para cálculos grandes, y \\(B\\) para cálculos chicos. Esto quiere decir que en la tabla total el tratamiento \\(A\\) está en desventaja porque se usa en casos más difíciles, pero el tratamiento \\(A\\) parece ser en general mejor.\n\nUna mejor respuesta a la pregunta de qué tratamiento es mejor es la que presenta los datos desagregados\nLa tabla desagregada de asignación del tratamiento nos informa acerca de cómo se está distribuyendo el tratamiento en los pacientes.\n\nIgual que en el ejemplo anterior, los resúmenes descriptivos están acompañados de hipótesis acerca del proceso generador de datos, y esto ilumina lo que estamos observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos.\n\n\nEjemplo (cálculos renales 2)\nContrastemos el ejemplo anterior usando exactamente los mismos datos, pero con una interpretación diferente. En este caso, los tratamientos son para mejorar alguna enfermedad del corazón. Sabemos que parte del efecto de este tratamiento ocurre gracias a una baja en presión arterial de los pacientes, así que después de administrar el tratamiento, se toma la presión arterial de los pacientes. Ahora tenemos la tabla agregada y desagregada como sigue:\n\n\nCódigo\ncorazon <- calculos |> \n  select(tratamiento, presión = tamaño, resultado) |> \n  mutate(presión = ifelse(presión == \"grandes\", \"alta\", \"baja\"))\ncorazon_agregada <- corazon |> \n   group_by(tratamiento, presión, resultado) |> \n   count()\ncorazon_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, presión, total, prop_mejora) |> \n   arrange(presión) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    presión \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    alta \n    263 \n    0.73 \n  \n  \n    B \n    alta \n    80 \n    0.69 \n  \n  \n    A \n    baja \n    87 \n    0.93 \n  \n  \n    B \n    baja \n    270 \n    0.87 \n  \n\n\n\n\n\n\n\nCódigo\ncorazon |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\n¿Cuál creemos que es el mejor tratamiento en este caso? ¿Deberíamos usar la tabla agregada o la desagregada por presión?\n\nEn este caso, la tabla agregada es más apropiada (B es mejor tratamiento).\nLa razón es que presión en este caso es una consecuencia de tomar el tratamiento, y como las tablas muestran, B es más exitoso en bajar la presión de los pacientes.\nSi sólo comparamos dentro de los grupos de presión baja o de presión alta, ignoramos parte del efecto del tratamiento en la probabilidad de mejorar."
  },
  {
    "objectID": "est-cd-1.html#diagramas-causales",
    "href": "est-cd-1.html#diagramas-causales",
    "title": "Estadística y ciencia de datos",
    "section": "Diagramas causales",
    "text": "Diagramas causales\nPodemos utilizar diagramas causales introducidos por Judea Pearl (Pearl, Glymour, y Jewell (2016)) para explicar por qué el análisis se hace de manera diferente en cada uno de los casos de arriba. Los diagramas causales son representaciones de nuestro conocimiento de dominio acerca de cómo se relacionan de manera causal las variables de interés. En el caso de cálculos renales, podemos escribir el diagrama como sigue:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Tratamiento\" [exposure,pos=\"-3,0\"]\n  \"Resultado\" [outcome,pos=\"3,0\"]\n  \"Tamaño\" [pos=\"0,1\"]\n  \"Tamaño\"  -> \"Tratamiento\"\n    \"Tamaño\" -> \"Resultado\"\n    \"Tratamiento\" -> \"Resultado\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag() \n\n\n\n\n\n\nEl tamaño de los cálculos afecta al resultado y a la asignación del tratamiento. Es un confusor si queremos entender el efecto del tratamiento en el resultado.\nPartimos los datos según este confusor para “comparar peras con peras”.\n\nSin embargo, en el segundo ejemplo tenemos:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Tratamiento\" [exposure,pos=\"-3,0\"]\n  \"Resultado\" [outcome,pos=\"3,0\"]\n  \"Presión\" [pos=\"0,1\"]\n  \"Tratamiento\"  -> \"Presión\"\n    \"Presión\" -> \"Resultado\"\n    \"Tratamiento\" -> \"Resultado\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\n\nEn este caso, la presión es consecuencia también del tratamiento, así que es un camino por medio del cual el tratamiento produce resultados.\nComparar el tratamiento dentro de grupos de presión alta o baja estima el efecto del tratamiento que no tiene qué ver con la regulación de la presión, lo cual da una respuesta incompleta.\n\nAdicionalmente, en ambos ejemplos, estamos suponiendo que no existen otras variables confusoras que puedan afectar nuestro análisis. Qué tan correcta es esa suposición depende de que conozcamos los detalles de cómo fueron recopilados estos datos.\n\nEjemplo: prevalencia de anemia\nEn un estudio de hospitales Australia se registró que 57% de una muestra pacientes tenían anemia cuando fueron ingresados. ¿Qué podemos decir acerca de la prevalencia de anemia en la población general de Australia? Con información básica acerca del proceso generador de esta muestra podemos concluir que será difícil generalizar con estos datos a la población general. La razón es que:\n\nMuchas enfermedades graves (por ejemplo del corazón) pueden producir anemia.\nEstas enfermededes hacen más probable que alguien sea hospitalizado.\nPor lo tanto, en este estudio hay una asociación entre tener anemia y ser seleccionado para el estudio (tener anemia sube la probabilidad de ser seleccionado para el estudio)\nNuestra conclusión es que el 57% es probablemente una sobreestimación de la prevalencia de anemia en la población.\n\n\n\nCódigo\ndag_1 <- dagitty('dag{\"Selección\" [exposure, pos = \"2,1\"]\n  \"Anemia\" [outcome, pos = \"-1, 2.5\"]\n  \"Hospitalización\" [pos=\"1,2\"]\n  \"Enfermedad\" [pos=\"0, 3\"]\n  \"Hospitalización\" -> \"Selección\"\n    \"Enfermedad\" -> \"Anemia\"\n    \"Anemia\" -> \"Hospitalización\"\n    \"Enfermedad\" -> \"Hospitalización\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\nEste diagrama indica que puede ser difícil generalizar con las personas que han sido seleccionadas, porque tanto la selección como la variable de interés tienen una causa común: la existencia o no de una enfermedad en la persona. Será difícil generalizar para las personas no observadas en el estudio.\n\n\nEjemplo: colisionadores 1\nAlgunos estudios fueron publicados en la primera mitad de 2020 que notaban que el porcentaje fumadores entre los casos positivos de COVID era menor que en la población general, y se hicieron algunas interpretaciones acerca de este hecho. Estos estudios se hicieron con personas que se hicieron una prueba.\nEn este ejemplo replicaremos cómo es que podemos encontrar esta asociación en este tipo de estudios aún cuando no exista tal asociación en la población general. Usaremos datos sintéticos (simulados).\nPrimero vamos a razonar acerca del proceso generador de datos y a hacer algunos supuestos:\n\nEn primer lugar, ¿cuándo decide hacerse alguien una prueba? A principios de 2020, son principalmente personas que tienen síntomas considerables, y trabajadores de salud (tengan o no síntomas).\nSer trabajador de salud incrementa el riesgo de contagiarse.\nEn algunos países, fumar está asociado con ser trabajador de salud (no tienen la misma tasa de tabaquismo que la población general).\nSólo observamos a las personas que se hicieron una prueba.\n\nPodemos resumir cualitativamente con el siguiente diagrama:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Covid\" [outcome, pos = \"-0.5, 2.5\"]\n  \"Prueba\" [pos=\"0,-2\"]\n  \"TrabSalud\" [pos=\"0, 3\"]\n  \"Sintomas\" [pos=\"-1, 1\"]\n  \"Fumar\" [pos = \"1, 1\"]\n  \"TrabSalud\" -> \"Covid\"\n  \"TrabSalud\" -> \"Prueba\"\n  \"TrabSalud\" -> \"Fumar\"\n  \"Covid\" -> \"Sintomas\"\n  \"Sintomas\" -> \"Prueba\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\nEl código para simular es el siguiente: todas las variables toman valores 0 o 1, pero con diferentes probabilidades y dependiendo de las variables que son padres en la gráfica de arriba:\n\n\nCódigo\nset.seed(821)\n#simular población\nn <- 1000000\ntrab_salud <- rbinom(n, 1, 0.01)\ncovid <- rbinom(n, 1, ifelse(trab_salud==1, 0.04, 0.01))\ndatos <- tibble(trab_salud = trab_salud, covid) |> \n  mutate(sintomas = rbernoulli(n, ifelse(covid == 1, 0.5, 0.01))) |> \n  mutate(prueba = rbernoulli(n, ifelse(trab_salud ==1, 0.99, 0.6 * sintomas + 0.01))) |> \n  mutate(fumar = rbernoulli(n, ifelse(trab_salud == 1, 0.3, 0.1))) |> \n  mutate(covid = ifelse(covid ==1, \"positivo\", \"negativo\")) |> \n  mutate(fumar = ifelse(fumar, \"fuma\", \"no_fuma\"))\n\n\nSuponemos ahora que tomamos como muestra a todas aquellas personas que se hicieron una prueba. En primer lugar, la proporción de fumadores en la muestra es un poco más alta que la población, porque los trabajadores de salud están sobrerrepresentados\n\n\nCódigo\ndatos_pruebas <- filter(datos, prueba == 1)\ntable(datos_pruebas$fumar) |> prop.table()\n\n\n\n     fuma   no_fuma \n0.1712317 0.8287683 \n\n\nY ahora vemos que están asociados fumar y salir positivo:\n\n\nCódigo\ntable(datos_pruebas$covid, datos_pruebas$fumar) |> prop.table(margin = 2) |> \n  round(2)\n\n\n          \n           fuma no_fuma\n  negativo 0.91    0.87\n  positivo 0.09    0.13\n\n\nEn la población no existe tal asociación, además de que la tasa de positivos es considerablemente más baja:\n\n\nCódigo\ntable(datos$covid, datos$fumar) |> prop.table(margin = 2) |> \n  round(3)\n\n\n          \n            fuma no_fuma\n  negativo 0.989   0.990\n  positivo 0.011   0.010\n\n\n\nEn este ejemplo, al seleccionar sólo aquellas personas que tomaron una prueba, cambia la relación entre tener covid y ser trabajador de salud, pues sólo los que tienen síntomas de la población general toman la prueba. Esto produce que una prueba negativa esté más relacionada con ser trabajador de salud, y por lo tanto, mayor probabilidad de ser fumador.\nTambién puede entenderse pensando que cuando tomamos solamente las personas que se hicieron pruebas, entonces los trabajadores de salud están sobrerrepresantados en la muestra.\n\n\n\nEjemplo: colisionador 2\n\nSupongamos que queremos entender la relación en desempeño en matemáticas y en español para estudiantes que entran a una universidad.\nEncontramos una relación negativa entre las calificaciones de los dos exámenes: parece ser que habilidad verbal se contrapone a habilidad numérica.\n\n¿Por qué tenemos que tener cuidado al interpretar esta correlación? ¿Existe esta correlación en la población general?\n\nDescubrimos que la universidad hace una calificación compuesta de español y matemáticas para que los alumnos sean aceptados en la universidad\nEsto quiere decir que para entrar es necesario al menos desempeñarse bien en alguna de las dos\n\nAhora observamos que aunque en la población general no hay tal relación, al seleccionar sólo a los alumnos de la universidad “activamos” una correlación debido al proceso de selección:\n\n\nCódigo\nset.seed(823)\ntibble(x = rnorm(2000), y = rnorm(2000)) |> \n  mutate(aceptados = x + y > 1.5 ) |> \nggplot(aes(x, y, colour = aceptados)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "est-cd-1.html#procesos-generadores-de-datos",
    "href": "est-cd-1.html#procesos-generadores-de-datos",
    "title": "Estadística y ciencia de datos",
    "section": "Procesos generadores de datos",
    "text": "Procesos generadores de datos\nNótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales y de dominio para interpretar y hacer hipótesis acerca de lo que vemos en la gráfica. Una visión descontextualizada no tiene mucha utilidad. Las explicaciones son típicamente complejas e intervienen distintos aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados.\n\n\n\n\n\n\nEl proceso generador de datos\n\n\n\nAl conjunto de esos aspectos que determinan los datos que finalmente observamos le llamamos el proceso generador de datos. Para datos que observamos “naturalmente” este proceso generalmente es complicado.\n\n\nEn la Ciencia de Datos buscamos entender las partes importantes del proceso generador\n\nLa descripción correcta de los datos se logra con ese entendimiento de dominio y del proceso generador.\nLa formulación y refinamiento de preguntas importantes y sus respuestas acerca de estos datos requiere entendimiento de dominio y del proceso generador.\nMás tarde, veremos que la inferencia estadística también depende de este entendimiento, junto con propuestas del diseño estadístico que nos permite obtener los datos necesarios para simplificar y dar certeza en el proceso de contestar las preguntas de interés.\n\nMucha parte de este trabajo no es estadístico, sino que es un esfuerzo por entender el dominio (como sugiere el título de artículo de David A. Friedman: Statistical Models and Shoe Leather)."
  },
  {
    "objectID": "est-cd-1.html#ejercicio-admisiones-de-berkeley",
    "href": "est-cd-1.html#ejercicio-admisiones-de-berkeley",
    "title": "Estadística y ciencia de datos",
    "section": "Ejercicio: admisiones de Berkeley",
    "text": "Ejercicio: admisiones de Berkeley\nConsideramos ahora los siguientes datos de admisión a distintos departamentos de Berkeley en 1975:\n\n\nCódigo\ndata(\"UCBAdmissions\")\nadm_original <- UCBAdmissions |> as_tibble() |> \n   pivot_wider(names_from = Admit, values_from = n) \nadm_original |> knitr::kable()\n\n\n\n\n \n  \n    Gender \n    Dept \n    Admitted \n    Rejected \n  \n \n\n  \n    Male \n    A \n    512 \n    313 \n  \n  \n    Female \n    A \n    89 \n    19 \n  \n  \n    Male \n    B \n    353 \n    207 \n  \n  \n    Female \n    B \n    17 \n    8 \n  \n  \n    Male \n    C \n    120 \n    205 \n  \n  \n    Female \n    C \n    202 \n    391 \n  \n  \n    Male \n    D \n    138 \n    279 \n  \n  \n    Female \n    D \n    131 \n    244 \n  \n  \n    Male \n    E \n    53 \n    138 \n  \n  \n    Female \n    E \n    94 \n    299 \n  \n  \n    Male \n    F \n    22 \n    351 \n  \n  \n    Female \n    F \n    24 \n    317 \n  \n\n\n\n\n\nCon algo de manipulación podemos ver tasas de admisión para Male y Female, y los totales de cada grupo que solicitaron en cada Departamento.\n\n\nCódigo\nadm_tbl <- adm_original |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected), 2), total = Admitted + Rejected) |> \n   select(Gender, Dept, prop_adm, total) |> \n   pivot_wider(names_from = Gender, values_from = prop_adm:total)\nadm_tbl |> knitr::kable()\n\n\n\n\n \n  \n    Dept \n    prop_adm_Male \n    prop_adm_Female \n    total_Male \n    total_Female \n  \n \n\n  \n    A \n    0.62 \n    0.82 \n    825 \n    108 \n  \n  \n    B \n    0.63 \n    0.68 \n    560 \n    25 \n  \n  \n    C \n    0.37 \n    0.34 \n    325 \n    593 \n  \n  \n    D \n    0.33 \n    0.35 \n    417 \n    375 \n  \n  \n    E \n    0.28 \n    0.24 \n    191 \n    393 \n  \n  \n    F \n    0.06 \n    0.07 \n    373 \n    341 \n  \n\n\n\n\n\nY complementamos con las tasas de aceptación a total por género, y tasas de aceptación por departamento:\n\n\nCódigo\nadm_original |> group_by(Gender) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n\n \n  \n    Gender \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    Female \n    557 \n    1278 \n    0.30 \n  \n  \n    Male \n    1198 \n    1493 \n    0.45 \n  \n\n\n\n\n\n\n\nCódigo\nadm_original |> group_by(Dept) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n\n \n  \n    Dept \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    A \n    601 \n    332 \n    0.64 \n  \n  \n    B \n    370 \n    215 \n    0.63 \n  \n  \n    C \n    322 \n    596 \n    0.35 \n  \n  \n    D \n    269 \n    523 \n    0.34 \n  \n  \n    E \n    147 \n    437 \n    0.25 \n  \n  \n    F \n    46 \n    668 \n    0.06 \n  \n\n\n\n\n\n\n¿Qué observas acerca de las tasas de admisión en cada departamento, diferenciadas por género? ¿Qué tiene qué ver con el número de personas que solicitan en cada departamento?\nEsta es una tabla descriptiva. Sin embargo, tiene que ser entendida en el contexto de los datos y su generación. ¿Qué hipótesis importantes sugieren estos datos? ¿Por qué hay tanta diferencia de género de solicitudes en algunos departamentos? ¿Por qué es sorprendente o no las variaciones en tasas de aceptación de estudiantes de cada género?"
  },
  {
    "objectID": "est-cd-1.html#diseño-estadístico-e-inferencia",
    "href": "est-cd-1.html#diseño-estadístico-e-inferencia",
    "title": "Estadística y ciencia de datos",
    "section": "Diseño estadístico e inferencia",
    "text": "Diseño estadístico e inferencia\nUna primera contribución importante de la estadística al análisis de datos contesta la siguiente pregunta:\n\nEl análisis correcto depende del proceso generador de datos\nIncluso cuando tenemos conocimiento detallado de dominio, es posible que algunos de nuestros supuestos sean cuestionables.\n\nSin embargo,\n\nSi pudiéramos alterar el proceso generador de datos de alguna manera razonable, ¿sería posible hacer un análisis que dependa de menos supuestos?\nEn lugar de usar los datos que tenemos a la mano, ¿podemos pensar en una manera de producir los datos que nos de más certeza acerca de las conclusiones que extraemos de ellos, y que nos permita extraer la mayor información posible?\n\nEl diseño estadístico (de experimentos, o de muestreo por ejemplo) nos guía a cómo modificar el proceso generador para simplificar el análisis, y en ese caso nos provee de herramientas para contestar preguntas de interés y cuantificar la incertidumbre ne las respuestas. Veremos más adelante por qué, pero por lo pronto señalamos alguna característica central:\n\nEn los ejemplos que vimos arriba, ocurren dificultades porque la aplicación del tratamiento o la selección de individuos depende de una variable relacionada también con la variable respuesta que nos interesa medir. Veremos que podemos usar aleatorización para cortar estas dependencias.\nEl diseño de muestras y experimentos también nos provee herramientas para decidir cuántos datos necesitamos y de qué tipo para dar respuestas con suficiente precisión para nuestros propósitos.\n\n\n\n\n\nJulious, Steven A, y Mark A Mullee. 1994. «Confounding and Simpson’s paradox». BMJ 309 (6967): 1480-81. https://doi.org/10.1136/bmj.309.6967.1480.\n\n\nPearl, J., M. Glymour, y N. P. Jewell. 2016. Causal Inference in Statistics: A Primer. Wiley. https://books.google.com.mx/books?id=L3G-CgAAQBAJ.\n\n\nTukey, John W. 1962. «The Future of Data Analysis». Ann. Math. Statist. 33 (1): 1-67. https://doi.org/10.1214/aoms/1177704711."
  },
  {
    "objectID": "resumenes.html",
    "href": "resumenes.html",
    "title": "Resúmenes y descripciones de datos",
    "section": "",
    "text": "Describir conjuntos de datos numéricos y categóricos\nDistintos resúmenes de datos que se pueden usar para esta descripción."
  },
  {
    "objectID": "analisis-resumenes-1.html",
    "href": "analisis-resumenes-1.html",
    "title": "1  Resúmenes para datos numéricos",
    "section": "",
    "text": "En esta parte establecemos las herramientas básicas que utilizaremos para describir y explorar conjuntos de datos. En esta parte veremos cómo analizar y comparar distintos bonches de datos numéricos."
  },
  {
    "objectID": "analisis-resumenes-1.html#cuantiles-o-percentiles-de-una-variable",
    "href": "analisis-resumenes-1.html#cuantiles-o-percentiles-de-una-variable",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.1 Cuantiles o percentiles de una variable",
    "text": "1.1 Cuantiles o percentiles de una variable\nEl primer concepto se refiere a entender cómo se distribuyen los datos a los largo de su escala de medición. Comenzamos con un ejemplo: los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos.\n\n\nCódigo\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\nslice_sample(propinas, n = 10) \n\n\n# A tibble: 10 × 6\n   cuenta_total propina fumador dia   momento num_personas\n          <dbl>   <dbl> <chr>   <chr> <chr>          <dbl>\n 1         34.6    3.68 Si      Dom   Cena               4\n 2         25.7    4    No      Dom   Cena               3\n 3         23.3    5.65 Si      Dom   Cena               2\n 4         22.8    2.18 No      Jue   Comida             3\n 5         15.5    2.02 Si      Jue   Comida             2\n 6         14.5    2    No      Jue   Comida             2\n 7         20.9    3.5  Si      Dom   Cena               3\n 8         11.6    1.5  Si      Sab   Cena               2\n 9         25.3    4.71 No      Dom   Cena               4\n10         20.9    4.08 No      Sab   Cena               2\n\n\nAquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta. Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día (Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida.\nEl primer tipo de comparaciones que nos interesa hacer es para una medición numérica es: ¿Varían mucho o poco los datos? ¿Cuáles son valores típicos o centrales? ¿Existen valores muy extremos alejados de valores típicos?\nSupongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales:\n\n\nCódigo\npropinas <- propinas |> \n  mutate(orden_cuenta = rank(cuenta_total, ties.method = \"first\"), \n         f = orden_cuenta / n()) \ncuenta <- propinas |>  select(orden_cuenta, f, cuenta_total) |>  arrange(f)\nbind_rows(head(cuenta), tail(cuenta)) |>  knitr::kable()\n\n\n\n\n \n  \n    orden_cuenta \n    f \n    cuenta_total \n  \n \n\n  \n    1 \n    0.0040984 \n    3.07 \n  \n  \n    2 \n    0.0081967 \n    5.75 \n  \n  \n    3 \n    0.0122951 \n    7.25 \n  \n  \n    4 \n    0.0163934 \n    7.25 \n  \n  \n    5 \n    0.0204918 \n    7.51 \n  \n  \n    6 \n    0.0245902 \n    7.56 \n  \n  \n    239 \n    0.9795082 \n    44.30 \n  \n  \n    240 \n    0.9836066 \n    45.35 \n  \n  \n    241 \n    0.9877049 \n    48.17 \n  \n  \n    242 \n    0.9918033 \n    48.27 \n  \n  \n    243 \n    0.9959016 \n    48.33 \n  \n  \n    244 \n    1.0000000 \n    50.81 \n  \n\n\n\n\n\ny graficamos los datos en orden, interpolando valores consecutivos.\n\n\n\n\n\nA esta función le llamamos la función de cuantiles para la variable cuenta total. Nos sirve para comparar directamente los distintos valores que observamos los datos según el orden que ocupan.\n\n\n\n\n\n\nCuantiles de datos numéricos\n\n\n\nEl cuantil \\(f\\) de un bonche de datos numéricos es el valor \\(q(f)\\), en la escala de medición de nuestros datos, tal que aproximadamente una fracción \\(f\\) de los datos está por abajo de \\(q(f)\\).\n\nAl cuantil \\(f=0.5\\) le llamamos la mediana.\nA los cuantiles \\(f=0.25\\) y \\(f=0.75\\) les llamamos cuantiles inferior y superior.\n\n\n\nNota: si los datos originales son \\(y_1, y_2, \\ldots, y_n\\), y los mismos datos ordenados son \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), entonces si \\(f= j/n\\), \\(q(f) = y_{(j)}\\). Si \\(f\\) toma un valor intermedio entre \\((j-1)/n\\) y \\(j/n\\), entonces interpolamos \\(y_{(j-1)}\\) y \\(y_{(j)}\\) para encontrar \\(q(f)\\).\nHay otras maneras de definir los cuantiles que pueden ser más convenientes. Los que estamos usando ahora son los cuantiles tipo 4:\n\nquantile(cuenta$cuenta_total, probs = c(6/244, 239/244), type = 4)\n\n2.459016% 97.95082% \n     7.56     44.30 \n\n\n¿Qué podemos leer en la gráfica de cuantiles?\nDispersión y valores centrales\n\nEl rango de datos va de unos 3 dólares hasta 50 dólares\nLos valores centrales (del cuantil 0.25 al 0.75, por ejemplo), están entre unos 13 y 25 dólares\nPodemos usar el cuantil 0.5 (mediana) para dar un valor central de esta distribución, que está alrededor de 18 dólares.\n\nY podemos dar resúmenes más refinados si es necesario\n\nEl cuantil 0.95 es de unos 35 dólares - sólo 5% de las cuentas son de más de 35 dólares\nEl cuantil 0.05 es de unos 8 dólares - sólo 5% de las cuentas son de 8 dólares o menos.\n\nFinalmente, la forma de la gráfica se interpreta usando su pendientes, haciendo comparaciones de diferentes partes de la gráfica:\n\nEntre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente es baja, lo que significa que al avanzar en los cuantiles, los valores observados no cambian mucho.\nCuando la pendiente es alta, quiere decir que los datos tienen más dispersión local o están más separados.\n\nY podemos considerar qué sucede en las colas de la distribucion:\n\nLa distribución de valores tiene asimetría: el 10% de las cuentas más altas tiene considerablemente más dispersión que el 10% de las cuentas más bajas. A veces decimos que la cola de la derecha es más larga que la cola de la izquierda\n\nEn algunos casos, es más natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos cuántos datos caen en cada cubeta:\n\n\n\n\n\nEs una gráfica más popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos.\nFinalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión menos común de Spear/Tufte (ST):\n\n\nCódigo\nlibrary(ggthemes)\ncuartiles <- quantile(cuenta$cuenta_total)\ncuartiles\n\n\n     0%     25%     50%     75%    100% \n 3.0700 13.3475 17.7950 24.1275 50.8100 \n\n\nCódigo\ng_1 <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + \n  labs(subtitle = \"Gráfica de cuantiles: Cuenta total\") +\n  geom_hline(yintercept = cuartiles[2], colour = \"gray\") + \n  geom_hline(yintercept = cuartiles[3], colour = \"gray\") +\n  geom_hline(yintercept = cuartiles[4], colour = \"gray\") +\n  geom_point(alpha = 0.5) + geom_line() \ng_2 <- ggplot(cuenta, aes(x = factor(\"ST\", levels =c(\"ST\")), y = cuenta_total)) + \n  geom_tufteboxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_3 <- ggplot(cuenta, aes(x = factor(\"T\"), y = cuenta_total)) + geom_boxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_4 <- ggplot(cuenta, aes(x = factor(\"P\"), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_1 + g_2 + g_3 + g_4 +\n  plot_layout(widths = c(8, 2, 2, 2))"
  },
  {
    "objectID": "analisis-resumenes-1.html#distribución-acumulada-empírica",
    "href": "analisis-resumenes-1.html#distribución-acumulada-empírica",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.2 Distribución acumulada empírica",
    "text": "1.2 Distribución acumulada empírica\nOtra forma de graficar la dispersión de los datos sin perder información es mediante la función de distribución acumulada empírica, o fda empírica. En un sentido, es la inversa de la función de cuantiles:\n\n\nCódigo\nggplot(cuenta, aes(x = cuenta_total)) +\n  stat_ecdf()\n\n\n\n\n\nEn esta gráfica, vemos que proporción de los datos que son iguales o están por debajo de cada valor en el eje horizontal.\n\nEn análisis de datos, es más frecuente utilizar la función de cuantiles pues existen versiones más generales que son útiles, por ejemplo, para evaluar ajuste de modelos probabilísticos\nEn la teoría, generalmente es más común utilizar la fda empírica, que tiene una única definición que veremos coincide con definiciones teóricas."
  },
  {
    "objectID": "analisis-resumenes-1.html#media-y-desviación-estándar",
    "href": "analisis-resumenes-1.html#media-y-desviación-estándar",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.3 Media y desviación estándar",
    "text": "1.3 Media y desviación estándar\nOtras medidas más comunes de localización y dispersión para conjuntos de datos son media y desviación estándar muestral.\nLa media de un conjunto de datos \\(x_1,\\ldots, x_n\\) es\n\\[\\bar{x} = \\frac{1}{n}\\sum x_i\\]\ny la desviación estándar es\n\\[\\hat{\\sigma} =\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\nEn general, no son muy apropiadas para iniciar el análisis exploratorio, y se requieren cuidados adicionales al utilizarlas, pues:\n\nSon medidas más difíciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Como ejercicio, intenta explicar intuitivamente qué es la media. Después prueba con la desviación estándar. Sin embargo, la mediana o el rango intercuartílico son fáciles de explicar.\nNo son resistentes a valores atípicos o erróneos. Su falta de resistencia los vuelve poco útiles en las primeras etapas de descripción, y muchas veces requieren transformaciones o cuidados adicionales/supuestos para evitar mal comportamiento por esa falta de resistencia.\n\nSin embargo,\n\nLa media y desviación estándar son computacionalmente convenientes, y para el trabajo de modelado, por ejemplo, tienen ventajas claras (cuando se cumplen supuestos). Por lo tanto regresaremos a estas medidas una vez que estudiemos modelos de probabilidad básicos.\nMuchas veces, ya sea por tradición, porque así se ha hecho el análisis antes, conviene usar estas medidas conocidas."
  },
  {
    "objectID": "analisis-resumenes-1.html#distribuciones-sesgadas-y-atípicos",
    "href": "analisis-resumenes-1.html#distribuciones-sesgadas-y-atípicos",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.4 Distribuciones sesgadas y atípicos",
    "text": "1.4 Distribuciones sesgadas y atípicos\nEn algunos casos tenemos que trabajar con mediciones que tienen una cola (usualmente la derecha) mucho más larga que la otra. Veamos cuáles son consecuencias típicas.\nConsideremos por ejemplos una muestra de los datos de ENIGH 2018\n\n\nCódigo\nenigh <- read_csv(\"./datos/enigh-ejemplo.csv\")\n\n\nY los deciles de ingreso son\n\n\nCódigo\nenigh <- mutate(enigh, ingreso_mensual_miles = INGTOT / 3000)\n\nenigh |> \n  summarise(\n    f = seq(0, 1, 0.1),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = seq(0, 1, 0.1), type = 4)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    0.0 \n    0.81 \n  \n  \n    0.1 \n    2.57 \n  \n  \n    0.2 \n    3.86 \n  \n  \n    0.3 \n    5.52 \n  \n  \n    0.4 \n    6.73 \n  \n  \n    0.5 \n    8.25 \n  \n  \n    0.6 \n    9.98 \n  \n  \n    0.7 \n    12.94 \n  \n  \n    0.8 \n    16.18 \n  \n  \n    0.9 \n    22.16 \n  \n  \n    1.0 \n    317.53 \n  \n\n\n\n\n\ndonde podemos ver cómo cuando nos movemos a deciles más altos, la dispersión aumenta. Existen algunos valores muy grandes. Un histograma no funciona muy bien con estos datos.\n\n\nCódigo\nggplot(enigh, aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSi filtramos los valores muy grandes, de todas formas encontramos una forma similar con una cola larga a la derecha:\n\n\nCódigo\nggplot(enigh |> filter(ingreso_mensual_miles < 90), \n       aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNótese que la media de estos datos no es un resúmen muy útil, porque es difícil de interpretar. Por los valores grandes, la media es considerablemente más alta que la mediana:\n\n\nCódigo\nenigh |> \n  summarise(\n    media = mean(ingreso_mensual_miles),\n    mediana =  quantile(ingreso_mensual_miles, probs = 0.5)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    media \n    mediana \n  \n \n\n  \n    12.04 \n    8.27 \n  \n\n\n\n\n\nEsta es otra razón para incluir información de cuantiles en la etapa descriptiva. Por ejemplo, podríamos resumir:\n\n\nCódigo\nenigh |> \n  summarise(\n    f = c(\"min\", 0.5, \"0.50\",  0.95, \"max\"),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = c(0, 0.05, 0.5, 0.95, 1))) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    min \n    0.81 \n  \n  \n    0.5 \n    1.92 \n  \n  \n    0.50 \n    8.27 \n  \n  \n    0.95 \n    32.24 \n  \n  \n    max \n    317.53 \n  \n\n\n\n\n\nOtra opción es utilizar una escala logarítmica. El logaritmo de los ingresos es más fácil de describir y veremos también más fácil de trabajar.\n\n\nCódigo\nggplot(enigh, \n       aes(x = ingreso_mensual_miles)) + \n  geom_histogram(binwidth = 0.12) +\n  scale_x_log10(breaks = c(1, 2, 4, 8, 16, 32, 64, 128, 256))\n\n\n\n\n\nPor las propiedades de los cuantiles, cualquier cantidad basada en cuantiles que se calcula en escala logarítmica puede pasarase a la escala original transformando\n\n\nCódigo\nquantile(log(enigh$ingreso_mensual_miles)) |> exp()\n\n\n         0%         25%         50%         75%        100% \n  0.8132833   4.7986689   8.2740789  14.1517930 317.5284167 \n\n\nCódigo\nquantile(enigh$ingreso_mensual_miles) \n\n\n         0%         25%         50%         75%        100% \n  0.8132833   4.7986692   8.2741033  14.1517942 317.5284167 \n\n\nNota: esto no sucede con medidas más complicadas como la media. El exponencial de la media de los logaritmos no es la media en la escala original."
  },
  {
    "objectID": "analisis-resumenes-1.html#comparando-grupos-con-variables-numéricas",
    "href": "analisis-resumenes-1.html#comparando-grupos-con-variables-numéricas",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.5 Comparando grupos con variables numéricas",
    "text": "1.5 Comparando grupos con variables numéricas\n\nEjemplo: precios de casas\nConsideramos datos de precios de ventas de la ciudad de Ames, Iowa. Nos interesa entender la variación del precio de las casas.\n\n\n\nCalculamos primeros unos cuantiles de los precios de las casas:\n\n\nCódigo\nquantile(casas |>  pull(precio_miles)) \n\n\n   0%   25%   50%   75%  100% \n 37.9 132.0 165.0 215.0 755.0 \n\n\nUna primera comparación que podemos hacer es considerar las distintas zonas de la ciudad. Podemos usar diagramas de caja y brazos para comparar precios en distintas zonas de la ciudad:\n\n\nCódigo\nggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip()\n\n\n\n\n\nNótese que de cada zona, los datos tienen una cola derecha más larga que la izquierda, e incluso hay valores extremos en la cola derecha que exceden el rango de variación usual. Una razón por la que puede suceder esto es que haya características particulares que agregan valor considerable a una casa, por ejemplo, el tamaño, una alberca, etc.\nEn primer lugar, podemos considerar el área de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo:\n\n\n\n\n\nCódigo\nggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip()\n\n\n\n\n\nNótese ahora que la variación alrededor de la media es mucho más simétrica, y ya no vemos tantos datos extremos. Aún más, la variación dentro de cada zona parece ser similar, y podríamos describir restos datos de la siguiente forma:\nCuantificamos la variación que observamos de zona a zona y la variación que hay dentro de zonas. La variación que vemos entre las medianas de la zona es:\n\n\nCódigo\ncasas |> group_by(nombre_zona) |> \n  summarise(mediana_zona = median(precio_m2)) |> \n  pull(mediana_zona) |> quantile() |> round()\n\n\n  0%  25%  50%  75% 100% \n 963 1219 1298 1420 1725 \n\n\nY las variaciones con respecto a las medianas dentro de cada zona, agrupadas, se resume como:\n\n\nCódigo\nquantile(casas |> group_by(nombre_zona) |> \n  mutate(residual = precio_m2 - median(precio_m2)) |> \n  pull(residual)) |> round()\n\n\n  0%  25%  50%  75% 100% \n-765 -166    0  172 1314 \n\n\nNótese que este último paso tiene sentido pues la variación dentro de las zonas, en términos de precio por metro cuadrado, es similar. Esto no lo podríamos hacer de manera efectiva si hubiéramos usado el precio de las casas sin ajustar por su tamaño.\nY vemos que la mayor parte de la variación del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tamaño de las casas. La variación dentro de cada zona es aproximadamente simétrica, aunque la cola derecha es ligeramente más larga con algunos valores extremos."
  },
  {
    "objectID": "analisis-resumenes-1.html#factor-y-respuesta-numéricos-opcional",
    "href": "analisis-resumenes-1.html#factor-y-respuesta-numéricos-opcional",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.6 Factor y respuesta numéricos (opcional)",
    "text": "1.6 Factor y respuesta numéricos (opcional)\nEn las secciones anteriores vimos cómo describir “bonches” de datos numéricos y categóricos. Adicionalmente, vimos cómo usar esas técnicas para comparar las descripciones a lo largo de varios subconjuntos de los datos.\nEn estos casos, muchas veces llamamos factor a la variables que forma los grupos, y respuesta a la variable que estamos comparando. Por ejemplo, en el caso de tomadores de té comparamos uso de complementos (respuesta) a lo largo de consumidores de distintos tipos de té (factor) En el caso de los precios de las casas comparamos el precio de las casas (respuesta) dependiendo del vecindario (factor) dónde se encuentran.\nCuando tenemos una factor numérico y una respuesta numérica podemos comenzar haciendo diagramas de dispersión. Por ejemplo,"
  },
  {
    "objectID": "analisis-resumenes-1.html#ejemplo-cuenta-total-y-propina",
    "href": "analisis-resumenes-1.html#ejemplo-cuenta-total-y-propina",
    "title": "1  Resúmenes para datos numéricos",
    "section": "Ejemplo: cuenta total y propina",
    "text": "Ejemplo: cuenta total y propina\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n\n\nPodríamos comenzar haciendo:\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_point() + geom_rug(colour = \"salmon\", alpha = 0.5)\n\n\n\n\n\nAhora queremos comparar la distribución de propina (respuesta) para distintos niveles del factor (cuenta_total). Por ejemplo, ¿cómo se compara propina cuando la cuenta es de 15 dólares vs 30 dólares?\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_vline(xintercept = c(15, 30), colour = \"red\") +\n   geom_point() \n\n\n\n\n\nVemos que los datos de propinas alrededor de 30 dólares están centrados en valores más grandes que en el nivel de 15 dólares, y también que hay más dispersión en el nivel de 30 dólares. Sin embargo, vemos que tenemos un problema: existen realmente muy pocos datos que tengan exactamente 15 o 30 dólares de cuenta. La estrategia es entonces considerar qué sucede cuando la cuenta está alrededor de 15 o alrededor de 30 dólares, donde alrededor depende del problema particular y de cuántos datos tenemos:\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.5) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.5) +\n   geom_point() \n\n\n\n\n\nConsiderando estos grupos de datos, podemos describir de las siguiente forma, por ejemplo:\n\n\nCódigo\npropinas |> \n   mutate(grupo = cut(cuenta_total,  breaks = c(0, 13, 17, 28, 32))) |> \n   filter(grupo %in% c(\"(13,17]\", \"(28,32]\")) |> \n   group_by(grupo) |> \n   summarise(\n      n = n(),\n      q10 = quantile(propina, 0.10),\n      mediana = quantile(propina, 0.5),\n      q90 = quantile(propina, 0.90),\n      rango_cuartiles = quantile(propina, 0.75) - quantile(propina, 0.25)) |> \n   kable(digits = 2)\n\n\n\n\n \n  \n    grupo \n    n \n    q10 \n    mediana \n    q90 \n    rango_cuartiles \n  \n \n\n  \n    (13,17] \n    57 \n    1.85 \n    2.47 \n    3.49 \n    1.0 \n  \n  \n    (28,32] \n    16 \n    2.02 \n    3.69 \n    5.76 \n    2.2 \n  \n\n\n\n\n\nConde confirmamos que el nivel general de propinas es más alto alrededor de cuentas de total 30 que de total 15, y la dispersión también es mayor. Podríamos hacer un diagrama de caja y brazos también."
  },
  {
    "objectID": "analisis-resumenes-1.html#suavizadores-locales",
    "href": "analisis-resumenes-1.html#suavizadores-locales",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.7 Suavizadores locales",
    "text": "1.7 Suavizadores locales\nEl enfoque del ejemplo anterior puede ayudar en algunos casos nuestra tarea descriptiva, pero quisiéramos tener un método más general y completo para entender cómo es una respuesta numérica cuando el factor es también numérico.\nEn este caso, podemos hacer por ejemplo medias o medianas locales. La idea general es, en términos de nuestro ejemplo de propinas:\n\nQueremos producir un resumen en un valor de cuenta total \\(x\\).\nConsideramos valores de propina asociados a cuentas totales en un intervalo \\([x-e, x+e]\\).\nCalculamos estadísticas resumen en este rango para la respuesta\nUsualmente también ponderamos más alto valores que están cerca de \\(x\\) y ponderamos menos valores más lejanos a \\(x\\)\n\nEste tipo de suavizadores se llaman a veces suavizadores loess (ver (Cleveland 1993)).\nPor ejemplo,\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_smooth(method = \"loess\", span = 0.5, degree= 0, \n               method.args = list(family = \"symmetric\"), se = FALSE) \n\n\nWarning: Ignoring unknown parameters: degree\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCódigo\n# symmetric es un método robusto iterativo, que reduce el peso de atípicos\n\n\nEl parametro span controla el tamaño de la ventana de datos que se toma en cada punto. Nótese como alrededor de 15 y 30 los valores por donde pasa el suavizador son similares a las medianas que escribimos arriba.\nPodemos ajustar en cada ventana tambien rectas de minimos cuadrados, y obtener un suavizador de tipo lineal. En la siguiente gráfica mostramos cómo funciona este suavizador para distintos tamaños de ventanas (span)\n\n\n\nSuavizador loess\n\n\n\n\n\nLos suavizadores loess tienen como fin mostrar alrededor de qué valor de distribuye la respuesta (eje vertical) para distintos valores del factor (eje horizontal). Se escoge span suficientemente baja de forma que mostremos patrones claros en los datos y casi no capturemos variación debida a los tamaños de muestra chicos.\n\n\nEn la animación anterior, un valor de span de 0.15 funciona aprpiadamente, y uno de 0.05 es demasiado bajo y uno de 1.0 es demasiado alto. Es importante explorar con el valor de span pues depende de cuántos datos tenemos y cómo es su dispersión.\nPodemos también mostrar estimaciones de medianas y cuantiles de la siguiente forma (nota: es necesario escoger lambda con cuidado, cuanto más alto sea lambda más suave es la curva obtenida):\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75)) +\n   scale_y_continuous(breaks = seq(0, 10, 1))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 15)\n\n\n\n\n\nFinalmente, entendimiento de los datos no permite también hacer gráficas más útiles. En este ejemplo particular podría por ejemplo calcular el porcentaje de la propina sobre la cuenta total:\n\n\nCódigo\npropinas <- mutate(propinas, pct_propina = propina / cuenta_total)\nggplot(propinas, aes(x = cuenta_total, y = pct_propina)) +\n   geom_point() +\n   scale_y_continuous(breaks = seq(0,1, 0.05)) +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 15)\n\n\n\n\n\nObserva que la descripción es más simple que si usamos propina cruda y cuenta\n\nPara cuentas chicas, el porcentaje de propina puede ser muy alto (aún cuando la propina en sí no es tan grande):\n\n\n\nCódigo\nfilter(propinas, pct_propina > 0.30) |> \n  arrange(desc(pct_propina)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    cuenta_total \n    propina \n    fumador \n    dia \n    momento \n    num_personas \n    pct_propina \n  \n \n\n  \n    7.25 \n    5.15 \n    Si \n    Dom \n    Cena \n    2 \n    0.71 \n  \n  \n    9.60 \n    4.00 \n    Si \n    Dom \n    Cena \n    2 \n    0.42 \n  \n  \n    3.07 \n    1.00 \n    Si \n    Sab \n    Cena \n    1 \n    0.33 \n  \n\n\n\n\n\n\nPara cuentas relativamente chicas (10 dólares, el porcentaje de propina está por encima de 15%). Este porcentaje tiende a reducirse a valores 10% y 15% para cuentas más grandes\nExiste variación considerable alrededor de estos valores centrales. El rango intercuartiles es aproximadamente de 5 puntos porcentuales.\n\nO de manera más resumida:\n\nLa mediana de propinas está ligeramente por arriba de 15% para cuantas relativamente chicas. Esta mediana baja hasta alrededor de 10%-15% para cuentas más grandes (más de 40 dólares)\nLa mitad de las propinas no varía más de unos 3 puntos porcentuales alrededor de estas medianas.\nExisten propinas atípicas: algunas muy bajas de 1 dólar, muy por debajo del 15%, y ocasionalmente algunas muy altas en porcentaje. Estas últimas ocurren ocasinalmente especialmente en cuentas chicas (por ejemplo, una propina de 1 dólar en una cuenta de 3 dólares).\n\n\n\n\n\nCleveland, William S. 1993. Visualizing Data. Hobart Press."
  },
  {
    "objectID": "analisis-resumenes-2.html",
    "href": "analisis-resumenes-2.html",
    "title": "2  Datos univariados categóricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones individuales. Consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "analisis-resumenes-2.html#datos-categóricos-y-tablas",
    "href": "analisis-resumenes-2.html#datos-categóricos-y-tablas",
    "title": "2  Datos univariados categóricos",
    "section": "2.1 Datos categóricos y tablas",
    "text": "2.1 Datos categóricos y tablas\nUna medición categórica es una que toma sus valores posibles en un conjunto que no es numérico. Consideremos los siguiente datos de 300 tomadores de té (Lê, Josse, y Husson (2008)):\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\n\n\n\n\nCódigo\n# cargamos y traducimos los datos\nte_tbl <- read_csv(\"./datos/tea.csv\") |> \n   mutate(id = row_number()) |> \n   select(id, Tea, How, sugar, how, price, age) |> \n   rename(tipo = Tea, complementos = How, azucar = sugar, \n          presentacion = how, precio = price, edad = age) |> \n   mutate(tipo = recode(tipo, black = \"negro\", green = \"verde\", `Earl Grey` = \"earl_grey\"),\n          complementos = recode(complementos, alone = \"solo\", milk = \"leche\", \n                                lemon = \"limón\", .default = \"otros\"),\n          azucar = recode(azucar, sugar = \"con_azúcar\", No.sugar = \"sin_azúcar\"),\n          presentacion = recode(presentacion, `tea bag`=\"bolsa\", \n                                unpackaged = \"suelto\", .default = \"mixto\"),\n          precio = recode(precio, p_upscale = \"fino\", p_branded = \"de_marca\",\n                          p_private_label = \"marca_propia\", p_variable = \"variable\",\n                          .default = \"no_sabe\"))\nsample_n(te_tbl, 10) |> kable()\n\n\n\n\n \n  \n    id \n    tipo \n    complementos \n    azucar \n    presentacion \n    precio \n    edad \n  \n \n\n  \n    59 \n    verde \n    limón \n    con_azúcar \n    mixto \n    variable \n    60 \n  \n  \n    228 \n    negro \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    37 \n  \n  \n    41 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    71 \n  \n  \n    205 \n    verde \n    solo \n    sin_azúcar \n    bolsa \n    de_marca \n    32 \n  \n  \n    36 \n    earl_grey \n    leche \n    con_azúcar \n    bolsa \n    variable \n    39 \n  \n  \n    223 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    23 \n  \n  \n    257 \n    negro \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    25 \n  \n  \n    237 \n    earl_grey \n    leche \n    con_azúcar \n    bolsa \n    de_marca \n    25 \n  \n  \n    25 \n    earl_grey \n    solo \n    sin_azúcar \n    suelto \n    de_marca \n    60 \n  \n  \n    169 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    variable \n    25 \n  \n\n\n\n\n\nMediciones como tipo, presentación o azucar son variables categóricas. Desde el punto de vista univariado, generalmente no es necesario resumir, sino simplemente agrupar y contar cuántas veces ocurre cada categoría. Por ejemplo\n\n\nCódigo\ntabla_1 <- te_tbl |> count(tipo) |> \n   arrange(desc(n))\ntabla_1 |> kable()\n\n\n\n\n \n  \n    tipo \n    n \n  \n \n\n  \n    earl_grey \n    193 \n  \n  \n    negro \n    74 \n  \n  \n    verde \n    33 \n  \n\n\n\n\n\nUsualmente es más útil reportar la porporción o porcentaje de casos por categoría\n\n\nCódigo\ntabla_2 <- te_tbl |> \n   count(tipo) |> \n   mutate(n_total = sum(n), prop = n / n_total) |> \n   select(tipo, n_total, prop) |> \n   mutate(across(where(is.numeric), round, 2)) |> \n   arrange(desc(prop))\ntabla_2 |> kable()\n\n\n\n\n \n  \n    tipo \n    n_total \n    prop \n  \n \n\n  \n    earl_grey \n    300 \n    0.64 \n  \n  \n    negro \n    300 \n    0.25 \n  \n  \n    verde \n    300 \n    0.11 \n  \n\n\n\n\n\nPodemos hacer varias variables juntas de la siguiente manera:\n\n\nCódigo\nperfiles_col_tbl <- te_tbl |> select(id, tipo, complementos, presentacion, azucar) |> \n   pivot_longer(cols = tipo:azucar, names_to = \"variable\", values_to = \"valor\") |> \n   count(variable, valor) |> \n   group_by(variable) |> \n   mutate(n_total = sum(n), prop = n / n_total) |>\n   mutate(prop = round(prop, 2)) |> \n   arrange(desc(prop), .by_group = TRUE)\nperfiles_col_tbl |> kable()\n\n\n\n\n \n  \n    variable \n    valor \n    n \n    n_total \n    prop \n  \n \n\n  \n    azucar \n    sin_azúcar \n    155 \n    300 \n    0.52 \n  \n  \n    azucar \n    con_azúcar \n    145 \n    300 \n    0.48 \n  \n  \n    complementos \n    solo \n    195 \n    300 \n    0.65 \n  \n  \n    complementos \n    leche \n    63 \n    300 \n    0.21 \n  \n  \n    complementos \n    limón \n    33 \n    300 \n    0.11 \n  \n  \n    complementos \n    otros \n    9 \n    300 \n    0.03 \n  \n  \n    presentacion \n    bolsa \n    170 \n    300 \n    0.57 \n  \n  \n    presentacion \n    mixto \n    94 \n    300 \n    0.31 \n  \n  \n    presentacion \n    suelto \n    36 \n    300 \n    0.12 \n  \n  \n    tipo \n    earl_grey \n    193 \n    300 \n    0.64 \n  \n  \n    tipo \n    negro \n    74 \n    300 \n    0.25 \n  \n  \n    tipo \n    verde \n    33 \n    300 \n    0.11 \n  \n\n\n\n\n\nPara leer más fácil, imprimimos individualmente estas tablas, o hacemos algo como lo que sigue para mostrarlas todas juntas:\n\n\nCódigo\nperfiles_col_tbl |> \n   ungroup() |> \n   select(-variable, -n_total) |> \n   kable() |>  \n   pack_rows(index = table(perfiles_col_tbl$variable))\n\n\n\n\n \n  \n    valor \n    n \n    prop \n  \n \n\n  azucar\n\n    sin_azúcar \n    155 \n    0.52 \n  \n  \n    con_azúcar \n    145 \n    0.48 \n  \n  complementos\n\n    solo \n    195 \n    0.65 \n  \n  \n    leche \n    63 \n    0.21 \n  \n  \n    limón \n    33 \n    0.11 \n  \n  \n    otros \n    9 \n    0.03 \n  \n  presentacion\n\n    bolsa \n    170 \n    0.57 \n  \n  \n    mixto \n    94 \n    0.31 \n  \n  \n    suelto \n    36 \n    0.12 \n  \n  tipo\n\n    earl_grey \n    193 \n    0.64 \n  \n  \n    negro \n    74 \n    0.25 \n  \n  \n    verde \n    33 \n    0.11"
  },
  {
    "objectID": "analisis-resumenes-2.html#comparando-grupos-con-variables-categóricas",
    "href": "analisis-resumenes-2.html#comparando-grupos-con-variables-categóricas",
    "title": "2  Datos univariados categóricos",
    "section": "2.2 Comparando grupos con variables categóricas",
    "text": "2.2 Comparando grupos con variables categóricas\nEste análisis generalmente es más interesante cuando comparamos grupos. Supongamos que nos interesa ver si existe una relación entre usar el tipo de té que toman estas personas y el uso de complementos como leche o limón. Podríamos entonces dividir los datos según el uso de azúcar y repetir para cada grupo las tablas mostradas arriba:\n\n\nCódigo\nperfiles_col_tbl <- te_tbl |> count(complementos, tipo) |> \n   group_by(tipo) |> \n   mutate(prop = n / sum(n)) |>\n   group_by(complementos) |> \n   select(-n) |> \n   pivot_wider(names_from = tipo, values_from = prop, values_fill = 0)\nperfiles_col_tbl |>  kable(digits = 2, caption = \"Perfiles por columna\")\n\n\n\n\nPerfiles por columna\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n  \n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n  \n\n\n\n\n\nComparando los perfiles de las columnas observamos variaciones interesantes: por ejemplo, los tomadores de Earl Grey tienden a usar más limón como complemento que otros grupos. Son resúmenes univariados que ahora comparamos a lo largo de grupos. Podemos hacer las comparaciones más simples si hacemos todas contra una columna marginal del uso general en la muestra de los distintos complementos_\n\n\nCódigo\ncomp_tbl <- te_tbl |> count(complementos) |> mutate(total = n / sum(n))\nperfiles_col_tbl <- left_join(perfiles_col_tbl, comp_tbl) |> \n      arrange(desc(total)) |> \n      select(-n)\n\n\nJoining, by = \"complementos\"\n\n\nCódigo\nperfiles_col_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n    0.65 \n  \n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n    0.21 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n    0.11 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n    0.03 \n  \n\n\n\n\n\nEn este punto, vemos que hay coincidencias y diferencias entre los grupos de tomadores de té. Podemos expresar esto de manera simple calculando índices contra la columna de total:\n\n\nCódigo\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric), ~ .x / total)) |> \n   select(-total)\nres_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    solo \n    1.02 \n    0.87 \n    1.17 \n  \n  \n    leche \n    0.94 \n    1.22 \n    0.87 \n  \n  \n    limón \n    1.13 \n    0.86 \n    0.55 \n  \n  \n    otros \n    0.52 \n    2.70 \n    0.00 \n  \n\n\n\n\n\nValores por encima de 1 indican columnas por arriba de la población general, y análogamente para valores por debajo de uno. Estas cantidades pueden escribirse en términos porcentuales, o se les puede restar 1 para terminar como una variación porcentual del promedio. A estas cantidades se les llama residuales crudos:\n\n\nCódigo\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) \nres_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.70 \n    -1.00 \n    0.03 \n  \n\n\n\n\n\nPodemos finalmente marcar la tabla:\n\n\nCódigo\nres_tbl |>  mutate(across(where(is.numeric), round, 2)) |> \n   mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.1, \"black\", \n                                         ifelse(.x < -0.1, \"red\", \"gray\"))))) |>\n   arrange(desc(total)) |> \n   kable(escape = FALSE) \n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03 \n  \n\n\n\n\n\n\n\n\n\n\n\nPerfiles\n\n\n\nA este tipo de análisis de tablas cruzadas a veces se le llama análisis de perfiles columna. Nos permite entender cómo varía la distribución de la variable de los renglones según el grupo indicado por la columna. - Desviaciones grandes en los residuales indican asociaciones fuertes entre la variable de los reglones y de las columnas - Recordemos que este análisis aplica a la muestra de datos que tenemos. Columnas con pocos individuos tienden a mostrar más variación y debemos ser cuidadosos al generalizar.\n\n\nPodemos incluir también totales para ayudarnos a juzgar las variaciones:\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n     \n    193 \n    74 \n    33 \n    1.00 \n  \n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03"
  },
  {
    "objectID": "analisis-resumenes-2.html#observación-perfiles-renglón-y-columna",
    "href": "analisis-resumenes-2.html#observación-perfiles-renglón-y-columna",
    "title": "2  Datos univariados categóricos",
    "section": "2.3 Observación: perfiles renglón y columna",
    "text": "2.3 Observación: perfiles renglón y columna\nEl análisis también lo podemos hacer con los perfiles de los renglones. Los residuales crudos que usamos para interpretar son los mismos. La razón es la siguiente:\nPara los perfiles columna, si escribimos \\(n_{+j}\\) como los totales por columna, y \\(n_{i+}\\) los totales por renglón, tenemos que los perfiles columna son:\n\\[c_{i,j} = \\frac{n_{i,j}}{n_{+j}}\\]\nEscribimos también \\(c_i = \\frac{n_{i+}}{n}\\) y \\(r_j = \\frac{n_{+j}}{n}\\) como los porcentajes marginales por columna y por renglón respectivamente.\nLos residuales son entonces\n\\[r_{i,j} = \\frac{\\frac{n_{i,j}}{n_{+,j}}} { \\frac{n_{i,+}}{n}} - 1 = \\frac{p_{i,j} - r_ic_j}{r_ic_j}\\]\nNótese que no importa entonces cómo comencemos el cálculo, por renglones o por columnas, el resultado es el mismo."
  },
  {
    "objectID": "analisis-resumenes-2.html#visualización-de-tablas-cruzadas-opcional",
    "href": "analisis-resumenes-2.html#visualización-de-tablas-cruzadas-opcional",
    "title": "2  Datos univariados categóricos",
    "section": "2.4 Visualización de tablas cruzadas (opcional)",
    "text": "2.4 Visualización de tablas cruzadas (opcional)\nPara tablas más grandes, muchas veces las técnicas que mostramos arriba no son suficientes para entender y presentar patrones importantes en los datos. En estos casos, buscamos reducir la dimensionalidad de los datos para poder presentarlos en una gráfica de dos dimensiones.\nPodemos utilizar análisis de correspondencias. A grandes rasgos (ver (Izenman 2009) para los detalles) buscamos una representación tal que:\n\nCada categoría de las columnas está representada por una flecha que sale del origen de nuestra gráfica\nCada categoría de los renglones está representada por un punto en nuestra gráfica\nSi proyectamos los puntos (renglones) sobre las direcciones de las columnas, entonces el tamaño de la proyección es lo más cercano posible a el residual correspondiente de las tablas del análisis mostrado arriba.\n\nPara construir esta gráfica, entonces, existe un proceso de optimización que busca representar lo más fielmente los residuales del análisis mostrado arriba en dos dimensiones, y de esta forma buscamos recuperar una buena parte de la información de los residuales de una manera más compacta."
  },
  {
    "objectID": "analisis-resumenes-2.html#ejemplo-tés-y-complementos",
    "href": "analisis-resumenes-2.html#ejemplo-tés-y-complementos",
    "title": "2  Datos univariados categóricos",
    "section": "2.5 Ejemplo: tés y complementos",
    "text": "2.5 Ejemplo: tés y complementos\n\n\nCódigo\nlibrary(ca)\ncorr_te <- ca(table(te_tbl$complementos, te_tbl$tipo))\nplot(corr_te, map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\nLa contribución de cada dimensión a la aproximación se indica en los ejes. Como vemos en la gráfica, y la suma de las contribuciones nos da la calidad de la representación, que en este caso es perfecta.\n::: {.cell type=‘comentario’}\n\n\nEl análisis de correspondencias es un tema relativamente avanzado de estadística multivariada, y su definición precisa requiere de matemáticas más avanzadas (por ejemplo la descomposición en valores singulares).\nCualquier hallazgo obtenido en este tipo de análisis debe ser verificado en las tablas correspondientes de perfiles\nHay distintos tipos de gráficas (biplots) asociadas al análisis de correspondencias, que privilegian representar mejor a distintos tipos de características de los datos\n\n:::"
  },
  {
    "objectID": "analisis-resumenes-2.html#ejemplo-robo-en-tiendas",
    "href": "analisis-resumenes-2.html#ejemplo-robo-en-tiendas",
    "title": "2  Datos univariados categóricos",
    "section": "2.6 Ejemplo: robo en tiendas",
    "text": "2.6 Ejemplo: robo en tiendas\nConsideramos los siguientes datos de robos en tiendas en Holanda por personas de distintas edades y genéros (Izenman (2009)). En este caso, las variables ya están cruzadas:\n\n\nCódigo\nhurto_tbl <- read_csv(\"./datos/hurto.csv\") |> \n   mutate(grupo = ifelse(grupo == \"-12 h\", \"01-12 h\", grupo),\n          grupo = ifelse(grupo == \"-12 m\", \"01-12 m\", grupo))\n\n\nRows: 18 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): grupo\ndbl (13): ropa, accesorios, tabaco, escritura, libros, discos, bienes, dulce...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCódigo\nhurto_tbl |> kable()\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    discos \n    bienes \n    dulces \n    juguetes \n    joyería \n    perfumes \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    81 \n    66 \n    150 \n    667 \n    67 \n    24 \n    47 \n    430 \n    743 \n    132 \n    32 \n    197 \n    209 \n  \n  \n    12-14 h \n    138 \n    204 \n    340 \n    1409 \n    259 \n    272 \n    117 \n    637 \n    684 \n    408 \n    57 \n    547 \n    550 \n  \n  \n    15-17 h \n    304 \n    193 \n    229 \n    527 \n    258 \n    368 \n    98 \n    246 \n    116 \n    298 \n    61 \n    402 \n    454 \n  \n  \n    18-20 h \n    384 \n    149 \n    151 \n    84 \n    146 \n    141 \n    61 \n    40 \n    13 \n    71 \n    52 \n    138 \n    252 \n  \n  \n    21-29 h \n    942 \n    297 \n    313 \n    92 \n    251 \n    167 \n    193 \n    30 \n    16 \n    130 \n    111 \n    280 \n    624 \n  \n  \n    30-39 h \n    359 \n    109 \n    136 \n    36 \n    96 \n    67 \n    75 \n    11 \n    16 \n    31 \n    54 \n    200 \n    195 \n  \n  \n    40-49 h \n    178 \n    53 \n    121 \n    36 \n    48 \n    29 \n    50 \n    5 \n    6 \n    14 \n    41 \n    152 \n    88 \n  \n  \n    50-64 h \n    137 \n    68 \n    171 \n    37 \n    56 \n    27 \n    55 \n    17 \n    3 \n    11 \n    50 \n    211 \n    90 \n  \n  \n    64+ h \n    45 \n    28 \n    145 \n    17 \n    41 \n    7 \n    29 \n    28 \n    8 \n    10 \n    28 \n    111 \n    34 \n  \n  \n    01-12 m \n    71 \n    19 \n    59 \n    224 \n    19 \n    7 \n    22 \n    137 \n    113 \n    162 \n    70 \n    15 \n    24 \n  \n  \n    12-14 m \n    241 \n    98 \n    111 \n    346 \n    60 \n    32 \n    29 \n    240 \n    98 \n    548 \n    178 \n    29 \n    58 \n  \n  \n    15-17 m \n    477 \n    114 \n    58 \n    91 \n    50 \n    27 \n    41 \n    80 \n    14 \n    303 \n    141 \n    9 \n    72 \n  \n  \n    18-20 m \n    436 \n    108 \n    76 \n    18 \n    32 \n    12 \n    32 \n    12 \n    10 \n    74 \n    70 \n    14 \n    67 \n  \n  \n    21-29 m \n    1180 \n    207 \n    132 \n    30 \n    61 \n    21 \n    65 \n    16 \n    12 \n    100 \n    104 \n    30 \n    157 \n  \n  \n    30-39 m \n    1009 \n    165 \n    121 \n    27 \n    43 \n    9 \n    74 \n    14 \n    31 \n    48 \n    81 \n    36 \n    107 \n  \n  \n    40-49 m \n    517 \n    102 \n    93 \n    23 \n    31 \n    7 \n    51 \n    10 \n    8 \n    22 \n    46 \n    24 \n    66 \n  \n  \n    50-64 m \n    488 \n    127 \n    214 \n    27 \n    57 \n    13 \n    79 \n    23 \n    17 \n    26 \n    69 \n    35 \n    64 \n  \n  \n    64+ m \n    173 \n    64 \n    215 \n    13 \n    44 \n    0 \n    39 \n    42 \n    6 \n    12 \n    41 \n    11 \n    55 \n  \n\n\n\n\n\nEsta tabla es más grande y difícil de entender tal cual está. Comenzamos por examinar las marginales:\n\n\nCódigo\nhurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop)) |> \n   kable(digits = 2)\n\n\n\n\n \n  \n    producto \n    n \n    prop \n  \n \n\n  \n    ropa \n    7160 \n    0.22 \n  \n  \n    escritura \n    3704 \n    0.11 \n  \n  \n    otros \n    3166 \n    0.10 \n  \n  \n    tabaco \n    2835 \n    0.09 \n  \n  \n    herramientas \n    2441 \n    0.07 \n  \n  \n    joyería \n    2400 \n    0.07 \n  \n  \n    accesorios \n    2171 \n    0.07 \n  \n  \n    dulces \n    2018 \n    0.06 \n  \n  \n    juguetes \n    1914 \n    0.06 \n  \n  \n    libros \n    1619 \n    0.05 \n  \n  \n    perfumes \n    1286 \n    0.04 \n  \n  \n    discos \n    1230 \n    0.04 \n  \n  \n    bienes \n    1157 \n    0.03 \n  \n\n\n\n\n\n\n\nCódigo\ngrupos_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(grupo) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop))\ngrupos_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    grupo \n    n \n    prop \n  \n \n\n  \n    12-14 h \n    5622 \n    0.17 \n  \n  \n    15-17 h \n    3554 \n    0.11 \n  \n  \n    21-29 h \n    3446 \n    0.10 \n  \n  \n    01-12 h \n    2845 \n    0.09 \n  \n  \n    21-29 m \n    2115 \n    0.06 \n  \n  \n    12-14 m \n    2068 \n    0.06 \n  \n  \n    30-39 m \n    1765 \n    0.05 \n  \n  \n    18-20 h \n    1682 \n    0.05 \n  \n  \n    15-17 m \n    1477 \n    0.04 \n  \n  \n    30-39 h \n    1385 \n    0.04 \n  \n  \n    50-64 m \n    1239 \n    0.04 \n  \n  \n    40-49 m \n    1000 \n    0.03 \n  \n  \n    18-20 m \n    961 \n    0.03 \n  \n  \n    01-12 m \n    942 \n    0.03 \n  \n  \n    50-64 h \n    933 \n    0.03 \n  \n  \n    40-49 h \n    821 \n    0.02 \n  \n  \n    64+ m \n    715 \n    0.02 \n  \n  \n    64+ h \n    531 \n    0.02 \n  \n\n\n\n\n\nIntentamos análisis de correspondencias para comparar los perfiles columna:\n\n\nCódigo\nhurto_df <- as.data.frame(hurto_tbl)\nrownames(hurto_df) <- hurto_tbl$grupo\nhurto_df$grupo <- NULL\ncorr_hurto <- ca(hurto_df)\ngrafica_datos <- plot(corr_hurto, map = \"rowgreen\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\nSegún esta gráfica, ¿qué categorias de productos están sobrerrepresentadas en cada grupo de edad? ¿Cómo tendrían que verse el análisis de perfiles columna?\n\nComo se aprecia, en la siguiente tabla, es difícil entender los patrones generales en los datos. Quitamos algunas columnas para imprimir más fácilmente\n\n\nCódigo\nperfiles_hurto_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   mutate(prop = n / sum(n)) |> \n   select(-n) |> \n   pivot_wider(names_from = producto, values_from = prop) \nperfiles_hurto_tbl |> \n   select(-bienes, -discos, -perfumes) |> \n   kable(digits = 2) |> \n   kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    0.01 \n    0.03 \n    0.05 \n    0.18 \n    0.04 \n    0.21 \n    0.39 \n    0.06 \n    0.08 \n    0.07 \n  \n  \n    12-14 h \n    0.02 \n    0.09 \n    0.12 \n    0.38 \n    0.16 \n    0.32 \n    0.36 \n    0.17 \n    0.22 \n    0.17 \n  \n  \n    15-17 h \n    0.04 \n    0.09 \n    0.08 \n    0.14 \n    0.16 \n    0.12 \n    0.06 \n    0.12 \n    0.16 \n    0.14 \n  \n  \n    18-20 h \n    0.05 \n    0.07 \n    0.05 \n    0.02 \n    0.09 \n    0.02 \n    0.01 \n    0.03 \n    0.06 \n    0.08 \n  \n  \n    21-29 h \n    0.13 \n    0.14 \n    0.11 \n    0.02 \n    0.16 \n    0.01 \n    0.01 \n    0.05 \n    0.11 \n    0.20 \n  \n  \n    30-39 h \n    0.05 \n    0.05 \n    0.05 \n    0.01 \n    0.06 \n    0.01 \n    0.01 \n    0.01 \n    0.08 \n    0.06 \n  \n  \n    40-49 h \n    0.02 \n    0.02 \n    0.04 \n    0.01 \n    0.03 \n    0.00 \n    0.00 \n    0.01 \n    0.06 \n    0.03 \n  \n  \n    50-64 h \n    0.02 \n    0.03 \n    0.06 \n    0.01 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.09 \n    0.03 \n  \n  \n    64+ h \n    0.01 \n    0.01 \n    0.05 \n    0.00 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.05 \n    0.01 \n  \n  \n    01-12 m \n    0.01 \n    0.01 \n    0.02 \n    0.06 \n    0.01 \n    0.07 \n    0.06 \n    0.07 \n    0.01 \n    0.01 \n  \n  \n    12-14 m \n    0.03 \n    0.05 \n    0.04 \n    0.09 \n    0.04 \n    0.12 \n    0.05 \n    0.23 \n    0.01 \n    0.02 \n  \n  \n    15-17 m \n    0.07 \n    0.05 \n    0.02 \n    0.02 \n    0.03 \n    0.04 \n    0.01 \n    0.13 \n    0.00 \n    0.02 \n  \n  \n    18-20 m \n    0.06 \n    0.05 \n    0.03 \n    0.00 \n    0.02 \n    0.01 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n  \n  \n    21-29 m \n    0.16 \n    0.10 \n    0.05 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.04 \n    0.01 \n    0.05 \n  \n  \n    30-39 m \n    0.14 \n    0.08 \n    0.04 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n    0.02 \n    0.01 \n    0.03 \n  \n  \n    40-49 m \n    0.07 \n    0.05 \n    0.03 \n    0.01 \n    0.02 \n    0.00 \n    0.00 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    50-64 m \n    0.07 \n    0.06 \n    0.08 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    64+ m \n    0.02 \n    0.03 \n    0.08 \n    0.00 \n    0.03 \n    0.02 \n    0.00 \n    0.00 \n    0.00 \n    0.02 \n  \n\n\n\n\n\n\n\nCódigo\nres_hurto_tbl <- left_join(perfiles_hurto_tbl, grupos_tbl |> rename(total = prop)) |> \n    select(-n) |> \n    select(-bienes, -discos, -perfumes) |> \n    mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) |> \n    mutate(across(where(is.numeric), round, 2)) \n\n\nJoining, by = \"grupo\"\n\n\nCódigo\nres_hurto_tbl |> \n    mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    select(-total) |> \n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    -0.87 \n    -0.65 \n    -0.38 \n    1.1 \n    -0.52 \n    1.48 \n    3.52 \n    -0.36 \n    -0.06 \n    -0.23 \n  \n  \n    12-14 h \n    -0.89 \n    -0.45 \n    -0.29 \n    1.24 \n    -0.06 \n    0.86 \n    1.1 \n    0 \n    0.32 \n    0.02 \n  \n  \n    15-17 h \n    -0.6 \n    -0.17 \n    -0.25 \n    0.33 \n    0.48 \n    0.14 \n    -0.44 \n    0.16 \n    0.53 \n    0.34 \n  \n  \n    18-20 h \n    0.06 \n    0.35 \n    0.05 \n    -0.55 \n    0.77 \n    -0.61 \n    -0.87 \n    -0.42 \n    0.11 \n    0.57 \n  \n  \n    21-29 h \n    0.26 \n    0.31 \n    0.06 \n    -0.76 \n    0.49 \n    -0.86 \n    -0.92 \n    -0.48 \n    0.1 \n    0.89 \n  \n  \n    30-39 h \n    0.2 \n    0.2 \n    0.15 \n    -0.77 \n    0.42 \n    -0.87 \n    -0.8 \n    -0.69 \n    0.96 \n    0.47 \n  \n  \n    40-49 h \n    0 \n    -0.02 \n    0.72 \n    -0.61 \n    0.2 \n    -0.9 \n    -0.87 \n    -0.76 \n    1.51 \n    0.12 \n  \n  \n    50-64 h \n    -0.32 \n    0.11 \n    1.14 \n    -0.65 \n    0.23 \n    -0.7 \n    -0.94 \n    -0.84 \n    2.07 \n    0.01 \n  \n  \n    64+ h \n    -0.61 \n    -0.2 \n    2.19 \n    -0.71 \n    0.58 \n    -0.14 \n    -0.74 \n    -0.74 \n    1.83 \n    -0.33 \n  \n  \n    01-12 m \n    -0.65 \n    -0.69 \n    -0.27 \n    1.13 \n    -0.59 \n    1.39 \n    1.07 \n    1.37 \n    -0.78 \n    -0.73 \n  \n  \n    12-14 m \n    -0.46 \n    -0.28 \n    -0.37 \n    0.5 \n    -0.41 \n    0.9 \n    -0.18 \n    2.65 \n    -0.81 \n    -0.71 \n  \n  \n    15-17 m \n    0.49 \n    0.18 \n    -0.54 \n    -0.45 \n    -0.31 \n    -0.11 \n    -0.84 \n    1.83 \n    -0.92 \n    -0.49 \n  \n  \n    18-20 m \n    1.1 \n    0.71 \n    -0.08 \n    -0.83 \n    -0.32 \n    -0.8 \n    -0.82 \n    0.06 \n    -0.8 \n    -0.27 \n  \n  \n    21-29 m \n    1.58 \n    0.49 \n    -0.27 \n    -0.87 \n    -0.41 \n    -0.88 \n    -0.9 \n    -0.35 \n    -0.81 \n    -0.22 \n  \n  \n    30-39 m \n    1.64 \n    0.43 \n    -0.2 \n    -0.86 \n    -0.5 \n    -0.87 \n    -0.7 \n    -0.62 \n    -0.72 \n    -0.37 \n  \n  \n    40-49 m \n    1.39 \n    0.56 \n    0.09 \n    -0.79 \n    -0.37 \n    -0.84 \n    -0.86 \n    -0.7 \n    -0.67 \n    -0.31 \n  \n  \n    50-64 m \n    0.82 \n    0.56 \n    1.02 \n    -0.81 \n    -0.06 \n    -0.7 \n    -0.76 \n    -0.71 \n    -0.62 \n    -0.46 \n  \n  \n    64+ m \n    0.12 \n    0.36 \n    2.51 \n    -0.84 \n    0.26 \n    -0.04 \n    -0.85 \n    -0.77 \n    -0.79 \n    -0.2 \n  \n\n\n\n\n\n\nCompara tus conclusiones del mapa de correspondencias con esta información de los residuales\n\nNota adicionalmente que el ordenamiento de las categorías en la primera dimensión del mapa de correspondencias ayuda a interpretar:\n\n\nCódigo\nres_hurto_tbl |> select(\"grupo\", \"escritura\", \"juguetes\", \"dulces\", \"joyería\",\n                         \"herramientas\", \"otros\", \"libros\", \"tabaco\", \"accesorios\", \n                         \"ropa\") |> \n    mutate(across(where(is.numeric), \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    escritura \n    juguetes \n    dulces \n    joyería \n    herramientas \n    otros \n    libros \n    tabaco \n    accesorios \n    ropa \n  \n \n\n  \n    01-12 h \n    1.1 \n    3.52 \n    1.48 \n    -0.36 \n    -0.06 \n    -0.23 \n    -0.52 \n    -0.38 \n    -0.65 \n    -0.87 \n  \n  \n    12-14 h \n    1.24 \n    1.1 \n    0.86 \n    0 \n    0.32 \n    0.02 \n    -0.06 \n    -0.29 \n    -0.45 \n    -0.89 \n  \n  \n    15-17 h \n    0.33 \n    -0.44 \n    0.14 \n    0.16 \n    0.53 \n    0.34 \n    0.48 \n    -0.25 \n    -0.17 \n    -0.6 \n  \n  \n    18-20 h \n    -0.55 \n    -0.87 \n    -0.61 \n    -0.42 \n    0.11 \n    0.57 \n    0.77 \n    0.05 \n    0.35 \n    0.06 \n  \n  \n    21-29 h \n    -0.76 \n    -0.92 \n    -0.86 \n    -0.48 \n    0.1 \n    0.89 \n    0.49 \n    0.06 \n    0.31 \n    0.26 \n  \n  \n    30-39 h \n    -0.77 \n    -0.8 \n    -0.87 \n    -0.69 \n    0.96 \n    0.47 \n    0.42 \n    0.15 \n    0.2 \n    0.2 \n  \n  \n    40-49 h \n    -0.61 \n    -0.87 \n    -0.9 \n    -0.76 \n    1.51 \n    0.12 \n    0.2 \n    0.72 \n    -0.02 \n    0 \n  \n  \n    50-64 h \n    -0.65 \n    -0.94 \n    -0.7 \n    -0.84 \n    2.07 \n    0.01 \n    0.23 \n    1.14 \n    0.11 \n    -0.32 \n  \n  \n    64+ h \n    -0.71 \n    -0.74 \n    -0.14 \n    -0.74 \n    1.83 \n    -0.33 \n    0.58 \n    2.19 \n    -0.2 \n    -0.61 \n  \n  \n    01-12 m \n    1.13 \n    1.07 \n    1.39 \n    1.37 \n    -0.78 \n    -0.73 \n    -0.59 \n    -0.27 \n    -0.69 \n    -0.65 \n  \n  \n    12-14 m \n    0.5 \n    -0.18 \n    0.9 \n    2.65 \n    -0.81 \n    -0.71 \n    -0.41 \n    -0.37 \n    -0.28 \n    -0.46 \n  \n  \n    15-17 m \n    -0.45 \n    -0.84 \n    -0.11 \n    1.83 \n    -0.92 \n    -0.49 \n    -0.31 \n    -0.54 \n    0.18 \n    0.49 \n  \n  \n    18-20 m \n    -0.83 \n    -0.82 \n    -0.8 \n    0.06 \n    -0.8 \n    -0.27 \n    -0.32 \n    -0.08 \n    0.71 \n    1.1 \n  \n  \n    21-29 m \n    -0.87 \n    -0.9 \n    -0.88 \n    -0.35 \n    -0.81 \n    -0.22 \n    -0.41 \n    -0.27 \n    0.49 \n    1.58 \n  \n  \n    30-39 m \n    -0.86 \n    -0.7 \n    -0.87 \n    -0.62 \n    -0.72 \n    -0.37 \n    -0.5 \n    -0.2 \n    0.43 \n    1.64 \n  \n  \n    40-49 m \n    -0.79 \n    -0.86 \n    -0.84 \n    -0.7 \n    -0.67 \n    -0.31 \n    -0.37 \n    0.09 \n    0.56 \n    1.39 \n  \n  \n    50-64 m \n    -0.81 \n    -0.76 \n    -0.7 \n    -0.71 \n    -0.62 \n    -0.46 \n    -0.06 \n    1.02 \n    0.56 \n    0.82 \n  \n  \n    64+ m \n    -0.84 \n    -0.85 \n    -0.04 \n    -0.77 \n    -0.79 \n    -0.2 \n    0.26 \n    2.51 \n    0.36 \n    0.12 \n  \n\n\n\n\n\n\nOtras dimensiones\nEn el caso anterior, la calidad de la representación es cercana al 80%. Existen algunas desviaciones que la posiblemente la gŕafica no explica del todo, y algunas proyecciones son aproximadas. Podemos ver cómo se ven otras dimensiones de este análisis para entender desviaciones adicionales:\n\n\nCódigo\nplot(corr_hurto, dim = c(1, 3), map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\n\n\n\nIzenman, A. J. 2009. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts en Statistics. Springer New York. https://books.google.com.mx/books?id=1CuznRORa3EC.\n\n\nLê, Sébastien, Julie Josse, y François Husson. 2008. «FactoMineR: An R Package for Multivariate Analysis». Journal of Statistical Software, Articles 25 (1): 1-18. https://doi.org/10.18637/jss.v025.i01."
  },
  {
    "objectID": "intro-inferencia.html",
    "href": "intro-inferencia.html",
    "title": "Inferencia estadística",
    "section": "",
    "text": "A grandes rasgos, en la inferencia estadística buscamos hacer afirmaciones acerca de una colección de datos de la cual sólo tenemos información parcial.\nNos concentraremos en dos de las situaciones más comunes:\nPor ejemplo, consideremos esta población de 15 personas:\nPara una muestra de ellos tenemos información acerca de su estatura y peso. ¿Qué podríamos decir acerca de la estatura y el peso de la población general?\nEn este caso, la situación se ve como sigue. Imaginemos que tenemos 15 personas con dolor de cabeza, y obtenemos los siguientes datos:\nNuestra pregunta en este caso es del tipo: ¿ayuda la aspirina a reducir el dolor de cabeza en esta población? ¿qué tanto ayuda? Igualmente, tenemos información incompleta, en el sentido de que sólo observamos un resultado potencial de cada persona, dependiendo de si tomó aspirina o no. Si supiéramos los dos resultados potenciales de cada persona entonces podríamos contestar la pregunta sin dificultad."
  },
  {
    "objectID": "intro-inferencia.html#proceso-de-selección-o-asignación",
    "href": "intro-inferencia.html#proceso-de-selección-o-asignación",
    "title": "Inferencia estadística",
    "section": "Proceso de selección o asignación",
    "text": "Proceso de selección o asignación\nLas preguntas que planteamos arriba son difíciles de contestar cuando no conocemos bien el proceso de selección de individuos en la muestra o no conocemos el proceso de asignación de la aspirina.\nPor ejemplo, llegaríamos a conclusiones muy distintas si nos dijeran que:\n\nEscogimos las 5 personas que usan ropa talla chica.\nEscogimos las 5 personas que llegaron primero en una carrera de 100 metros.\nEscogimos las personas cuyo día de nacimiento era más bajo.\n\nO en el ejemplo de la aspirina,\n\nSólo dimos aspirinas a las personas que reportaron un nivel de dolor de cabeza muy alto.\nSolo dimos aspirina a las personas que llegaron primero en una carrera de 100 metros.\nDimos una aspirina exclusivamente a las personas cuyo día de nacimiento es par.\n\n\n\n\n\n\n\nTip\n\n\n\nDiscute qué conclusiones podrías llegar en cada uno de estos escenarios. Puedes usar diagramas como los de la sección anterior para explicar tus respuestas.\n\n\nLos casos 1 y 2 en ambas poblaciones son en general difíciles de resolver adecuadamente, y explicaremos con más ejemplos. Adicionalmente, es también más difícil cuantificar el nivel de incertidumbre de nuestras respuestas, pues dependen de muchos detalles del proceso de selección o asignación.\n\n\n\n\n\n\nProceso generador de datos y selección/asignación\n\n\n\nCuando el proceso de selección de observaciones o asignación tiene relaciones complicadas con las cantidades de interés, puede ser muy difícil dar respuesta a preguntas inferenciales de manera adecuada, y es importante entender el proceso que genera los datos, muchas veces a un nivel muy detallado."
  },
  {
    "objectID": "intro-pruebas-hipotesis.html",
    "href": "intro-pruebas-hipotesis.html",
    "title": "3  Introducción a pruebas de hipótesis",
    "section": "",
    "text": "Las primeras técnicas que veremos intentan contestar la siguiente pregunta:\n\nSi observamos cierto patrón en los datos, ¿cómo podemos cuantificar la evidencia de que es un patrón notable y no sólo debido a fluctuaciones en los datos particulares que tenemos? ¿Cómo sabemos que no estamos sobreinterpretando esas fluctuaciones?\n\nPor ejemplo:\n\nUn sistema tiene cierto comportamiento “usual” para el cual tenemos datos históricos. El sistema presenta fluctuaciones en el tiempo.\nObservamos la última salida de nuestro sistema. Naturalmente, tiene fluctuaciones. ¿Esas fluctuaciones son consistentes con la operación usual del sistema? ¿Existe evidencia para pensar que algo en el sistema cambió?"
  },
  {
    "objectID": "intro-pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "href": "intro-pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "title": "3  Introducción a pruebas de hipótesis",
    "section": "3.2 Comparación con poblaciones de referencia",
    "text": "3.2 Comparación con poblaciones de referencia\nEn las prueba de hipótesis, tratamos de construir distribuciones de referencia para comparar resultados que obtengamos con un “estándar” de variación, y juzgar si nuestros resultados son consistentes con la referencia o no (Box et al. (1978)).\nEn algunos casos, ese estándar de variación puede construirse con datos históricos.\n\nEjemplo\nSupongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado “normal” durante un tiempo considerable, y cuando observamos nuevos datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando de manera similar.\nDigamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo mostramos cinco ejemplos donde el sistema opera normalmente, que muestra la variabilidad en el tiempo en ventanas cortas del sistema.\nAhora suponemos que obtenemos una nueva ventana de datos. ¿Hay evidencia en contra de que el sistema sigue funcionando de manera similar?\nNuestra primera inclinación debe ser comparar: en este caso, compararamos ventanas históricas con nuestra nueva serie:\n\n\n\n\n\nCódigo\n# usamos datos simulados para este ejemplo\nset.seed(8812)\nhistoricos <- simular_serie(2000)\n\n\n\n\n\n\n\n¿Vemos algo diferente en los datos nuevos (el panel de color diferente)?\nIndpendientemente de la respuesta, vemos que hacer este análisis de manera tan simple no es siempre útil: seguramente podemos encontrar maneras en que la nueva muestra (4) es diferente a muestras históricas. Por ejemplo, ninguna de muestras tiene un “forma de montaña” tan clara.\nNos preguntamos si no estamos sobreinterpretando variaciones que son parte normal del proceso.\nPodemos hacer un mejor análisis si extraemos varias muestras del comportamiento usual del sistema, graficamos junto a la nueva muestra, y revolvemos las gráficas para que no sepamos cuál es cuál. Entonces la pregunta es:\n\n¿Podemos detectar donde están los datos nuevos?\n\nEsta se llama una prueba de lineup, o una prueba de ronda de sospechosos (Wickham et al. (2010)). En la siguiente gráfica, en uno de los páneles están los datos recientemente observados. ¿Hay algo en los datos que distinga al patrón nuevo?\n\n\nCódigo\n# nuevos datos\nobs <- simular_serie(500, x_inicial = last(obs$obs))\n# muestrear datos históricos\nprueba_tbl <- muestrear_ventanas(historicos, obs[1:20, ], n_ventana = 20)\n# gráfica de pequeños múltiplos\nggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + \n     facet_wrap(~rep, nrow = 4) + scale_y_log10()\n\n\n\n\n\nEjercicio: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Qué implica que la gráfica que escogamos como “más diferente” no sean los datos nuevos? ¿Qué implica que le “atinemos” a la gráfica de los datos nuevos?\nAhora observamos al sistema en otro momento y repetimos la comparación. En el siguiente caso obtenemos:\n\n\n\n\n\nAunque es imposible estar seguros de que ha ocurrido un cambio, la diferencia de una de las series es muy considerable. Si identificamos los datos correctos, la probabilidad de que hayamos señalado la nueva serie “sobreinterpretando” fluctuaciones en un proceso que sigue comportándose normalente es 0.05 - relativamente baja. Detectar los datos diferentes es evidencia en contra de que el sistema sigue funcionando de la misma manera que antes.\nObservaciones y terminología:\n\nLlamamos hipótesis nula a la hipótesis de que los nuevos datos son producidos bajo las mismas condiciones que los datos de control o de referencia.\nSi no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia en contra de la hipótesis nula.\nSi escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia en contra de la hipótesis nula.\n\n¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos?\n\nCuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. Decimos que el nivel de significancia de la prueba es la probabilidad de seleccionar a los datos correctos cuando la hipótesis nula es cierta (el sistema no ha cambiado). En el caso de 20 paneles, la significancia es de 1/20 = 0.05.\nSi acertamos, y la diferencia es más notoria y fue muy fácil detectar la gráfica diferente (pues sus diferencias son más extremas), esto también sugiere más evidencia en contra de la hipótesis nula.\nFinalmente, esta prueba rara vez (o nunca) nos da seguridad completa acerca de ninguna conclusión, aún cuando hiciéramos muchos páneles.\n\n\n\n\n\n\n\nNota\n\n\n\nPruebas de hipótesis\nEn su escencia, en las pruebas de hipótesis comparamos los datos de interés con una colección de datos de referencia. Nuestro interés es decidir si los datos de interés son consistentes con los datos de referencia. Veremos que hay distintas maneras de definir “consistencia”:\n\nPodemos preguntarnos si el proceso generador de los datos de interés es consistente con el proceso generador de los datos de referencia\nPodemos también preguntarnos por aspectos particulares del proceso generador, por ejemplo, si los datos parecen tener valores excepcionalmente grandes o excepcionalmente chicos, si tienen más dispersión que los datos de referencia, etc."
  },
  {
    "objectID": "intro-pruebas-hipotesis.html#cuantificando-la-distribución-de-referencia",
    "href": "intro-pruebas-hipotesis.html#cuantificando-la-distribución-de-referencia",
    "title": "3  Introducción a pruebas de hipótesis",
    "section": "3.3 Cuantificando la distribución de referencia",
    "text": "3.3 Cuantificando la distribución de referencia\nEn el ejemplo anterior estamos intentando dectectar cualquier desviación del comportamiento normal del sistema de una manera rigurosa. Podemos hacerlo más cuantitativo creando estadísticas resumen de las series. Por ejemplo, podríamos utilizar la variabilidad que tienen las series alrededor de su nivel general.\n\n\nCódigo\nsd_simple <- function(x){\n  # suavizamiento exponencial\n  mod <- HoltWinters(x, beta=FALSE, gamma=FALSE)\n  suavizamiento <- fitted(mod)[,1] |> as.numeric()\n  sd(x[-1] - suavizamiento)\n}\nreferencia_tbl <- muestrear_ventanas(historicos, n_ventana = 1500) |> \n  pluck(\"lineup\") |> \n  group_by(rep) |> \n  summarise(est_prueba = sd_simple(obs))\nreferencia_tbl |> head()\n\n\n# A tibble: 6 × 2\n    rep est_prueba\n  <int>      <dbl>\n1     1      1.08 \n2     2      0.843\n3     3      0.934\n4     4      0.830\n5     5      0.799\n6     6      1.20 \n\n\n\n\nCódigo\nggplot(referencia_tbl, aes(x = est_prueba)) + \n  geom_histogram() +\n  geom_vline(xintercept = sd_simple(observados$obs), colour = \"red\") +\n  annotate(\"text\", x = 2.5, y = 30, \n     label = \"diferencia observada\", colour = \"red\", angle = 90)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nY confirmamos que en efecto el valor observado (línea roja) es uno muy extremo, y poco consistente con el comportamiento usual del sistema.\nEl valor p (de una cola) Se define como la probabilidad de observar este resultado, o uno más grande, suponiendo que el sistema está funcionando usualmente, y en este caso lo calculamos como:\n\n\nCódigo\ndiferencia_obs <- sd_simple(observados$obs)\nreferencia_2 <- bind_rows(referencia_tbl,\n  tibble(rep = 0, est_prueba = diferencia_obs))\nreferencia_2 |> \n  mutate(mayor_obs = est_prueba > diferencia_obs) |> \n  summarise(valor_p = mean(mayor_obs)) |> \n  kable() |> kable_paper(full_width = FALSE)\n\n\n\n\n \n  \n    valor_p \n  \n \n\n  \n    0.0019987 \n  \n\n\n\n\n\nQue cuantifica que es muy poco probable observar el sistema en el estado actual si fuera cierto que no ha sufrido cambios.\n\n\n\n\n\n\nEstadísticas de prueba\n\n\n\nUna estadística de prueba es un resumen de datos, a partir del cual construimos una distribución de referencia bajo los supuestos de la hipótesis nula. Distintas estadísticas miden distintos aspectos de las diferencias que puede haber entre los datos de prueba y los de referencia.\n\n\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, y Andreas Buja. 2010. «Graphical inference for infovis». IEEE Transactions on Visualization and Computer Graphics 16 (6): 973-79."
  },
  {
    "objectID": "pruebas-hipotesis-aleatorizacion.html",
    "href": "pruebas-hipotesis-aleatorizacion.html",
    "title": "4  Aleatorización en inferencia causal",
    "section": "",
    "text": "Empezaremos con ejemplos de inferencia causal y consideramos el ejemplo de (Box et al. (1978)).\nSupongamos que un jardinero aficionado tiene un fertilizante, y quiere ver si tiene un efecto agregarlo a sus plantas. Nuestro jardinero solamente tiene una línea donde caben 20 plantas.\nCuando las plantas crezcan, observaremos variabilidad, independientemente de si se usa fertilizante o no. Esta variabilidad proviene de muchos factores ambientales, variaciones en las condiciones del suelo, insectos, etc. Interpretar los resultados correctamente implica necesariamente cuantificar esa variabilidad.\nEl jardinero escogió algunos lugares dónde poner el fertilizante y dónde no. El resultado que obtuvo es:\nPara decidir qué tan bueno es el nuevo fertilizante, el jardinero decide usar la siguiente estadística \\(D\\) (Kolmogorov-Smirnov):\nUn valor de \\(D\\) grande sugiere que el fertilizante tiene algún efecto. En nuestro experimento, obtuvimos:\nLa diferencia más grande en estas curvas es:\nParece ser que las plantas con fertilizante tuvieron mejores resultados (la distribución de las fertilizadas está recorrida hacia la derecha). El problema aquí es que las plantas tienen variabilidad, y la diferencia que observamos, que no es muy grande, podría deberse a esa variabilidad, y no tener qué ver con el fertilizante.\n¿Cómo juzgamos si este resultado puede atribuirse a variabilidad en el crecimiento de cada planta?\nReescribimos nuestros datos como:\nNótese que escribimos en cada caso el dato observado y el no observado.\nAhora supongamos que el tratamiento no tiene ningún efecto sobre el crecimiento de las plantas. Bajo esta hipótesis, podemos rellenar los valores no observados: en cada caso, el dato faltante lo conocemos, y es igual al valor observado para cada planta.\nBajo esta hipótesis, podemos calcular qué pasaría si hubiéramos escogido distintas plantas para el tratamiento de fertilizante.\nSimplemente consideramos todas las permutaciones de la columna \\(T\\), y vemos el valor que tiene nuestra estadística \\(D\\) en cada caso. La distribución resultante es construida bajo la hipótesis de que el fertilizante no tiene efecto, y muestra la variabilidad de nuestra estadística bajo esta hipótesis.\nUsualmente, en lugar de calcular todas las permutaciones, simulamos un número grande de ellas (de mil a 10 mil, por ejemplo). Abajo mostramos dos ejemplos:\nY aquí vemos todos los posibles resultados bajo distintas asignaciones del fertilizante, bajo la hipótesis del que el fertilizante no tiene ningún efecto. Adicionalmente, marcamos el valor que observamos en el experimento.\nComo vemos, el resultado que obtuvimos está en el lado alto de la destribución. Parece ser que el fertilizante tiene algún efecto.\nSin embargo, hay un hueco en nuestro argumento. Por ejemplo,\nSi esto es cierto, entonces nuestro argumento no es válido. Quizá la diferencia es grande porque el fertilizante se aplicó a plantas con más potencial desde un principio.\nUna solución simple es la siguiente:\nLlamamos a este valor-p de la prueba, y cuanto más chico es, mayor evidencia tenemos contra la hipótesis nula."
  },
  {
    "objectID": "pruebas-hipotesis-aleatorizacion.html#pruebas-de-hipótesis-visuales",
    "href": "pruebas-hipotesis-aleatorizacion.html#pruebas-de-hipótesis-visuales",
    "title": "4  Aleatorización en inferencia causal",
    "section": "4.1 Pruebas de hipótesis visuales",
    "text": "4.1 Pruebas de hipótesis visuales\nOtra idea es hacer una prueba visual. Primero graficamos varias replicaciones de datos nulos, es decir, datos en donde hemos permutado al azar la columna de tratamiento. Entrenamos nuestra percepción a variaciones consistentes con la hipótesis nula:\n\n\nCódigo\nlibrary(nullabor)\n\nset.seed(83814)\n# comenzamos con rorsach, viendo datos nulos\nreps_rorschach <- rorschach(method = null_permute(\"T\"), n = 20, res_obs) |> \n  as_tibble()\nggplot(reps_rorschach, aes(sample = y, colour = T, group = T)) +\n  stat_qq(distribution = stats::qunif) +\n  stat_qq_line(distribution = stats::qunif, fullrange = TRUE) +\n  facet_wrap(~ .sample) + theme(strip.background =element_rect(fill=\"gray85\"))\n\n\n\n\n\nY ahora hacemos nuestra prueba. En la siguiente gráfica 19 cajas tienen datos nulos, y una caja tiene los datos verdaderos.\n¿Puedes identificar los datos verdaderos?\n\n\nCódigo\nreps <- lineup(method = null_permute(\"T\"), n = 20, res_obs) |> \n  as_tibble()\n\n\ndecrypt(\"IDHt lN8N Ed 2VFE8EVd 3b\")\n\n\nCódigo\nggplot(reps, aes(sample = y, colour = T, group = T)) +\n  stat_qq(distribution = stats::qunif) +\n  stat_qq_line(distribution = stats::qunif, fullrange = TRUE) +\n  facet_wrap(~ .sample) + theme(strip.background =element_rect(fill=\"gray85\"))\n\n\n\n\n\n\nEsta prueba es exacta: la probabilidad de identificar los datos correctamente cuando el fertilizante no tiene efecto es menor o igual a 0.05 (valor \\(p\\)). Esta es la probabilidad de equivocadamente declarar que tenemos evidencia de que la hipótesis nula no se cumple.\n\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#prueba-de-permutaciones-para-proporciones",
    "href": "pruebas-hipotesis-2.html#prueba-de-permutaciones-para-proporciones",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Prueba de permutaciones para proporciones",
    "text": "Prueba de permutaciones para proporciones\nVeremos otro ejemplo donde podemos hacer más concreta la idea de distribución nula o de referencia usando pruebas de permutaciones. Supongamos que con nuestra muestra de tomadores de té, queremos probar la siguiente hipótesis nula:\n\nLos tomadores de té en bolsas exclusivamente usan azúcar más a tasas simillares que los tomadores de té suelto (que pueden o no también tomar té en bolsita).\n\nLos datos que obtuvimos en nuestra encuesta, en conteos, son:\n\n\n\n\n\nCódigo\nte_azucar <- tea |> select(how, sugar) |> \n  mutate(how = ifelse(how == \"tea bag\", \"bolsa_exclusivo\", \"suelto o bolsa\"))\nte_azucar |> group_by(how, sugar) |> tally() |> \n  spread(how, n) |> \n  formatear_tabla()\n\n\n\n\n \n  \n    sugar \n    bolsa_exclusivo \n    suelto o bolsa \n  \n \n\n  \n    No.sugar \n    81 \n    74 \n  \n  \n    sugar \n    89 \n    56 \n  \n\n\n\n\n\nY en proporciones tenemos que:\n\n\n\n\n \n  \n    how \n    prop_azucar \n    n \n  \n \n\n  \n    bolsa_exclusivo \n    0.52 \n    170 \n  \n  \n    suelto o bolsa \n    0.43 \n    130 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEn este ejemplo, podemos usar una estádistica de prueba numérica, por ejemplo, la diferencia entre las dos proporciones:\n\\[p_1 - p_2\\].\n(tomadores de en bolsa solamente vs. suelto y bolsa). El proceso sería entonces:\n\nLa hipótesis nula es que los dos grupos tienen distribuciones iguales, que este caso quiere decir que en la población, tomadores de té solo en bolsa usan azúcar a las mismas tasas que tomadores de suelto o bolsas.\nBajo nuestra hipótesis nula (proporciones iguales), producimos una cantidad grande (por ejemplo 10 mil o más) de muestras permutando las etiquetas de los grupos.\nEvaluamos nuestra estadística de prueba en cada una de las muestras permutadas.\nEl conjunto de valores obtenidos nos da nuestra distribución de referencia (ya no estamos limitados a 20 replicaciones como en las pruebas gráficas).\nY la pregunta clave es: ¿el valor de la estadística en nuestra muestra es extrema en comparación a la distribución de referencia?\n\n\n\nCódigo\n# ESta función calcula la diferencia entre grupos de interés\ncalc_diferencia <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(how) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(how, prop_azucar) |> \n    mutate(diferencia_prop = bolsa_exclusivo - `suelto o bolsa`) |> pull(diferencia_prop)\n}\n# esta función hace permutaciones y calcula la diferencia para cada una\npermutaciones_est <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}}:= permutar({{variable}})) |> calc_diferencia()))\n  bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n}\n\n\nLa diferencia observada es:\n\n\nCódigo\ndif_obs <- calc_diferencia(te_azucar)\ndif_obs |> round(3)\n\n\n[1] 0.093\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\n\nCódigo\nvalores_ref <- permutaciones_est(te_azucar, how, calc_diferencia, n = 10000)\n\n\nY graficamos nuestros resultados (con un histograma y una gráfica de cuantiles, por ejemplo). la estadística evaluada un cada una de nuestras muestras permutadas:\n\n\nCódigo\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\n\nEste es el rango de fluctuación usual para nuestra estadística *bajo la hipótesis de que los dos grupos de tomadores de té consumen té a la misma tasa.\nEl valor que obtuvimos en nuestros datos es 0.0927602, que no es un valor extremo en la distribución de referencia que vimos arriba: esta muestra no aporta mucha evidencia en contra de que los grupos tienen distribuciones similares.\nPodemos graficar otra vez marcando el valor de referencia:\n\n\nCódigo\n# Función de distribución acumulada (inverso de función de cuantiles)\ndist_perm <- ecdf(valores_ref$diferencia)\n# Calculamos el percentil del valor observado\npercentil_obs <- dist_perm(dif_obs)\n\n\n\n\nCódigo\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\") +\n    geom_hline(yintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = 0.3, y = dif_obs - 0.05, label = \"diferencia observada\", colour = \"red\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \") +\n    geom_vline(xintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = dif_obs, y = 2000, label = percentil_obs,vjust = -0.2, colour = \"red\")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\n\nY vemos que es un valor algo (pero no muy) extremo en la distribución de referencia que vimos arriba: esta muestra no aporta una gran cantidad de evidencia en contra de que los grupos tienen distribuciones similares, que en este caso significa que los dos grupos usan azúcar a tasas similares.\n\nValor p\nNótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un valor p. Podemos calcular, por ejemplo:\n\nValor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la probabilidad de observar una diferencia tan extrema o más extrema de lo que observamos?\n\nConsiderando en este caso interpretamos extrema como que cae lejos de donde a mayoría de la distribución se concentra, podemos calcular el valor p como sigue. A partir de el valor observado, consideramos cuál dato es menor: la probabilidad bajo lo hipótesis nula de observar una diferencia mayor de a que observamos, o la probabilidad de observar una diferencia menor a la que observamos. Tomamos el mínimo y multiplicamos por dos (Hesterberg (2015)):\n\n\nCódigo\n2 * min(dist_perm(dif_obs), (1 - dist_perm(dif_obs)))\n\n\n[1] 0.0892\n\n\nEste valor p se considera como evidencia “moderada” en contra de la hipótesis nula. Valores p más chicos (observaciones más extremas en comparación con la referencia) aportan más evidencia en contra de la hipótesis de que los grupos de tomadores de té , y valores más grandes aportan menos evidencia."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#tomadores-de-té-2",
    "href": "pruebas-hipotesis-2.html#tomadores-de-té-2",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Tomadores de té 2",
    "text": "Tomadores de té 2\nAhora hacemos una prueba de permutaciones otro par de proporciones con el mismo método. La hipótesis nula ahora es:\n\nLos tomadores de té Earl Gray usan azúcar a una tasa similar a los tomadores de té negro\n\nLos datos que obtuvimos en nuestra encuesta, en conteos, son: ::: {.cell} ::: {.cell-output-display}\n\n\n \n  \n    sugar \n    black \n    Earl Grey \n  \n \n\n  \n    No.sugar \n    51 \n    84 \n  \n  \n    sugar \n    23 \n    109 \n  \n\n\n\n::: :::\nY en porcentajes tenemos que:\n\n\nCódigo\nprop_azucar <- te_azucar |> group_by(Tea, sugar) |> tally() |> \n  group_by(Tea) |> mutate(prop = 100 * n / sum(n), n = sum(n)) |> \n  filter(sugar == \"sugar\") |> select(Tea, prop_azucar = prop, n) |> \n  mutate('% usa azúcar' = round(prop_azucar)) |> select(-prop_azucar)\nprop_azucar |> formatear_tabla()\n\n\n\n\n \n  \n    Tea \n    n \n    % usa azúcar \n  \n \n\n  \n    black \n    74 \n    31 \n  \n  \n    Earl Grey \n    193 \n    56 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEscribimos la función que calcula diferencias para cada muestra:\n\n\nCódigo\ncalc_diferencia_2 <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(Tea) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(Tea, prop_azucar) |> \n    mutate(diferencia_prop = `Earl Grey` - black) |> pull(diferencia_prop)\n}\n\n\nLa diferencia observada es:\n\n\n[1] 0.254\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\n\nCódigo\nset.seed(2)\nvalores_ref <- permutaciones_est(te_azucar, Tea, calc_diferencia_2, n = 10000)\n\n\nY podemos graficar la distribución de referencia otra vez marcando el valor observado\n\n\n\n\n\n\n\n\nEn este caso, la evidencia es muy fuerte en contra de la hipótesis nula, pues el resultado que obtuvimos es muy extremo en relación a la distribución de referencia. El valor p es cercano a 0."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#ejemplo-tiempos-de-fusión",
    "href": "pruebas-hipotesis-2.html#ejemplo-tiempos-de-fusión",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Ejemplo: tiempos de fusión",
    "text": "Ejemplo: tiempos de fusión\nConsideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce con información verbal).\nPodemos usar una prueba de permutaciones, esta vez justificándola por el hecho de que los tratamientos se asignan al azar: si los tratamientos son indistinguibles, entonces las etiquetas de los grupos son solo etiquetas, y permutarlas daría muestras igualmente verosímiles.\nEn este caso, compararemos gráficas de cuantiles de los datos con los producidos por permutaciones:\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  n = col_double(),\n  time = col_double(),\n  nv.vv = col_character()\n)\n\n\ndecrypt(\"IDHt lN8N Ed 2VFE8EVd 33\")\n\n\n\n\n\nEjercicio: ¿Podemos identificar los datos? En general, muy frecuentemente las personas identifican los datos correctamente, lo que muestra evidencia considerable de que la instrucción verbal altera los tiempos de respuesta de los partipantes, y en este caso ayuda a reducir el tiempo de fusión de los estereogramas."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#ejemplo-tiempos-de-fusión-2",
    "href": "pruebas-hipotesis-2.html#ejemplo-tiempos-de-fusión-2",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Ejemplo: tiempos de fusión 2",
    "text": "Ejemplo: tiempos de fusión 2\nPodemos usar las pruebas de permutaciones para distintos de tipos de estadísticas: medianas, medias, comparar dispersión usando rangos intercuartiles o varianzas, etc.\nRegresamos a los tiempos de fusión. Podemos hacer una prueba de permutaciones para la diferencia de las medias o medianas, por ejemplo. En este ejemplo usaremos una medida de centralidad un poco diferente, como ilustración: el promedio de los cuartiles superior e inferior de las dos distribuciones. Usaremos el cociente de estas dos cantidades para medir su diferencia\n\n\nCódigo\nstat_fusion <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\ncalc_fusion <- function(stat_fusion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion(time)) |> \n      spread(nv.vv, est) |> mutate(dif = VV / NV ) |> pull(dif)\n  }\n  fun\n}\n\n\n\n\nCódigo\ncalc_cociente <- calc_fusion(stat_fusion)\ndif_obs <- calc_cociente(fusion)\n# permutar\nvalores_ref <- permutaciones_est(fusion, nv.vv, calc_cociente, n = 10000)\ndist_perm_nv <- ecdf(valores_ref$diferencia) \ncuantil_obs <- dist_perm_nv(dif_obs)\n\n\n\n\n\n\n\nY el valor p de dos colas es\n\n\nCódigo\ndist_perm_nv <- ecdf(valores_ref$diferencia)\n2 * min(dist_perm_nv(dif_obs), 1- dist_perm_nv(dif_obs))\n\n\n[1] 0.0354\n\n\nLo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas: la caja del diagrama de caja y brazos para el grupo VV está encogida por un factor menor a 1."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#ojo-otros-tipos-de-hipótesis-nulas",
    "href": "pruebas-hipotesis-2.html#ojo-otros-tipos-de-hipótesis-nulas",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Ojo: otros tipos de hipótesis nulas",
    "text": "Ojo: otros tipos de hipótesis nulas\nLa pruebas de permutaciones son más útiles cuando nuestra hipótesis nula se refiere que la distribución de los grupos son muy similares, o la independencia entre observaciones y grupo. Esto también aplica cuando queremos probar por ejemplo, que una variable numérica Y es independiente de X.\n\nHay algunas hipótesis que no se pueden probar con este método, como por ejemplo, las que se refieren a una sola muestra: ¿los datos son consistentes con que su media es igual a 5?\nAdicionalmente, en algunas ocasiones queremos probar aspectos más específicos de las diferencias: como ¿son iguales las medias o medianas de dos grupos de datos? ¿Tienen dispersión similar?\n\nLas pruebas de permutaciones no están tan perfectamente adaptadas a este problema, pues prueban todos los aspectos de las distribuciones que se comparan, aún cuando escogamos una estadística particular que pretende medir, por ejemplo, diferencia de medias. Eso quiere decir que podemos rechazar igualdad de medias, por ejemplo, cuando en realidad otra característica de las distribuciones es la que difiere mucho en las poblaciones\nEn algunas referencias (ver (chitim?), Efron y Tibshirani (1993)) se argumenta que de todas formas las pruebas de permutaciones son relativamente robustas a esta desadaptación. Un caso excepcional, por ejemplo, es cuando las poblaciones que comparamos resultan tener dispersión extremadamente distinta, y adicionalmente los tamaños de muestra de los grupos son muy desiguales (otra vez, ver ejemplos en (chitim?))."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#comparando-distribuciones",
    "href": "pruebas-hipotesis-2.html#comparando-distribuciones",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Comparando distribuciones",
    "text": "Comparando distribuciones\nAhora intentamos un ejemplo más típico.\nSupongamos tenemos muestras para tres grupos a, b y c, que quiere decir que dentro de cada grupo, el proceso e selección de los elementos se hace de manera al azar y de manera simétrica (por ejemplo cada elemento tiene a misma probabiidad de ser seleccionado, y las extracciones se hacen de manera independiente.)\nQueremos comparar las distribuciones de los datos obtenidos para cada grupo. Quizá la pregunta detrás de esta comparación es: el grupo de clientes b recibió una promoción especial. ¿Están gastando más? La medición que comparamos es el gasto de los clientes.\n\n\n\n\n\nEn la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias que observamos se deben variación muestral, por ejemplo.\nPodemos construir ahora una hipótesis nula, que establece que las observaciones provienen de una población similar:\n\nLas tres poblaciones (a, b, c) son prácticamente indistiguibles. En este caso, la variación que observamos se debería a que tenemos información incompleta.\n\nComo en el ejemplo anterior necesitamos construir o obtener una distribución de referencia para comparar qué tan extremos o diferentes son los datos que observamos. Esa distribución de referencia debería estar basada en el supuesto de que los grupos producen datos de distribuciones similares.\nSi tuvieramos mediciones similares históricas de estos tres grupos, quizá podríamos extraer datos de referencia y comparar, como hicimos en el ejempo anterior. Pero esto es menos común en este tipo de ejemplos."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#permutaciones-y-el-lineup",
    "href": "pruebas-hipotesis-2.html#permutaciones-y-el-lineup",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Permutaciones y el lineup",
    "text": "Permutaciones y el lineup\nPara abordar este problema podemos pensar en usar permutaciones de los grupos de la siguiente forma (Box et al. (1978), Hesterberg (2015)):\n\nSi los grupos producen datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información.\nPodríamos permutar al azar las etiquetas y observar nuevamente la gráfica de caja y brazos por grupos.\nSi la hipótesis nula es cierta (grupos idénticos), esta es una muestra tan verosímil como la que obtuvimos.\nAsí que podemos construir datos de referencia permutando las etiquetas de los grupos al azar, y observando la variación que ocurre.\nSi la hipótesis nula es cercana a ser cierta, no deberíamos de poder distinguir fácilmente los datos observados de los producidos con las permutaciones al azar.\n\nVamos a intentar esto, por ejemplo usando una gráfica de cuantiles simplificada. Hacemos un lineup, o una rueda de sospechosos (usamos el paquete H. Wickham, Chowdhury, y Cook (2012), ver Hadley Wickham et al. (2010)), donde 19 de los acusados son generados mediante permutaciones al azar de la variable del grupo, y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera\nUsamos una gráfica que muestra los cuantes 0.10, 0.50, 0.90:\n\n\nCódigo\nset.seed(88)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\n\ndecrypt(\"IDHt lN8N Ed 2VFE8EVd 3b\")\n\n\nCódigo\nreps_mezcla <- reps |>  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177))\ngrafica_cuantiles(reps_mezcla, grupo_1, x) + \n    facet_wrap(~.sample, ncol = 5) + ylab(\"x\") + \n    labs(caption = \"Mediana y percentiles 10% y 90%\")+ geom_point(aes(colour = grupo_1))\n\n\n`summarise()` has grouped output by 'grupo_1'. You can override using the\n`.groups` argument.\n\n\n\n\n\nY la pregunta que hacemos es podemos distinguir nuestra muestra entre todas las replicaciones producidas con permutaciones?\nEjercicio: ¿dónde están los datos observados? Según tu elección, ¿qué tan diferentes son los datos observados de los datos nulos?\nEn este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones similares y es factible que las diferencias que observamos se deban a variación muestral.\n\nSi la persona escoge los verdaderos datos, encontramos evidencia en contra de la hipótesis nula (los tres grupos son equivalentes). En algunos contextos, se dice que los datos son significativamente diferentes al nivel 0.05. Esto es evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo.\nSi la persona escoge uno de los datos permutados, no encontramos evidencia en contra de que los tres grupos producen datos con distribuciones similares."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#comparaciones-con-lineup-2",
    "href": "pruebas-hipotesis-2.html#comparaciones-con-lineup-2",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Comparaciones con lineup 2",
    "text": "Comparaciones con lineup 2\nRepitimos el ejemplo para otra muestra (en este ejemplo el proceso generador de datos es diferente para el grupo b):\n\n\n\n\n\nHacemos primero la prueba del lineup:\n\n\nCódigo\nset.seed(121)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\n\ndecrypt(\"IDHt lN8N Ed 2VFE8EVd 3A\")\n\n\nCódigo\ngrafica_cuantiles(reps |>  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), \n                             grupo_escondido, x) + facet_wrap(~.sample) + ylab(\"x\") +\n    coord_flip() + geom_point(aes(colour = grupo_escondido))\n\n\n`summarise()` has grouped output by 'grupo_escondido'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nPodemos distinguir más o menos claramente que está localizada en valores más altos y tiene mayor dispersión. En este caso, como en general podemos identificar los datos, obtenemos evidencia en contra de que los tres grupos tienen distribuciones iguales."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#la-crisis-de-replicabilidad",
    "href": "pruebas-hipotesis-2.html#la-crisis-de-replicabilidad",
    "title": "5  Más de pruebas de hipótesis",
    "section": "La “crisis de replicabilidad”",
    "text": "La “crisis de replicabilidad”\nRecientemente (Ioannidis (2005)) se ha reconocido en campos de ciencias sociales y medicina la crisis de replicabilidad. Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados posteriormente por otros investigadores. Por ejemplo:\n\nHacer poses poderosas produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas\nMostrar palabras relacionadas con “viejo” hacen que las personas caminen más lento (efectos de priming)\n\nEn todos estos casos, el argumento de la evidencia de estos efectos fue respaldada por una prueba de hipótesis nula con un valor p menor a 0.05. La razón es que ese es el estándar de publicación seguido por varias áreas y revistas. La tasa de no replicabilidad parece ser mucho más alta (al menos la mitad o más según algunas fuentes, como la señalada arriba) que lo sugeriría la tasa de falsos positivos (menos de 5%)\nEste problema de replicabilidad parece ser más frecuente cuando:\n\nSe trata de estudios de potencia baja: mediciones ruidosas y tamaños de muestra chicos.\nEl plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando se están investigando “fenómenos no estudiados antes”)\n\n¿A qué se atribuye esta crisis de replicabilidad?"
  },
  {
    "objectID": "pruebas-hipotesis-2.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "href": "pruebas-hipotesis-2.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "title": "5  Más de pruebas de hipótesis",
    "section": "El jardín de los senderos que se bifurcan",
    "text": "El jardín de los senderos que se bifurcan\nAunque haya algunos ejemplos de manipulaciones conscientes –e incluso, en menos casos, malintencionadas– para obtener resultados publicables o significativos (p-hacking), como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando estamos buscando las comparaciones correctas. Algunas pueden ser:\n\nTransformar los datos (tomar o no logaritmos, u otra transformación)\nEditar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)\nDistintas maneras de interpretar los criterios de inclusión de un estudio (por ejemplo, algunos participantes mostraron tener gripa, o revelaron que durmieron muy poco la noche anterior, etc. ¿los dejamos o los quitamos?)\n\nDado un conjunto de datos, las justificaciones de las decisiones que se toman en cada paso son razonables, pero con datos distintos las decisiones podrían ser diferentes. Este es el jardín de los senderos que se bifurcan Gelman, que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.\nEsto es exacerbado por:\n\nTamaños de muestra chicos y efectos “inestables” que se quieren medir (por ejemplo en sicología)\nEl hecho de que el criterio de publicación es obtener un valor p < 0.05, y la presión fuerte sobre los investigadores para producir resultados publicables (p < 0.05)\nEl que estudios o resultados similares que no obtuvieron valores \\(p\\) por debajo del umbral no son publicados o reportados.\n\nVer por ejemplo el comunicado de la ASA.\nOjo: esas presiones de publicación no sólo ocurre para investigadores en sicología. Cuando trabajamos en problemas de análisis de datos en problemas que son de importancia, es común que existan intereses de algunas partes o personas involucradas por algunos resultados u otros (por ejemplo, nuestros clientes de consultoría o clientes internos). Eso puede dañar nuestro trabajo como analistas, y el avance de nuestro equipo. Aunque esas presiones son inevitables, se vuelven manejables cuando hay una relación de confianza entre las partes involucradas."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "href": "pruebas-hipotesis-2.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Ejemplo: decisiones de análisis y valores p",
    "text": "Ejemplo: decisiones de análisis y valores p\nEn el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como ilustración. Ahora intentamos usar distintas mediciones de la diferencia entre los grupos, usando distintas medidas resumen y transformaciones (por ejemplo, con o sin logaritmo). Aquí hay unas 12 combinaciones distintas para hacer el análisis (multiplicadas por criterios de “aceptación de datos en la muestra”, que simulamos tomando una submuestra al azar):\n\n\nCódigo\ncalc_fusion <- function(stat_fusion, trans, comparacion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion({{ trans }}(time))) |> \n      spread(nv.vv, est) |> mutate(dif = {{ comparacion }}) |> pull(dif)\n  }\n  fun\n}\nvalor_p <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}} := permutar({{variable}})) |> calc_diferencia()))\n  perms <- bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n  perms_ecdf <- ecdf(perms$diferencia)\n  dif <- calc_diferencia(datos)\n  2 * min(perms_ecdf(dif), 1- perms_ecdf(dif))\n}\n\n\n\n\nCódigo\nset.seed(7272)\nmedia_cuartiles <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\n# nota: usar n=10000 o más, esto solo es para demostración:\ncalc_dif <- calc_fusion(mean, identity, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n\n[1] 0.072\n\n\nCódigo\ncalc_dif <- calc_fusion(mean, log, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n\n[1] 0.024\n\n\nCódigo\ncalc_dif <- calc_fusion(median, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n\n[1] 0.016\n\n\nCódigo\ncalc_dif <- calc_fusion(media_cuartiles, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n\n[1] 0.026\n\n\nSi existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces los valores p pueden tener poco significado."
  },
  {
    "objectID": "pruebas-hipotesis-2.html#alternativas-o-soluciones",
    "href": "pruebas-hipotesis-2.html#alternativas-o-soluciones",
    "title": "5  Más de pruebas de hipótesis",
    "section": "Alternativas o soluciones",
    "text": "Alternativas o soluciones\nEl primer punto importante es reconocer que la mayor parte de nuestro trabajo es exploratorio (recordemos el proceso complicado del análisis de datos de refinamiento de preguntas). En este tipo de trabajo, reportar valores p puede tener poco sentido, y mucho menos tiene sentido aceptar algo “verdadero” cuando pasa un umbral de significancia dado.\nNuestro interés principal al hacer análisis es expresar correctamente y de manera útil la incertidumbre asociada a las conclusiones o patrones que mostramos (asociada a variación muestral, por ejemplo) para que el proceso de toma de decisiones sea informado. Un resumen de un número (valor p, o el que sea) no puede ser tomado como criterio para tomar una decisión que generalmente es compleja. En la siguiente sección veremos cómo podemos mostrar parte de esa incertidumbre de manera más útil.\nPor otra parte, los estudios confirmatorios (donde se reportan valores p) también tienen un lugar. En áreas como la sicología, existen ahora movimientos fuertes en favor de la repetición de estudios prometedores pero donde hay sospecha de grados de libertad del investigador. Este movimiento sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos.\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York.\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86.\n\n\nIoannidis, John PA. 2005. «Why most published research findings are false». PLoS medicine 2 (8): e124.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, y Andreas Buja. 2010. «Graphical inference for infovis». IEEE Transactions on Visualization and Computer Graphics 16 (6): 973-79.\n\n\nWickham, H, NR Chowdhury, y D Cook. 2012. «nullabor: Tools for graphical inference». R package version 0.2 1: 213."
  },
  {
    "objectID": "estimacion-aleatorizacion.html",
    "href": "estimacion-aleatorizacion.html",
    "title": "6  Inferencia por intervalos",
    "section": "",
    "text": "En esta parte consideramos hacer inferencia a una población. Quiséramos decir algo acerca de la mediana de una población cuando solo tenemos información parcial.\nEn este ejemplo, queremos aprender acerca de los precios de casas en una región determinada (los precios están en miles de dólares).\nSupongamos que tenemos la siguiente datos, que sólo son parte de la población:\nSupongamos que queremos estimar el valor \\(m\\) que es la mediana poblacional. Este valor tiene la propiedad de que separa a los datos en dos grupos de tamaño igual.\nPara construir estos rangos haremos algunos cálculos básicos.\nEn primer lugar, si observáramos un valor \\(y\\) en la muestra, ¿cuál es la probabilidad de que caiga por arriba de la mediana? Si no sabemos cómo se extrajo la muestra, o se extrajo bajo un mecanismo complicado, esta pregunta es difícil de contestar.\nSin embargo, si este valor se extrajo tomando un valor al azar entre todos los de la población, entonces sabemos que la probabilidad es\n\\[P(y <= m) = 0.5\\]\nCon esta idea básica, podríamos por ejemplo calcular: ¿cuál es la probabilidad, de que la mediana esté entre los valores el mínimo y el máximo de la muestra?\nDenotemos por \\(y_1,y_2,\\ldots, y_n\\) la muestra observada. Denotaremos a la muestra ordenada del valor más chico al valor más grande como\n\\[y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\]\nNuestra primera pregunta entonces es:\nRespuesta: La probabilidad de que la mediana esté por debajo del mínimo \\(y_{(1)}\\) de la muestra es \\((1/2)^n\\), pues todos los valores de la muestra tienen que caer por arriba de la mediana. La probabilidad de que la mediana esté por arriba del máximo \\(y_{(1)}\\) es también \\((1/2)^n\\). Así que la probabilidad de el verdadero valor de la mediana esté en este intervalo es de $ 2(1/2)^n$\nEsta es la probabilidad de tirar solamente soles o solamente águilas en \\(n\\) tiradas de una moneda, Si \\(X\\) denota el número de soles obtenidos en \\(n\\) tiradas de dados, buscamos calcular la probabilidad de que la mediana esté en intervalo como\n\\[1 - P(X > n - 1) - P(X \\leq 0) = 1 - 2(1/2)^n\\]\nPrácticamente, es casi seguro que este intervalo contenga a la media verdadero. Esto no es muy informativo. Probemos ahora con \\([y_{(2)}, y_{(n-1)}]\\). Por un argumento similar, la probabilidad de la mediana verdadera esté en este intervalo es:\nEsto también es altamente probable. Sin embargo, la probabilidad de la mediana poblacional esté entre \\([y_{6}, y_{15}]\\) es\nAsí, este intervalo tiene probabilidad de casi 90% de probabilidad de contener al verdadero valor:\nAhora extraemos el intervalo correspondiente:\nY este es un intervalo de confianza de cerca de 90% para la mediana de la población. Cuando tomamos muestras más grandes, podemos obtener mejores precisiones. Por ejemplo, para una muestra de 151 casas, usaríamos\nTomamos una muestra y extraemos el intervalo:\nEsto intervalos tienen la garantía inferencial, y no es necesario hacer ningún supuesto, excepto en que la muestra se escoge al azar."
  },
  {
    "objectID": "estimacion-aleatorizacion.html#remuestreo",
    "href": "estimacion-aleatorizacion.html#remuestreo",
    "title": "6  Inferencia por intervalos",
    "section": "6.1 Remuestreo",
    "text": "6.1 Remuestreo\nPodemos pensar en este procedimiento de una manera diferente.\nSi la muestra completa se seleccionó al azar, entonces, si escogemos un valor no observado en la muestra, tenemos que \\(y\\) tiene la misma probabilidad de caer en cualquiera de los intervalos\n\\[I_1 = (0, y_1), I_2 = [y_1, y_2), I_3 = [y_2, y_3),\\ldots, I_{n} = [y_{n}, \\infty ]\\]\nLa razón es que si consideramos la muestra original con el nuevo dato agregado tomado al azar, todos los ordenamientos de \\(y_1, y_2, \\ldots, y_n, y_{n+1}\\) son igualmente probables: esto implica que es igualmente probable que \\(y_{n+1}\\) caiga en cualquiera de estos intervalos (probabilidad \\(1/{n+1}\\)).\n\nPodemos imputar nuestra población entera tomando escogiendo para cada valor no observado un intervalo,\nPodemos entonces muestrear de esta población otra vez para obtener una muestra nueva.\nRepetimos los dos anteriores una gran cantidad de veces para ver todos los posibles valores de medianas que podemos obtener.\n\nCada muestra es una sucesión de intervalos. Acumulamos para obtener una distribución sobre estos intervalos. La probabilidad de uqe la mediana salga en alguno de los intervalos extremos es igual a\n\\[(1/n)^{n/2},\\]\nque es un número del orden de \\[10^{-5}\\], que es chico aún cuando \\(n=10\\). Para valores más grandes, esta probabilidad es astronómicamente chica. Como el intervalo final consistirá de datos observados en nuestra muestra, en el resto de los intervalos no importa donde están los verdaderos poblacionales, y nos basta con saber si están o no en cada intervalo.\nEste proceso se llama remuestreo, y lo hacemos a continuación\n\n\nCódigo\nremuestrear <- function(muestra){\n  slice_sample(muestra, prop = 1, replace = TRUE) |> \n  summarise(mediana = median(precio_miles))\n}\nremuestras <- map_df(1:10000, ~ remuestrear(muestra))\n\n\n\n\nCódigo\nggplot(remuestras, aes(x = mediana)) +\n  geom_histogram(breaks = muestra |> pull(precio_miles)) +\n  geom_rug()\n\n\n\n\n\n\n\nCódigo\nquantile(remuestras$mediana, c(0.051, 0.948), type = 8)\n\n\n 5.1% 94.8% \n  146   235 \n\n\ny obtenemos un resultado idéntico al argumento de arriba. La cobertura es prácticamente exacta, y no afecta al resultado lo que sucede en las colas de la distribución, gracias a que estamos usando la mediana."
  },
  {
    "objectID": "estimacion-aleatorizacion.html#remuestreo-para-otras-estadísticas",
    "href": "estimacion-aleatorizacion.html#remuestreo-para-otras-estadísticas",
    "title": "6  Inferencia por intervalos",
    "section": "6.2 Remuestreo para otras estadísticas",
    "text": "6.2 Remuestreo para otras estadísticas\nPodemos repetir este método para otras estadísticas como la media. Sin embargo, en este caso no podemos dar garantías independientes de la distribución poblacional. La razón es que las colas pueden ser muy extremas.\n\nremuestrear_media <- function(muestra){\n  slice_sample(muestra, prop = 1, replace = TRUE) |> \n  summarise(media = mean(precio_miles))\n}\nremuestras_media <- map_df(1:10000, ~ remuestrear_media(muestra))\n\n\n\nCódigo\nggplot(remuestras_media) +\n  geom_histogram(aes(x = media), binwidth = 1.5) +\n  geom_rug(data = muestra, aes(x = precio_miles))\n\n\n\n\n\nY nuestro problema en este caso es que la media sí cambia dependiendo de cómo es la distribución dentro de cada intervalo, incluyendo los intervalos izquierdo y derecho."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Ejemplo: estimación e intervalos de confianza",
    "text": "Ejemplo: estimación e intervalos de confianza\nRegresamos a nuestro ejemplo anterior donde muestreamos 3 grupos, y nos preguntábamos acerca de la diferencia de sus medianas. En lugar de hacer pruebas de permutaciones (con gráficas o numéricas), podríamos considerar qué tan precisa es cada una de nuestras estimacione para las medianas de los grupos, por ejemplo.\nNuestros resultados podríamos presentarlos como sigue:\n\n\n\n\n\nDonde en rojo está nuestro estimador puntual de la mediana de cada grupo (la mediana muestral), y las rectas mustran un intervalo de 95% para nuestra estimación de la mediana: esto quiere decir que los valores poblacionales tienen probabilidad aproximada de 95% de estar dentro del intervalo.\nEste análisis comunica correctamente que tenemos incertidumbre alta acerca de nuestras estimaciones (especialmente grupos b y c), y que no tenemos mucha evidencia de que el grupo b tenga una mediana poblacional considerablemente más alta que a o c."
  },
  {
    "objectID": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Interpretación de intervalos de confianza",
    "text": "Interpretación de intervalos de confianza\nGeneralmente, “intervalo de confianza” (de 90% de confianza, por ejemplo) significa, desde el punto de vista frecuentista:\n\nCada muestra produce un intervalo distinto. Para el 90% de las muestras posibles, el intervalo cubre al valor poblacional.\nAsí que con alta probabilidad, el valor poblacional está dentro del intervalo.\nIntervalos más anchos nos dan más incertidumbre acerca de dónde está el verdadero valor poblacional (y al revés para intervalos más angostos)\n\nExisten también “intervalos creíbles” (de 90% de probabilidad, por ejemplo), que se interpetan de forma bayesiana:\n\nCon alta probabilidad, creemos que el valor poblacional está dentro del intervalo creíble.\n\nLa técnica que veremos a continuación (bootstrap) se puede interpretar de las dos maneras.\n\nLa interpretación bayesiana puede ser más natural\nLa interpretación frecuentista nos da maneras empíricas de probar si los intervalos de confianza están bien calibrados o no: es un mínimo que “intervalos del 90%” debería satisfacer.\n\nAsí que tomamos el punto de vista bayesiano en la intepretación, pero buscamos que nuestros intervalos cumplan o aproximen bien garantías frecuentistas (discutimos esto más adelante)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "href": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Cómo producir intervalos para estimación",
    "text": "Cómo producir intervalos para estimación\nExisten muchas técnicas para construir estos intervalos que muestran la incertidumbre en nuestras estimaciones: métodos basados en distribuciones estándar, métodos paramétricos y no paramétricos, distintos métodos bayesianos (entonces se llaman intervalos creíbles o de probabilidad), etc.\nEn este curso, como ejemplo, y también por ser una técnica versátil, presentaremos el bootstrap no paramétrico (ver Efron y Tibshirani (1993)), donde utilizaremos simulación (y poder de cómputo) para producir este tipo de intervalos, bajo ciertas condiciones de extracción de la muestra que discutiremos más adelante."
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Distribución de Muestreo",
    "text": "Distribución de Muestreo\nSupongamos que consideramos la población de casas de nuestro ejemplo anterior\n\n\nCódigo\ncasas_pob <- casas |> select(id, precio_miles, nombre_zona)\ncasas_pob |> sample_n(20) |> formatear_tabla()\n\n\n\n\n \n  \n    id \n    precio_miles \n    nombre_zona \n  \n \n\n  \n    721 \n    275.0 \n    StoneBr \n  \n  \n    237 \n    185.5 \n    CollgCr \n  \n  \n    270 \n    148.0 \n    Edwards \n  \n  \n    269 \n    120.5 \n    IDOTRR \n  \n  \n    362 \n    145.0 \n    BrkSide \n  \n  \n    1152 \n    149.9 \n    Edwards \n  \n  \n    973 \n    99.5 \n    SawyerW \n  \n  \n    1037 \n    315.5 \n    Timber \n  \n  \n    1092 \n    160.0 \n    Somerst \n  \n  \n    1286 \n    132.5 \n    BrkSide \n  \n  \n    1208 \n    200.0 \n    CollgCr \n  \n  \n    540 \n    272.0 \n    CollgCr \n  \n  \n    367 \n    159.0 \n    NAmes \n  \n  \n    283 \n    207.5 \n    NridgHt \n  \n  \n    52 \n    114.5 \n    BrkSide \n  \n  \n    761 \n    127.5 \n    NAmes \n  \n  \n    1297 \n    155.0 \n    NAmes \n  \n  \n    822 \n    93.0 \n    OldTown \n  \n  \n    956 \n    145.0 \n    Crawfor \n  \n  \n    790 \n    187.5 \n    ClearCr \n  \n\n\n\n\n\nY nos interesa saber, para la población, cuál es la mediana de los precios de casas. Suponemos que no tenemos acceso a los datos poblacionales, y decidimos diseñar una encuesta para tomar una muestra de 50 casas que fueron vendidas en cierto periodo. Suponemos una muestra aleatoria simple con reemplazo (la población es grande y no hay mucha diferencia entre hacerlo con o sin reemplazo) de tamaño fijo, por ejemplo \\(n = 50\\)\nBuscamos estimar la mediana poblacional con la mediana de nuestra muestra:\n\n\nCódigo\nfun_muestra <- function(x){\n    median(x)\n}\n\n\nComo es de esperarse, distintas muestras dan distintas estimaciones de la mediana\n\n\nCódigo\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     156\n\n\nCódigo\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     145\n\n\nEn estimación, uno de los conceptos básicos el de la distribución de muestreo. La distribución de muestreo son los valores que puede tomar nuestro estimador bajo todas las posibles muestras que pudiéramos obtener.\n¿Por qué es importante este concepto? La distribución de muestreo del estimador nos indica qué tan lejos o cerca vamos a caer del verdadero valor poblacional que queremos estimar. No sabemos qué muestra vamos a obtener, pero con la distribución de muestreo podemos saber qué tan mal o bien nos puede ir y con qué probabilidades."
  },
  {
    "objectID": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Aproximando la distribución de muestreo",
    "text": "Aproximando la distribución de muestreo\nEn nuestro ejemplo tenemos la población (esto normalmente no es cierto) y podemos extraer un número muy grande de muestras de tamaño 50. Calculamos el estimador para cada una de esas muestras. El código es simple:\n\n\nCódigo\n# Repetir 5000 veces\nmediana_muestras <- map_dbl(1:5000, ~ casas_pob |> \n    sample_n(50, replace = T) |>  # muestra de 50\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) # calcular mediana de la muestra\n\n\nAhora examinamos la distribución de los valores que obtuvimos:\n\n\nCódigo\nsims_dm <- tibble(muestra = 1:length(mediana_muestras), mediana_precio = mediana_muestras)\nvalor_poblacional <- median(casas$precio_miles) \nggplot(sims_dm, aes(sample = mediana_muestras)) + geom_qq(distribution = stats::qunif) +\n    ylab(\"Mediana muestral\") + xlab(\"f\") + labs(subtitle = \"Distribución de muestreo para mediana (n = 50)\") +\n    geom_hline(yintercept = valor_poblacional, colour = \"red\") +\n    annotate(\"text\", x = 0.2, y = valor_poblacional+5, label = \"Mediana poblacional\", colour = \"red\")\n\n\n\n\n\n\nCon esta gráfica podemos juzgar qué tan lejos puede caer nuestra estimación muestral del valor poblacional. Cuanto más concentrada esté alrededor del valor poblacional, la probabilidad es más alta de que obtengamos una estimación precisa cuando tomemos una muestra particular. Podemos hacer un histograma también:\n\n\n\n\n\n\nLos cuantiles que cubren a un 95% de las muestras son:\n\n\nCódigo\nquantile(mediana_muestras - valor_poblacional , c(0.025, 0.975)) |> round(1)\n\n\n 2.5% 97.5% \n-21.5  21.9 \n\n\n\nEsto quiere decir que si estimamos con una muestra el valor poblacional, esperamos con 95% de probabilidad que el error sea menos de unas 20 unidades. *Esta es la precisión de nuestro estimador.\n\nSi usamos una muestra más grande (n = 200, por ejemplo) podemos obtener un resultado más preciso:\n\n\n\n\n\nY como es de esperarse, vemos que muestras más grandes resultan en menos variablidad, y menor error de estimación.\n\nMejores distribuciones de muestreo: más concentradas alrededor del verdadero valor poblacional"
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Distribución de muestreo y distribución poblacional",
    "text": "Distribución de muestreo y distribución poblacional\nUna confusión inicial que es común es entre la distribución de muestreo y la distribución poblacional. La poblacional muestra cómo se distribuyen los valores de la variable de interés:\n\n\nCódigo\nggplot(casas_pob, aes(x = precio_miles)) + geom_histogram() +\n    geom_vline(xintercept = valor_poblacional)\n\n\n\n\n\nQue es muy diferente que las distribuciones de muestreo de nuestros dos estimadores:\n\n\nCódigo\ng_dist_muestreo"
  },
  {
    "objectID": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "href": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "title": "7  Remuestreo: el bootstrap",
    "section": "El mundo bootstrap",
    "text": "El mundo bootstrap\nEl problema que tenemos ahora es que normalmente sólo tenemos una muestra, así que no es posible calcular las distribuciones de muestreo como hicimos arriba. Sin embargo, podemos hacer lo siguiente:\n\nSi tuviéramos la distribución poblacional, simulamos muestras para aproximar la distribución de muestreo de nuestro estimador, y así entender su variabilidad.\nPero no tenemos la distribución poblacional\nSin embargo, podemos estimar la distribución poblacional con nuestros valores muestrales\n\nMundo bootstrap\n\nSi usamos la estimación del inciso anterior, entonces usando 1 podríamos tomar muestras de nuestros datos muestrales, como si fueran de la población, y usando el mismo tamaño de muestra. El muestreo lo hacemos con reemplazo, como la muestra original.\nA la distribución resultante le llamamos distribución bootstrap de la muestra\nUsamos la distribución bootstrap de la muestra para estimar la variabilidad en nuestra estimación con la muestra original.\n\nVeamos que sucede para un ejemplo concreto. Primero extraemos nuestra muestra:\n\n\nCódigo\nset.seed(2112)\nmuestra <- sample_n(casas_pob, 150, replace = T)\n\n\nEsta muestra nos da nuestro estimador de la distribución poblacional:\n\n\nCódigo\nbind_rows(muestra |> mutate(tipo = \"muestra\"),\n    casas_pob |> mutate(tipo = \"población\")) |> \nggplot(aes(sample = precio_miles, colour = tipo, group = tipo)) + \n    geom_qq(distribution = stats::qunif, alpha = 0.7, size = 2) + \n    scale_color_colorblind()\n\n\n\n\n\nY vemos que la aproximación es razonable, especialmente en las partes centrales de la distribución. Usamos nuestra muestra para estimar la población.\nPara evaluar ahora la variabilidad de nuestro estimador, podemos extraer un número grande de muestras con reemplazo de tamaño 150 de la muestra - estamos en el mundo Bootstrap!\n\n\nCódigo\nmediana_muestras <- map_dbl(1:5000, ~ muestra |>  \n    sample_n(150, replace = T) |>\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) \n\n\nY nuestra estimación de la distribución de muestreo es entonces:\n\n\nCódigo\nbootstrap <- tibble(mediana = mediana_muestras)\nggplot(bootstrap, aes(sample = mediana)) + geom_qq(distribution = stats::qunif)\n\n\n\n\n\nY podemos calcular ahora un intervalo de confianza del 90% simplemente calculando los cuantiles de esta distribución (no son los cuantiles de la muestra original!):\n\n\nCódigo\nlimites_ic <- quantile(mediana_muestras, c(0.05,  0.95)) |> round()\nlimites_ic\n\n\n 5% 95% \n154 173 \n\n\nPresentaríamos nuestro resultado como sigue: nuestra estimación puntual de la mediana es 165, con un intervalo de confianza del 90% de (154, 173)"
  },
  {
    "objectID": "remuestreo-bootstrap.html#experimento-de-simulación",
    "href": "remuestreo-bootstrap.html#experimento-de-simulación",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Experimento de simulación",
    "text": "Experimento de simulación\nEn nuestro ejemplo, podemos ver varias muestras (por ejemplo 20) de tamaño 100, y vemos cómo se ve la aproximación a la distribución de la población:\n\n\n\n\n\nPodemos calcular las distribuciones de remuestreo para cada muestra bootstrap, y compararlas con la distribución de muestreo real."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "href": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Cobertura nominal y cobertura real",
    "text": "Cobertura nominal y cobertura real\n¿Cómo sabemos que la cobertura nominal del 90% es cercana a la realidad? Sería muy malo que los intervalos fueran demasiado anchos (exageramos la variabilidad) o demasiado angostos (damos la idea de que nuestra estimación es más precisa de lo que realmente es). Que esto se cumpla depende de:\n\nCuál es la estadística de interés\nCómo es la población\nEl tamaño de muestra y otros aspectos del muestreo\n\nVarias observaciones útiles se pueden consultar en Hesterberg (2015) y en Efron y Tibshirani (1993) (por ejemplo, el bootstrap no funciona bien para estadísticas como el mínimo o el máximo). En estas referencias también pueden consultarse recomendaciones de cómo mejorar intervalos basados en boostrap - los que vimos se llaman intervalos de percentiles, pero hay más opciones simples que se desempeñan mejor en ciertos casos.\nY siempre podemos hacer ejercicios de simulación bajo ciertos supuestos acerca de la población para una estadística dada, y estimar empíricamente si la cobertura es adecuada. \n\nEjemplo\nConstruimos para nuestra población varias muestras bootstrap con sus respectivos intervalos de cuantiles. ¿Qué porcentaje de veces cubrimos al verdadero valor?\n\n\n\n\n\nCódigo\n#rep_remuestreo <- map(1:200, ~ muestras_boot(.x, B = 2000, fun_muestra = fun_muestra)) |> bind_rows()\nrep_remuestreo <- read_csv(\"./datos/bootstrap_reps.csv\")\n\n\nCon nuestras muestras, checamos ahora nuestros intervalos y su cobertura\n\n\nCódigo\nintervalos <- rep_remuestreo |> \n    group_by(n, rep) |> \n    summarise(inf =  quantile(mediana, 0.05), sup = quantile(mediana, 0.95)) |> \n    mutate(valor_poblacional = median(casas_pob$precio_miles))\n\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\n\n\nCódigo\nggplot(intervalos, aes(x = rep, ymin = inf, ymax = sup)) + \n    geom_hline(yintercept = median(casas_pob$precio_miles), colour = \"salmon\") +\n    geom_linerange(alpha = 0.7) +\n    facet_wrap(~n) \n\n\n\n\n\nLa cobertura para nuestros intervalos es:\n\n\nCódigo\nintervalos |> mutate(cubre = valor_poblacional > inf & valor_poblacional < sup) |> \n     group_by(n) |> summarise(cobertura = mean(cubre), \n                               ee_cobertura = (sd(cubre) /sqrt(n())) |> round(3)) \n\n\n# A tibble: 2 × 3\n      n cobertura ee_cobertura\n  <dbl>     <dbl>        <dbl>\n1    50     0.895        0.022\n2   150     0.915        0.02 \n\n\nPara este número de repeticiones, estos números son consistentes con la cobertura nominal de 90%."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "href": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Ejemplo: estereogramas",
    "text": "Ejemplo: estereogramas\nEn este caso, queremos hacer inferencia sobre la diferencia de tiempo de reconocimiento de los grupos. Como discutimos antes, preferimos hacer comparaciones multiplicativas. En este caso particular, compararemos el cociente de las medias:\nPodemos adaptar el bootstrap en este caso para dos grupos: hacemos remuestreo de cada grupo, comparamos diferencias, y repetimos\n\n\nCódigo\nfusion <- read_table(\"./datos/fusion_time.txt\")\nmuestra_boot <- function(datos, grupo, medicion, fun_muestra, comparacion){\n    est_boot <- datos |> group_by({{ grupo }}) |> \n      sample_n(n(), replace = T) |> \n      summarise(est = fun_muestra( {{ medicion }})) |> \n      spread(nv.vv, est) |> \n      mutate(comp = {{ comparacion }}) |> \n      pull(comp)\n    est_boot\n}\nmuestra_boot(fusion, nv.vv, time, median, VV / NV) |> round(2)\n\n\n[1] 0.59\n\n\nLa distribución de remuestreo es:\n\n\nCódigo\nreps_boot <- map_dbl(1:2000, ~ muestra_boot(fusion, nv.vv, time, mean, VV / NV))\nggplot(tibble(cociente_boot = reps_boot), aes(sample = reps_boot)) +\n  geom_qq(distribution = stats::qunif) + xlab(\"f\") + ylab(\"Cociente\")\n\n\n\n\n\ny un intervalo de 90% sería:\n\n\nCódigo\nquantile(reps_boot, c(0.05, 0.95)) |> round(2)\n\n\n  5%  95% \n0.46 0.91 \n\n\nY esta sería una forma de presentar nuestros resultados: hay probabilidad considerable de que el efecto de este tratamiento sea marginal (una reducción de 10%), aunque lo más probables es que tenga un efecto consdierable (reducción alrededor de 60% del tiempo de fusión)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "href": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Ventajas y desventajas del bootstrap",
    "text": "Ventajas y desventajas del bootstrap\n\nEl bootstrap es una técnica versátil generalmente fácil de implementar (ventaja) - especialmente cuando a algún nivel podemos suponer que las muestras son idependientes e idénticamente distribuidas (desventaja).\n\nPor ejemplo: en muestreo estratificado, podemos hacer bootstrap sobre cada estrato por separado. En muestreo complejo, podemos hacer bootstrap de unidades primarias de muestreo, etc.\n\nRequiere más cómputo que fórmulas estándar (desventaja), pero tenemos flexibilidad (ventaja) para aplicar en estadísticas diferentes de manera muchas veces trivial (ventaja).\nEs una técnica estándar en el análisis de datos que se usa en un rango grande de aplicaciones (ventaja).\nEn el caso de muestras chicas y ciertas distribuciones poblacionales, los intervalos bootstrap de percentiles que vimos aquí pueden ser un poco angostos y no cumplir la cobertura nominal por ejemplo, si la muestra es de tamaño < 40. la cobertura puede ser de 90% en lugar de 95% en algunos casos (población normal, o de 80% en lugar de 95% en una poblacion exponencial), ver Hesterberg (2015)). Hay mejores opciones en estos casos (por ejempo, intervalos bootstrap-t, que se calculan fácilmente también).\nFinalmente, en casos donde tenemos la población total, o el supuesto de muestras aleatorias es dudoso, lo podemos utilizar más informalmente como un análisis de sensibilidad de nuestros resultados. Es una perturbación a los datos (que podemos combinar con otros tipos de perturbaciones) para juzgar qué tan fuertemente depende nuestro análisis de los datos que tenemos a mano."
  },
  {
    "objectID": "remuestreo-bootstrap.html#sesgo",
    "href": "remuestreo-bootstrap.html#sesgo",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Sesgo",
    "text": "Sesgo\nAlgunos estimadores comunes (por ejemplo, cociente de dos cantidades aleatorias) pueden sufrir de **sesgo* grande, especialmente en el caso de muestras chicas. Esto afecta la cobertura, pues es posible que nuestros intervalos no tengan “cobertura simétrica”, por ejemplo. Para muchos estimadores, y muestras no muy chicas, esté sesgo tiende a ser poco importante y no es necesario hacer correcciones.\nPodemos evaluar el sesgo comparando la media de nuestras replicaciones bootstrap con el valor muestral que obtuvimos (para estadísticas funcionales, ver Hesterberg (2015)). Si el tamaño del sesgo es chico comparado con la dispersión de la distribución bootstrap (por ejemplo, menos de 20% de la desviación estándar, Efron y Tibshirani (1993)), no es muy importante hacer correcciones.\nEn caso de que esta cantidad sea relativamente grande en relación a la dispersión de la distribución bootstrap, hay variantes los intervalos bootstrap de percentiles que mejoran esta situación (Efron y Tibshirani (1993))."
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Bootstrap y estimadores complejos: suavizadores",
    "text": "Bootstrap y estimadores complejos: suavizadores\nEl bootstrap es una técnica versátil. Por ejemplo, podemos usarlo para juzgar la variabilidad de un suavizador:\n\n\nCódigo\ngraf_casas <- function(data){\n    ggplot(data |> filter(calidad_gral < 7), \n        aes(x = area_habitable_sup_m2)) + \n        geom_point(aes(y = precio_m2_miles), alpha = 0.75) +\n        geom_smooth(aes(y = precio_m2_miles), method = \"loess\", span = 0.7, \n                se = FALSE, method.args = list(degree = 1, family = \"symmetric\"))     \n}\nset.seed(250)\ncasas_muestra <- sample_frac(casas, 0.2)\ngraf_casas(casas_muestra)\n\n\n\n\n\nPodemos hacer bootstrap para juzgar la estabilidad del suavizador:\n\n\nCódigo\nsuaviza_boot <- function(x, data){\n    # remuestreo\n    muestra_boot <- sample_n(data, nrow(data), replace = T)\n    ajuste <- loess(precio_m2_miles ~ area_habitable_sup_m2, data = muestra_boot, \n                    degree = 1, span = 0.7, family = \"symmetric\")\n    datos_grafica <- tibble(area_habitable_sup_m2 = seq(25, 250, 5))\n    ajustados <- predict(ajuste, newdata = datos_grafica)\n    datos_grafica |> mutate(ajustados = ajustados) |> \n        mutate(rep = x)\n}\nreps <- map(1:10, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n\n\n\n\nCódigo\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 1, colour = \"red\") \n\n\n\n\n\nDonde vemos que algunas cambios de pendiente del suavizador original no son muy interpretables (por ejemplo, para áreas chicas) y alta variabilidad en general en los extremos. Podemos hacer más iteraciones para calcular bandas de confianza:\n\n\nCódigo\nreps <- map(1:200, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 0.2, colour = \"red\")"
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "title": "7  Remuestreo: el bootstrap",
    "section": "Bootstrap y estimadores complejos: tablas de perfiles",
    "text": "Bootstrap y estimadores complejos: tablas de perfiles\nPodemos regresar al ejemplo de la primera sesión donde calculamos perfiles de los tomadores de distintos tés: en bolsa, suelto, o combinados:\n\n\n\n\n\n\n\n\n\n\n \n  \n    price \n    tea bag \n    tea bag+unpackaged \n    unpackaged \n    promedio \n  \n \n\n  \n    p_upscale \n    -0.71 \n    -0.28 \n    0.98 \n    28 \n  \n  \n    p_variable \n    -0.12 \n    0.44 \n    -0.31 \n    36 \n  \n  \n    p_cheap \n    0.3 \n    -0.53 \n    0.23 \n    2 \n  \n  \n    p_branded \n    0.62 \n    -0.16 \n    -0.45 \n    25 \n  \n  \n    p_private label \n    0.72 \n    -0.22 \n    -0.49 \n    5 \n  \n  \n    p_unknown \n    1.58 \n    -0.58 \n    -1 \n    3 \n  \n\n\n\n\n\n\n\n\n\n\nHacemos bootstrap sobre toda la muestra, y repetimos exactamente el mismo proceso de construción de perfiles:\n\n\nCódigo\nboot_perfiles <- map(1:1000, function(x){\n    te_boot <- te |> sample_n(nrow(te), replace = TRUE)\n    calcular_perfiles(te_boot) |> mutate(rep = x)\n}) |> bind_rows()\n\n\nAhora resumimos y graficamos, esta vez de manera distinta:\n\n\nCódigo\nresumen_perfiles <- boot_perfiles |> group_by(how, price) |> \n    summarise(perfil_media = mean(perfil), ymax = quantile(perfil, 0.9), ymin = quantile(perfil, 0.10)) \n\n\n`summarise()` has grouped output by 'how'. You can override using the `.groups`\nargument.\n\n\nCódigo\nresumen_bolsa <- resumen_perfiles |> ungroup() |> \n    filter(how == \"tea bag\") |> select(price, perfil_bolsa = perfil_media)\nresumen_perfiles <- resumen_perfiles |> left_join(resumen_bolsa) |> \n    ungroup() |> \n    mutate(price = fct_reorder(price, perfil_bolsa))\n\n\nJoining, by = \"price\"\n\n\nCódigo\nggplot(resumen_perfiles, aes(x = price, y = perfil_media, ymax = ymax, ymin = ymin)) + \n    geom_point(colour = \"red\") + geom_linerange() +\n    facet_wrap(~how) + coord_flip() +\n    geom_hline(yintercept = 0, colour = \"gray\") + ylab(\"Perfil\") + xlab(\"Precio\")\n\n\n\n\n\nNótese una deficiencia clara del bootstrap: para los que compran té suelto, en la muestra no existen personas que desconocen de dónde provienen su té (No sabe/No contestó). Esto produce un intervalo colapsado en 0 que no es razonable.\nPodemos remediar esto de varias maneras: quitando del análisis los que no sabe o no contestaron, agrupando en otra categoría, usando un modelo, o regularizar usando proporciones calculadas con conteos modificados: por ejemplo, agregando un caso de cada combinación (agregaría 18 personas “falsas” a una muestra de 290 personas).\n\n\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86."
  },
  {
    "objectID": "probabilidad.html",
    "href": "probabilidad.html",
    "title": "Introducción a teoría de probabilidades",
    "section": "",
    "text": "Hasta este punto del curso asumimos que el lector tiene un buen grado de familiaridad con la teoría básica de la probabilidad, pero dado que los usos de la probabilidad dentro de un marco bayesiano son mucho más amplios, en este capítulo repasaremos brevemente los fundamentos de la teoría de probabilidad."
  },
  {
    "objectID": "modelo-prob.html",
    "href": "modelo-prob.html",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "",
    "text": "En esta parte comenzaremos a tratar el concepto de probabilidad de ocurrencia de eventos acerca de los que tenemos incertidumbre. Veremos que:"
  },
  {
    "objectID": "modelo-prob.html#equiprobabilidad-y-simetría",
    "href": "modelo-prob.html#equiprobabilidad-y-simetría",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.1 Equiprobabilidad y simetría",
    "text": "8.1 Equiprobabilidad y simetría\nHistóricamente, el concepto de probabilidad nació en el contexto de juegos de azar, y cómo definir apuestas ajustas o equitativas. Por ejemplo:\n\nSupongamos que nos cuesta 1 peso entrar a un juego de dados, y que ganamos si tiramos un 3. ¿Cuánto dinero deberíamos ganar si sale un 3 para que el costo de entrada sea justo?\n\nEl argumento iría como sigue: existen 6 posibles resultados, y como el dado se tira bien y está bien hecho, entonces sería lo mismo apostar a cualquier número. Entonces, si seis personas entran al juego apostando a distintos números, cada uno pagando 1 peso, sería justo que el ganador se llevara 6 pesos.\n\nEn este caso, definimos la probabilidad de obtener un 3 (o cualquier otro número) como 1/6, que es cociente entre la apuesta inicial entre la cantidad recibida si ganamos la apuesta justa.\n\nNótese que el criterio de “justicia” proviene de simetrías del experimento: si el dado no fuera simétrico, por ejemplo, esta sería una manera mala de definir probabilidades.\nEn general, una manera de difinir probabilidad es la siguiente:\n\n\n\nEspacios equiprobables\nSi un evento \\(A\\) puede ocurrir de \\(k\\) maneras de \\(n\\) posibles, y es indiferente apostar a cualquiera de los \\(n\\) posibles resultados, entonces\n\\[P(A) = \\frac{k}{n}\\]\n\n\nCon este enfoque podemos resolver diversos problemas de probabilidad, por ejemplo:"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dados",
    "href": "modelo-prob.html#ejemplo-dados",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dados",
    "text": "Ejemplo: dados\n¿Cuál es la probabilidad de que tiremos una suma de 9 con dos dados de seis lados?\nEn este caso, los resultados son pares \\((x, y)\\) donde \\(x\\) es el resultado del dado 1 y \\(y\\) es el resultado del dado 2. Existen 36 pares, y todos ellos son equivalentes. Para tirar un 9, tenemos que lograr alguna de las siguientes tiradas:\n\\[(3, 6), (4, 5), (5, 4), (6, 3)\\]\nDe modo que la probabilidad de tirar una suma de 9 , que escribimos como \\(S=9\\), es\n\\[P(S = 9) = 4/36 = 1/9\\]"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-cartas",
    "href": "modelo-prob.html#ejemplo-cartas",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "Ejemplo: cartas",
    "text": "Ejemplo: cartas\nSacamos dos cartas sucesivamente de una baraja de 52 cartas, donde 26 son negras y 26 rojas. ¿Cuál es la probabilidad de que la segunda carta que saquemos sea negra? Podemos denotar como \\(N_2\\) el evento que sucede cuando la segunda carta que sacamos es negra. La segunda carta que sacamos puede ser cualquiera de las 52, y somos indiferentes en apostar a cualqueira de las cartas para salir en segundo lugar, así que\n\\[P(N_2) = 26 / 52 = 1/2\\]\nEstos espacios equiprobables nos dan nuestros modelos probabilísticos más simples. Están basados en simetrías del espacio de resultados."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "href": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.2 Probabilidad y frecuencias relativas",
    "text": "8.2 Probabilidad y frecuencias relativas\nLa conexión entre experimentos reales y modelos de probabilidad, está en que si repetimos muchas veces el experimento, entonces las frecuencias relativas de ocurrencia de los eventos aproxima a las probabilidades teóricas. Por ejemplo, si tiramos muchos volados con una moneda bien balanceada, esperamos obtener alrededor de 1/2 de soles y 1/2 de águilas, y si tiramos un dado muchas veces esperamos obtener alrededor de 1/6 de las tiradas un 5, por ejemplo (bajo los modelos de resultados equiprobables correspondientes).\nEn realidad, esta es otra definición de probabilidad en términos de repeticiones de experimentos.\n\n\n\nProbabilidades y frecuencias\nSupongamos que repetimos una gran cantidad \\(n\\) de veces un experimento, y que registramos \\(k_n\\) = cuántas veces ocurre un evento \\(A\\). La probabilidad de que ocurra \\(A\\) es\n\\[\\lim_{n\\to\\infty} \\frac{k_n}{n} \\to P(A), \\]\nes decir, \\(P(A)\\) el la frecuencia al largo plazo de ocurrencia de \\(A\\).\n\n\nAunque podríamos hacer algunos experimentos físicos más reales, para este curso podemos hacer simulaciones de computadora del experimento que nos interesa."
  },
  {
    "objectID": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "href": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "Ejemplo: simulación de un dado",
    "text": "Ejemplo: simulación de un dado\nPrimero hacemos un dado. Podemos simular una tirada de dado como:\n\n\nCódigo\nsimular_dado <- function(caras = 1:6){\n   sample(caras, 1)\n}\nsimular_dado()\n\n\n[1] 4\n\n\nAhora simulamos una gran cantidad de tiradas de dado:\n\n\nCódigo\nset.seed(199213)\nn <- 5000\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nhead(sims_dado) \n\n\n# A tibble: 6 × 2\n  n_sim resultado\n  <int>     <int>\n1     1         3\n2     2         6\n3     3         3\n4     4         3\n5     5         6\n6     6         3\n\n\nEsta es una variable numérica, pero como toma valores enteros del uno al seis, podemos resumir con frecuencias, como si fuera categórica:\n\n\nCódigo\nsims_dado %>% \n   count(resultado) %>% \n   mutate(frec_relativa = n / sum(n))\n\n\n# A tibble: 6 × 3\n  resultado     n frec_relativa\n      <int> <int>         <dbl>\n1         1   806         0.161\n2         2   814         0.163\n3         3   857         0.171\n4         4   898         0.180\n5         5   856         0.171\n6         6   769         0.154\n\n\nY nuestro modelo teórico (resultados equiprobables) coincide razonablemente bien con las frecuencias observadas a largo plazo. Podemos ver cómo convergen las frecuencias relativas por ejemplo del resultado 1:\n\n\nCódigo\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")\n\n\n\n\n\nNótese que cuando hay pocas repeticiones podemos ver fluctuaciones considerablemente grandes de la frecuencia relativa observada de unos. Sin embargo, conforme aumentamos el tamaño de la meustra observada, esas fluctuaciones son más chicas.\nVeamos otra simulación:\n\n\nCódigo\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")"
  },
  {
    "objectID": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "href": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.3 Datos y modelos de probabilidad",
    "text": "8.3 Datos y modelos de probabilidad\n¿Cómo podemos usar modelos de probablidad para describir datos observados? La idea (simplificada) es la siguiente:\n\nHacemos una hipótesis acerca de cómo es el modelo de probabilidad asociado a un fenómeno\nObservamos una muestra de datos del fenómeno que nos interesa\nEvaluamos si las fluctuaciones observadas debidas a la información limitada que tenemos (una muestra) son consistentes con el modelo de probabilidad\n\nConsideremos el ejemplo de los dados. Supongamos que lanzamos el dado un número no muy grande de veces, y observamos:\n\n\nCódigo\nfrecs_obs <- tibble(resultado = 1:6,\n                    n = c(5, 7, 5, 10, 8, 5)) %>% \n   mutate(frec = n / sum(n))\nfrecs_obs %>% kable(digits = 2)\n\n\n\n\n \n  \n    resultado \n    n \n    frec \n  \n \n\n  \n    1 \n    5 \n    0.12 \n  \n  \n    2 \n    7 \n    0.17 \n  \n  \n    3 \n    5 \n    0.12 \n  \n  \n    4 \n    10 \n    0.25 \n  \n  \n    5 \n    8 \n    0.20 \n  \n  \n    6 \n    5 \n    0.12 \n  \n\n\n\n\n\nY nos preguntamos si este resultado podría ser observado bajo los supuestos de nuestro modelo de probabilidad, que en este caso, es el de resultados equiprobables. Podemos por ejemplo graficar los datos junto con simulaciones del modelo, en búsqueda de desajustes:\n\n\nCódigo\nset.seed(8834)\n# una vez\nsim_exp <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()))\n# 19 veces\nsims_exp <- map_df(1:19, function(x){\n         sims <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()) )\n         sims$rep <- x\n         sims\n         })\n\n\n\n\nCódigo\nfrec_sims <- sims_exp %>% \n   group_by(rep, resultado) %>% \n   summarise(n = n()) %>% \n   mutate(frec = n / sum(n))\n\n\n`summarise()` has grouped output by 'rep'. You can override using the `.groups`\nargument.\n\n\nCódigo\nobs_sims_tbl <- bind_rows(frec_sims, frecs_obs %>% mutate(rep = 20))\nggplot(obs_sims_tbl, aes(x = resultado, y = frec)) +\n   geom_col() +\n   facet_wrap(~rep)\n\n\n\n\n\nEn este caso, no vemos ninguna característica de los datos observados que no sea consistente las fluctuaciones esperadas para un tamaño de muestra de \\(n=40\\).\nPregunta: ¿qué defecto le ves a esta gráfica que tiene los datos en la posición 20? ¿Cómo podríamos hacer una evaluación más apropiada de la calidad del ajuste?\nObservación: como veremos, muchas veces proponemos modelos que tienen parámetros que deben ser estimados con la muestra. Este caso más común es más complejo que el explicado arriba, pero el proceso es similar."
  },
  {
    "objectID": "modelo-prob.html#espacios-no-equiprobables",
    "href": "modelo-prob.html#espacios-no-equiprobables",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.4 Espacios no equiprobables",
    "text": "8.4 Espacios no equiprobables\nDepende cómo planteemos nuestro experimento aleatorio, puede ser o no apropiado el modelo equiprobable. Veremos ahora algunos ejemplos donde construimos modelos no equiprobables"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dos-dados",
    "href": "modelo-prob.html#ejemplo-dos-dados",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dos dados",
    "text": "Ejemplo: dos dados\nSupongamos que nos interesa la suma de dos tiradas de dados. Comenzamos con un modelo equiprobable sobre los resultados de cada tirada, que denotamos como \\((x,y)\\). Cada resultado tiene probabilidad 1/36.\nSin embargo, sólo nos interesa la suma. Contando posibles resultados podemos dar la probabilidad de cada resultado:\n\n\n\nSuma\nResultados\nProb\n\n\n\n\n2\n(1,1)\n1/36\n\n\n3\n(1,2),(2,1)\n2/36\n\n\n4\n(1,3),(2,2),(3,1)\n3/36\n\n\n5\n(1,4),(2,3),(3,2),(4,1)\n4/36\n\n\n6\n(1,5),(2,4),(3,3),(4,2),(5,1)\n5/36\n\n\n7\n(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\n6/36\n\n\n8\n(2,6),(3,5),(4,4),(5,3),(6,2)\n5/35\n\n\n9\n(3,6),(4,5),(5,4),(6,3)\n4/36\n\n\n10\n(4,6),(5,5),(6,4)\n3/36\n\n\n11\n(5,6), (6,5)\n2/36\n\n\n12\n(6,6)\n1/36\n\n\n\nY entonces terminamos con la siguiente distribución no equiprobable sobre las posibles sumas:\n\n\nCódigo\nprobs_suma <- tibble(suma = 2:12) %>% \n   mutate(prob = (6 - abs(suma - 7)) / 36)\nprobs_suma %>% kable(digits = 3)\n\n\n\n\n \n  \n    suma \n    prob \n  \n \n\n  \n    2 \n    0.028 \n  \n  \n    3 \n    0.056 \n  \n  \n    4 \n    0.083 \n  \n  \n    5 \n    0.111 \n  \n  \n    6 \n    0.139 \n  \n  \n    7 \n    0.167 \n  \n  \n    8 \n    0.139 \n  \n  \n    9 \n    0.111 \n  \n  \n    10 \n    0.083 \n  \n  \n    11 \n    0.056 \n  \n  \n    12 \n    0.028 \n  \n\n\n\n\n\nEsta distribución la podemos graficar como sigue:\n\n\nCódigo\ng_teorica <- ggplot(probs_suma, aes(x = suma, y = prob)) +\n   geom_col() +\n   scale_x_continuous(breaks = 2:12)\ng_teorica\n\n\n\n\n\nPodríamos tirar dos dado un gran número de veces para verificar si este modelo da las probabilidades correctas (o más propiamente dicho, si estas observaciones son consistentes con el modelo de probabilidad mostrado arriba)."
  },
  {
    "objectID": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "href": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.5 Reglas básicas de probabilidad",
    "text": "8.5 Reglas básicas de probabilidad\nTanto modelos equiprobables como la interpretación frecuentista de la probabilidad resultan en un conjunto de reglas útiles para operar con probabilidades. Estas son reglas generales que aplican independientemente de la interpretación particular de la probabilidad que utilicemos.\n\nLa probabilidad del evento cierto es 1 Si \\(A\\) es el evento que cubre todos los posibles resultados, entonces \\(P(A) = 1\\).\n\nAsegúrate que puedes demostrar esto para un espacio equiprobable, por ejemplo.\n\nLa probabilidad de que un evento \\(A\\) no ocurra es \\(1- P(A)\\).\n\n¿Por qué es cierto esto en espacios equiprobables? ¿Y según la definición frecuentista? Usa esta regla y el ejemplo anterior para calcular la probabilidad de tirar una suma menor a 12 cuando tiramos dos dados.\nEsto normalmente lo escribimos como \\(P(A^c) = 1 - P(A)\\).\nDecimos que dos eventos \\(A\\) y \\(B\\) son disjuntos cuando no pueden ocurrir simultánemente, o en otras palabras, el conjunto de resultados donde ocurre \\(A\\) tiene intersección vacía con el conjunto de resultados donde ocurre \\(B\\).\n\nLa probabilidad de que ocurra un resultado dentro de un conjunto de eventos disjuntos es igual a la suma de las probabilidades de dichos eventos\n\nEsto lo podemos escribir como sigue: si \\(A_1, A_2, \\dots, A_n\\) son eventos disjuntos, entonces\n\\[P(A_1\\cup A_2\\cup \\cdots \\cup A_n) = P(A_1) + P(A_2) + \\cdots + P(A_n)\\]\nUtiliza esta regla para\n\nCalcula la probabilidad de tirar una suma mayor a 9 en dos tiradas de dados.\nCalcula la probabilidad de obtener un par al sacar dos cartas de una baraja usual de póquer.\nMás difícil: hay una caja con \\(100\\) pelotas, y una de ellas es dorada. Si sacamos al azar \\(20\\) pelotas, una a la vez, ¿cuál es la probabilidad de que nos salga la pelota dorada? ¿ y si sacamos 57 pelotas?\n\nSoluciones:\n\nPara tirar más de 9, podemos tirar 10, 11 o 12, así que \\(P(A) = P(A_{10}\\cup A_{11} \\cup A_{12})\\). Estos útlimos tres eventos son mutuamente excluyentes, así que por la regla de la suma,\n\n\\[P(A) = P(A_{10}) + P(A_{11}) + P(A_{12})\\] y consultando nuestra tabla de arriba,\n\\[P(A) = \\frac{3}{36} + \\frac{2}{36} + \\frac{1}{36} = 1/6\\]\n\nPodemos sacar \\(52(51)\\) pares distintos. Ahora calculamos la probabidad de sacar un par particular, por ejemplo de ases. Hay \\(4(3)=12\\) distintos pares de ases (en el orden que los sacamos). Lo mismo podemos decir de pares de 2, 3, etc. Así que sumando sobre cada una de estas posibilidades obtenemos:\n\n\\[P(A) = 13\\frac{4(3)}{52(51)} \\approx 0.0588\\]\nTambién podríamos calcular de la siguiente forma: sacamos la primera carta y la vemos. La siguiente carta puede ser una de 51 restantes, y solo con tres de ellas completamos un par, de forma que\n\\[P(A) = 3 / 51 \\approx 0.0588\\]\n\nSea \\(D\\) el evento donde en el proceso sacamos la pelota dorada, y sea \\(D_1\\) el evento donde sacamos la pelota dorada en nuestra primera extracción, \\(D_2\\) si la sacamos en la segunda, etc. Tenemos que\n\n\\[D =D_1 \\cup D_2 \\cup\\cdots \\cup D_{20}\\]\n(sacamos la dorada si y sólo si la sacamos en la extracción 1, 2 , etc. hasta 20) Además, solo hay una pelota dorada, de manera que \\(D_i\\cap D_j= \\emptyset\\), es decir, no pueden ocurrir juntos \\(D_1\\) y \\(D_2\\), \\(D_5\\) y \\(D_11\\), etc., así que por la regla de la suma\n\\[P(D) = P(D_1) + P(D_2) + \\dots + P(D_{20})\\]\nFinalmente, \\(P(D_1)= 1/100\\), pues hay 100 pelotas y todas tienen igual probabilidad de aparecen en la primera extracción. Pero también \\(P(D_2) = 1/100\\), pues todas las pelotas tienen la misma probabilidad de aparecer en la segunda extracción. Se sigue entonces que, haciendo la suma de arriba, que: \\(P(D) = 20(\\frac{1}{100}) = 0.20\\)"
  },
  {
    "objectID": "modelo-prob.html#simulación-y-probabilidad",
    "href": "modelo-prob.html#simulación-y-probabilidad",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.6 Simulación y probabilidad",
    "text": "8.6 Simulación y probabilidad\nUsando la interpetación frecuentista, también es posible resolver una variedad de problemas de probabilidad usando simulación. La idea es, si queremos aproximar la probabilidad \\(P(E)\\) de un evento es:\n\nDefinimos el espacio de resultados del experimento aleatorio.\nSimulamos el experimento aleatorio un número grande de veces.\nCalculamos para cuáles de esas simulaciones se cumple el evento \\(E\\)\nEstimamos la frecuencia relativa de ocurrencia de \\(E\\) a lo largo de todas las simulaciones.\n\n\nEjemplo: simulando dos dados\nPara dos dados, el espacio de resultados podemos escribirlo como los resultados conjunto de dos tiradas: \\((x,y)\\). Cada dado se tira de forma separada, y los resultados son equiprobables. Definimos entonces:\n\n\nCódigo\nsim_dados <- function(num_dados = 2, num_caras = 6){\n   resultado <- sample(1:num_caras, num_dados, replace = TRUE)\n   names(resultado) <- paste0(\"dado_\", 1:num_dados)\n   resultado\n}\nsim_dados()\n\n\ndado_1 dado_2 \n     5      3 \n\n\nAhora simulamos un número grande de veces el experimento:\n\n\nCódigo\nset.seed(2323)\nsims <- map_df(1:10000, ~ sim_dados())\nsims\n\n\n# A tibble: 10,000 × 2\n   dado_1 dado_2\n    <int>  <int>\n 1      5      6\n 2      3      3\n 3      2      5\n 4      3      5\n 5      6      4\n 6      2      2\n 7      3      6\n 8      6      5\n 9      3      4\n10      1      1\n# … with 9,990 more rows\n\n\nAhora que tenemos estas simulaciones, podemos estimar por ejemplo la probabilidad de tirar más de nueve con dos dados. Primero calculamos en qué simulaciones ocurre \\(E\\):\n\n\nCódigo\nsims_e <- \n   sims %>% \n   mutate(suma = dado_1 + dado_2) %>% \n   mutate(evento_E = (suma > 9)) \nsims_e\n\n\n# A tibble: 10,000 × 4\n   dado_1 dado_2  suma evento_E\n    <int>  <int> <int> <lgl>   \n 1      5      6    11 TRUE    \n 2      3      3     6 FALSE   \n 3      2      5     7 FALSE   \n 4      3      5     8 FALSE   \n 5      6      4    10 TRUE    \n 6      2      2     4 FALSE   \n 7      3      6     9 FALSE   \n 8      6      5    11 TRUE    \n 9      3      4     7 FALSE   \n10      1      1     2 FALSE   \n# … with 9,990 more rows\n\n\nY ahora calculamos la frecuencia relativa de ocurrencia de \\(E\\)\n\n\nCódigo\nsims_e %>% \n   summarise(frec_e = mean(evento_E))\n\n\n# A tibble: 1 × 1\n  frec_e\n   <dbl>\n1  0.169\n\n\nQue confirma a dos decimales el resultado que obtuvimos arriba usando la regla de la suma (o contando resultados).\n\n\nEjemplo: problema de pelotas\nExtraemos al azar\n\n\nCódigo\nset.seed(232)\npelotas <- c(\"dorada\", rep(\"otra\", 99))\nsim_extraccion <- function(){\n    sample(pelotas, 20, replace = FALSE)\n}\nsim_extraccion()\n\n\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nSimulamos el experimento aleatorio:\n\n\nCódigo\nsims_pelotas <- map(1:10000, ~ sim_extraccion())\nsims_pelotas[1:2]\n\n\n[[1]]\n [1] \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n [9] \"dorada\" \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n[17] \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n\n[[2]]\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nChecamos si ocurre o no el evento se extrajo la pelota dorada:\n\n\nCódigo\nevento_dorada <- map_lgl(sims_pelotas, ~ any(str_detect(.x, \"dorada\")))\nevento_dorada[1:10]                   \n\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nY calculamos la frecuencia relativa de ocurrencia del evento:\n\n\nCódigo\nmean(evento_dorada)\n\n\n[1] 0.2007\n\n\nQue es consistente con el resultado que obtuvimos arriba haciendo cálculos.\n\n\n\nPara estimar probabilidades de ocurrencia de un evento usualmente es posible hacer simulaciones por computadora. Esto es especialmente importante cuando el experimento y evento que consideramos hace difícil (a veces imposible) hacer cálculos analíticos.\nEste tipo de métodos basados en simulación se llaman en general métodos de Monte Carlo.\n\n\n\n\n8.6.1 Ejemplo:\nSupongamos que ponemos 15 puntos igualmente espaciados en una circunferencia. Si escogemos tres puntos al azar, ¿cuál es la probabilidad de que formen un triángulo equilátero? Este problema se puede resolver contando los posibles resultados donde se forma un triángulo equilátero, pero no es trivial. Veamos cómo la haríamos simulando:\n\n\nCódigo\nsimular_3_puntos <- function(){\n   puntos_angulo <- 0:14 * (2  *pi / 15)\n   sample(puntos_angulo, 3)\n}\nsimular_3_puntos()\n\n\n[1] 3.769911 3.351032 1.675516\n\n\nCalcular dado el resultado requiere algo de consideración, pero finalmente terminamos con (¿qué otra manera se te ocurre?):\n\n\nCódigo\nes_equilatero <- function(tres_puntos){\n   tol <- 10^(-6)\n   puntos_ord <- sort(tres_puntos)\n   dif_1 <- puntos_ord[3] - puntos_ord[2]\n   dif_2 <- puntos_ord[2] - puntos_ord[1]\n   dif_3 <- 2*pi + puntos_ord[1] - puntos_ord[3]\n   if(abs(dif_1 - dif_2) < tol & abs(dif_2 - dif_3) < tol){\n      equilatero <- TRUE\n   } else {\n      equilatero <- FALSE\n   }\n   equilatero\n}\n\n\nSimulamos:\n\n\nCódigo\nsims_puntos <- map(1:50000, ~ simular_3_puntos())\n\n\nCalculamos la ocurrencia del evento de interés:\n\n\nCódigo\nevento <- map_lgl(sims_puntos, ~ es_equilatero(.x))\n\n\nY al frecuencia relativa es:\n\n\nCódigo\nmean(evento)\n\n\n[1] 0.01168\n\n\n¿Puedes resolver este problema contando los posibles triángulos, y cuáles de ellos son equiláteros?"
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva",
    "href": "modelo-prob.html#probabilidad-subjetiva",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.7 Probabilidad subjetiva",
    "text": "8.7 Probabilidad subjetiva\nExiste otra manera más flexible de interpretar la probabilidad que tiene que ver con un “grado de creencia” en que un evento va a ocurrir. Por ejemplo, el evento \\(A\\) podría ser “va a llover mañana”, y según mi conocimiento establezco que \\[P(A) = 0.25\\]\nEsta es una probablidad subjetiva pero no arbitraria. Para entender por qué es eso, pensemos en un juego de azar que consiste jugar una lotería de 100 boletos, donde podemos jugar con un número de boletos dado. Comparamos entonces dos alternativas:\n\nSi \\(A\\) ocurre, es decir, llueve mañana, entonces gano 1000 pesos.\nLa lotería de 100 boletos tiene un único premio de 1000 pesos.\n\nEntonces me puedo preguntar:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 5 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.05\\). Ahora puedo preguntarme:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 20 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.20\\) Sin embargo, probablemente si me pregunto si prefiero jugar 1 o jugar 2 con 90 boletos, prefería jugar la lotería, de modo yo creo que \\(P(A)<0.90.\\)\nCon estos ejercicios de preferencias sobre apuestas comparando con juegos de azar simples, puedo ver cuál es la probabilidad que yo le asigno a cualquier evento. Es posible demostrar que una probabilidad definida de esta manera cumple todas las reglas de probabilidad que mostramos arriba (si mis decisiones son racionales). Es posible definir modelos de probabilidad con este enfoque subjetivo, y es posible operar con ellos de manera usual con la maquinaria matemática.\nLo poderoso de este enfoque es que nos permite atacar problemas que 1) no está claro cómo podrían derivarse a partir de modelos equiprobables, y 2) no está claro que sea posible reproducir el experimento muchas veces para ver cual es la frecuencia relativa de ocurrencia del evento. Por ejemplo:\n\n¿Cuál es la probabilidad de que esta persona X contraiga una enfermedad particular?\n¿Cuál es la probabilidad de que un candidato Y gane una elección?\n¿Qué tan probable es que el próximo año el crecimiento del PIB sea mayor a 1%?\n\nEn todos estos casos importantes donde tenemos que tomar decisiones, la única alternativa real es pensar en términos de probabilidad subjetiva.\nEn la práctica, este tipo de enfoque se utiliza para poner probabilidades sobre cantidades o aspectos difíciles de medir directamente pero que son relativamente simples, y luego se utilizan modelos y reglas de probabilidad para producir otras probabilidades de interés más complicadas."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "href": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "title": "8  Básicos de probabilidad y simulación",
    "section": "8.8 Probabilidad subjetiva y calibración frecuentista",
    "text": "8.8 Probabilidad subjetiva y calibración frecuentista\nComo podemos imaginarnos, para muchas decisiones importantes, el enfoque subjetivo por sí solo puede no ser suficiente. Por ejemplo, imaginemos que trabajamos en INEGI queremos estimar el ingreso total de los hogares en México. Sería difícil proponer para esta tarea un enfoque puramente subjetivo.\nEn estos caso, podemos usar un enfoque donde utilizamos las probabilidades de manera subjetiva (por ejemplo elicitadas de un pánel de expertos), pero demostramos también que nuestro método tiene propiedades frecuentistas apropiadas: por ejemplo, que nuestros intervalos de 95% de estimación son realmente de 95%. Esto normalmente se cumple 1) Checando supuestos de nuestro modelo 2) Sometiendo nuestro método a distintos escenarios de estimación, y checando que se cumplen estimaciones frecuentistas apropiadas. En realidad, usualmente estos dos últimos pasos, 1 y 2, deben ser llevados a cabo no importa el enfoque de interpretación que utilicemos, pues cualquier método, frecuentista o subjetivo, tiene condiciones bajo las que puede fallar."
  },
  {
    "objectID": "prob-condicional.html",
    "href": "prob-condicional.html",
    "title": "9  Probabilidad condicional e independencia",
    "section": "",
    "text": "Uno los conceptos centrales de la probabilidad es el de probabilidad condicional:\nMuchos de los problemas de esta sección son de (Ross (1998))."
  },
  {
    "objectID": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "href": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.1 Probabilidad condicional en espacios equiprobables.",
    "text": "9.1 Probabilidad condicional en espacios equiprobables.\nSupongamos que en un experimento simétro de \\(n\\) posibles resultados, sabemos que ocurrió el evento \\(F\\), es decir, un conjunto de resultados fijo. ¿Cómo podemos calcular la probabilidad de que ocurra un evento \\(E\\) dado que sabemos que \\(F\\) ocurrió? Esta probabilidad se escribe\n\\[P(E|F)\\]\n\n9.1.1 Ejemplo: dos dados\nSupongamos que tiramos dos dados, y nos dicen que la suma de los dos datos es igual a 6. ¿Cuál es la probablidad condicional de haber tirado al menos un cinco dado que la suma es 6?\nSolución: los resultados equiprobables que resultan en un tiro de suma 6 son \\(F= \\{ (5,1),(4,2),(3,3),(2,4),(1,5)\\}\\), que son 5 posibles resultados. En solamente 2 de ellos tiramos un cinco. Como estos resultados son equiprobables, si \\(E\\) es el evento “tirar al menos un 5”,\n\\[P(E|F) = 2/5\\]\nPodemos formalizar de la siguiente manera: para calcular \\(P(E|F)\\) contamos todos los resultados de \\(F\\) donde también ocurre \\(E\\) y dividimos entre las maneras en que puede ocurrir \\(F\\):\n\nEn nuestro caso hay muchos resultados posibles donde tiramos al menos un 5, por ejemplo \\((5,1), (5,2), (5,6)\\) y así sucesivamente. Sin embargo, solo en 2 de ellos la suma es 5.\n\n\n\n\nEn un espacio de resultados equiprobables, si \\(E\\) y \\(F\\) son eventos, entonces\n\\[P(E|F) = \\frac{n(E\\cap F)}{n(F)},\\]\nes decir, dividimos el número de maneras en que pueden ocurrir \\(E\\) y \\(F\\) simultáneamente entre el número de maneras en que puede ocurrir \\(F\\).\n\n\nNótese que otra manera de ver esta definición es como sigue: una vez que sabemos que ocurrió \\(F\\), restringimos todo nuestro análisis a resultados dentro de \\(F\\), y proseguimos como si se tratara de una probabilidad usual.\n\n\n9.1.2 Ejemplo: dos volados\nSupongamos que tiramos dos volados. Cuál es la probabilidad condicional de que los dos volados sean sol (evento \\(E\\)) dado que 1) El primer volado es sol? 2) Alguno de los dos volados es sol, 3) Los dos volados son águilas?\nHay 2 resultados donde el primer volado es sol (enuméralos), así que la primer probabilidad es \\(P(E|F_1)=1/2\\). Explica por qué la segunda probabilidad condicional es igual a \\(P(E|F_2)=1/3\\). ¿Cuánto vale \\(P(E|F_3)\\)?\n\n\n9.1.3 Ejemplo: tres cartas\nSupongamos que extraemos tres cartas al azar de una baraja de 52 cartas (13 son corazones). Nos muestran que la segunda y la tercera carta son corazones. ¿Cuál es la probabilidad condicional de que la primera sea un corazón?\nSolución: como resultados para las primeras tres cartas seleccionadas tenemos \\((x_1,x_2,x_3)\\). Nos interesan solamente los resultados \\((x_1, corazon_1, corazon_2)\\). Existen \\(13*12*11 + 39*13*12 = 7800\\) resultados posibles (primero contamos los que tienen un corazón al principio, y luego los que no tienen un corazón al principio, de modo que la probabilidad que buscamos es\n\\[\\frac{13(12)(11)}{13(12)(11) + 39(13)(12)} = \\frac{11}{11 + 39} = 0.22\\]\nInterpreta la simplificación de arriba para describir una manera más simple de calcular esta probabilidad condicional."
  },
  {
    "objectID": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "href": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.2 Simulación y probabilidad condicional",
    "text": "9.2 Simulación y probabilidad condicional\nUna manera de aproximar probabilidades condicionales es simulando el experimento que nos interesa, y calculando frecuencias relativas solamente sobre la información que sabemos que ocurrió: es decir, filtramos las simulaciones escogiendo sólo las que son consistentes con la información dada.\n\nEjemplo: simulación tres cartas\n\n\nCódigo\nbaraja <- tibble(numero = 1:13) %>% \n   crossing(tibble(figura = c(\"C\", \"D\", \"T\", \"P\"))) %>% \n   mutate(carta = paste(numero, figura))\nnrow(baraja)\n\n\n[1] 52\n\n\nCódigo\nbaraja\n\n\n# A tibble: 52 × 3\n   numero figura carta\n    <int> <chr>  <chr>\n 1      1 C      1 C  \n 2      1 D      1 D  \n 3      1 P      1 P  \n 4      1 T      1 T  \n 5      2 C      2 C  \n 6      2 D      2 D  \n 7      2 P      2 P  \n 8      2 T      2 T  \n 9      3 C      3 C  \n10      3 D      3 D  \n# … with 42 more rows\n\n\n\n\nCódigo\ncartas <- baraja$carta\nexp_3_cartas <- function(cartas){\n   sample(cartas, 3) \n}\nset.seed(132185)\nexp_3_cartas(cartas)\n\n\n[1] \"7 C\"  \"8 P\"  \"13 T\"\n\n\nSimulamos el experimento\n\n\nCódigo\nsims <- map(1:20000, ~ exp_3_cartas(cartas))\nsims[1:5]\n\n\n[[1]]\n[1] \"4 C\"  \"8 T\"  \"12 D\"\n\n[[2]]\n[1] \"11 D\" \"6 P\"  \"7 C\" \n\n[[3]]\n[1] \"12 C\" \"1 D\"  \"5 C\" \n\n[[4]]\n[1] \"12 D\" \"9 D\"  \"13 T\"\n\n[[5]]\n[1] \"7 T\"  \"13 D\" \"13 T\"\n\n\nSin más información, la probabilidad de corazón en la primera extracción es:\n\n\nCódigo\nsims %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n\n[1] 0.2474\n\n\nQue debe estar alrededor de 1/4. Ahora condicionamos a que las cartas 2 y 3 son corazones:\n\n\nCódigo\nsims_F <- sims %>%\n   keep(~ str_detect(.x[2], \"C\") & str_detect(.x[3], \"C\"))\nlength(sims_F)\n\n\n[1] 1203\n\n\nCódigo\nsims_F[1:5]\n\n\n[[1]]\n[1] \"9 T\"  \"3 C\"  \"12 C\"\n\n[[2]]\n[1] \"5 D\"  \"2 C\"  \"13 C\"\n\n[[3]]\n[1] \"5 C\"  \"10 C\" \"1 C\" \n\n[[4]]\n[1] \"4 P\" \"6 C\" \"1 C\"\n\n[[5]]\n[1] \"8 C\"  \"13 C\" \"11 C\"\n\n\nY sobre estas simulaciones hacemos el mismo cálculo de arriba:\n\n\nCódigo\nsims_F  %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n\n[1] 0.2236076\n\n\nEsto nos da una aproximación de \\(P(E|F)\\). Nótese que si \\(F\\) es un evento con probabilidad baja, entonces será necesario correr más veces el experimento, pues el número de veces que ocurre \\(F\\) es relativamente bajo.\nPodemos definir también la probabilidad condicional en general, para cualquier probabilidad \\(P\\) no necesariamente resultante de un modelo equiprobable:\n\n\n\nLa **probabilidad condicional* del evento \\(E\\) dado que ocurrió el evento \\(F\\) se define como\n\\[P(E|F) = \\frac{P(E y F)}{P(F)}\\]"
  },
  {
    "objectID": "prob-condicional.html#regla-de-la-multiplicación",
    "href": "prob-condicional.html#regla-de-la-multiplicación",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.3 Regla de la multiplicación",
    "text": "9.3 Regla de la multiplicación\nA veces nos interesa calcular la probabilidad de que dos eventos ocurran, y conocemos \\(P(F)\\) y \\(P(E|F)\\). En ese caso podemos usar la definición de probabilidad condicional para escribir al regla del producto:\n\\[P(EF) = P(F)P(E|F)\\]\nPor ejemplo, si queremos calcular la probablidad de extraer dos corazones de una baraja usual, tenemos que \\(P(C_1) = 13/52 = 1/4\\). \\(P(C_2|C_1)\\) es fácil de calcular, pues si la primera carta que sacamos es un corazón, entonces para la segunda extracción hay 51 cartas, de las cuales 12 son corazones, de forma que \\(P(C_2|C_1) = 12/51\\). Usando la regla del producto, quedamos con\n\\[P(C_1C_2) = P(C_1)P(C_2|C_1) = \\frac{13}{52}\\frac{12}{51} \\approx 0.0588\\]\nPodemos generalizar esto a\n\n\n\nRegla del producto\n\\[P(E_1E_2E_3\\cdots E_n) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\cdots P(E_n|E_1\\cdots E_{n-1}\\]\n\n\n\nEjercicio\nSe divide al azar una baraja de 52 de cartas en 4 pilas iguales. Calcula la probabilidad de cada pila tenga exactamente un as.\nPodemos hacer el evento \\(E_1\\) que el as de corazones y el de diamantes están en diferentes pilas, \\(E_2\\) el evento de que el as de corazones, el de diamantes, y el de tréboles están en diferentes pilas, y finalmente \\(E_3\\) el evento de que todos los ases están en diferentes pilas. Nótese que buscamos la probabilidad \\(P(E_3)\\), pero será más fácil si escribimos:\n\\[P(E_3) = P(E_3E_2E_1) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\]\nPrimero, el as de corazones está en alguna de las pilas. La probabilidad de el as de diamantes no esté en esa pila es \\(P(E_1) = 1 - 12/51 = 39/51\\) (¿por qué?), pues la pila que tiene el as de corazones tiene otras 12 cartas de las 51 disponibles. La probabilidad de que el as de diamantes sea una de esas 12 cartas es entonces 12/51.\nSi se cumple \\(E_1\\), entonces el as de corazones y de diamantes están en pilas distintas. Entonces la probabilidad de que el as de tréboles caiga en una de esas dos pilas es \\(24/50\\), así que\n\\[P(E_2|E_1) = 1 -24/50 = 25/50\\]\nFinalmente, si el as de corazones, diamantes y tréboles están en distintas pilas, entonces la probabilidad de que el de espadas caiga en la una de esas pilas es \\(36/49\\), de modo que\n\\[P(E_3|E_2E_1) = 1 - 36/49 = 13/49\\].\nUsando la reglal producto,\n\\[P(E_1E_2E_3) = \\frac{39(26)(13)}{51(50)(49)} \\approx 0.105\\]"
  },
  {
    "objectID": "prob-condicional.html#nota-falacias-probabilísticas",
    "href": "prob-condicional.html#nota-falacias-probabilísticas",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.4 Nota: falacias probabilísticas",
    "text": "9.4 Nota: falacias probabilísticas\nUna de las razones por las que es importante usar la teoría de probabilidad para manipular probabilidades es que las personas, naturalmente, tienden a hacer “cálculos mentales probabilísticos” incorrectos, lo cual a fin de cuentas lleva a decisiones mal informadas. Existe amplia evidencia de esto, ver por ejemplo el trabajo de: Kahneman y Tversky\n“Before their work, economists had gotten far in their analyses of decision making under uncertainty by assuming that people correctly estimate probabilities of various outcomes or, at least, do not estimate these probabilities in a biased way […]. But Kahneman and Tversky found that this is not true: the vast majority of people misestimate probabilities in predictable ways.”\nAhora podemos descutir una de ellas, la falacia de conjunción. Consideremos la siguiente pregunta:\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\n\n¿Qué es más probable?\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement.\n\n¿Qué crees que tiende a escoger la mayoria de las personas como la más probable? ¿Qué regla de probabilidad puedes usar para demostrar que esto es una falacia?"
  },
  {
    "objectID": "prob-condicional.html#regla-de-probablilidad-total",
    "href": "prob-condicional.html#regla-de-probablilidad-total",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.5 Regla de probablilidad total",
    "text": "9.5 Regla de probablilidad total\nUna regla que usaremos varias veces es la regla de probabilidad total, que establece que\n\\[P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\\]\ndonde el evento \\(F^c\\) significa que \\(F\\) no ocurrió.\nEsta regla es útil en muchos casos para calcular probabilidades de un evento dependiendo de la ocurrencia o no de otro.\n\nEjemplo\nSacamos dos cartas de una bajara de 52 cartas. Vimos un ejemplo donde queríamos calcular la probabilidad de \\(N_2=\\) la segunda carta que sacamos es negra. Argumentamos usando espacio equiprobables que \\(P(N_2) = 0.5\\).\nSi \\(N_1 =\\) la primera carta es negra, entonces la ley de probabilidad total explica por qué pasa esto tomando en cuenta el resultado de la primera extracción:\n\nTenemos que \\(P(N_2|N_1) = 25/51\\) y \\(P(N_1) = 1/2\\)\nAdemás, \\(P(N_2|N_1^c) = 26/51\\) y \\(P(N_1^c) = 1/2,\\)\n\nde forma que\n\\[P(N_2) = (25/51)(1/2) + (26/51)(1/2) = \\frac{(25 + 26)}{51(2)} = /2\\]\nLa ley de probabilidades totales consiste de las reglas de ponderación usuales que conocemos.\n\n\nEjemplo\nEn un país hay 20% de adultos de 20 años o menos, y 80% de adultos de 21 años o más. Entre los adultos de 20 años o menos, el 90% solo usa plataformas digitales para ver televisión. Entre los adultos de 21 años o más, el 15% solo usa plataformas digitales para ver televisión. Si tomamos un adulto al azar de esta población, ¿cuál es la probabilidad de que solo use digital para ver TV?\nLa respuesta es\n\\[ 0.90(0.20) + 0.15(0.80) = 0.3\\]"
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.6 Ejercicio: dados y monedas",
    "text": "9.6 Ejercicio: dados y monedas\nSupongamos que tiramos un dado. Tiramos tantos volados como el número que salió en la tirada de dado, y registramos el número de soles. ¿Cuál es la probabilidad de que obtengamos cero soles?\nSea \\(X=\\) número que obtuvimos en la tirada de dado, y sea \\(Y=\\) número de soles obtenidos.\nCalcular directamente \\(P(Y=0)\\) puede hacerse de manera simple con la ley de probabilidad total, pues\n\n\\(P(Y=0|X=1) = 1/2\\), prob de ningún sol en 1 volado\n\\(P(Y=0|X = 2) = 1/4\\) prob de ningún sol en dos volados (suponiendo independencia de los volados)\n\\(P(Y=0|X=3) = 1/8\\), prob de ningún sol en tres volados.\n\ny así sucesivamente. Como \\(P(X=i)=1/6\\) para cualquier número del uno al seis, la probabilidad \\(P(Y=0)\\), usando probabilidad total, es\n\\[(1/6)(1/2) + (1/6)(1/2)^2 +(1/6)(1/2)^3 +(1/6)(1/2)^4 +(1/6)(1/2)^5 +(1/6)(1/2)^6\\]\nque es igual a\n\n\nCódigo\nprobs_x <- rep(1/6, 6)\nprobs_y_x = 0.5^(1:6)\nsum(probs_y_x * probs_x)\n\n\n[1] 0.1640625\n\n\nCheca usando simulación."
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.7 Ejercicio: dados y monedas (simulación)",
    "text": "9.7 Ejercicio: dados y monedas (simulación)\nEste es un experimento más interesante para simular:\n\n\nCódigo\nsimular_soles <- function(){\n   dado <- sample(1:6, 1)\n   soles <- sample(c(\"sol\", \"águila\"), dado, replace = TRUE)\n   num_soles = sum(as.numeric(soles == \"sol\"))\n   num_soles\n}\nset.seed(82332)\nsimular_soles()\n\n\n[1] 3\n\n\nSi hacemos 10 mil simulaciones:\n\n\nCódigo\nsims <- map_dbl(1:10000, ~ simular_soles())\nqplot(sims)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLa frecuencia relativa de cero soles es:\n\n\nCódigo\nsum(sims==0) / length(sims)\n\n\n[1] 0.1655\n\n\nCalcula también la probabilidad de obtener 2 soles o más en este experimento (puedes usar la simulación)."
  },
  {
    "objectID": "prob-condicional.html#regla-de-bayes",
    "href": "prob-condicional.html#regla-de-bayes",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.8 Regla de Bayes",
    "text": "9.8 Regla de Bayes\nLa regla de Bayes es una fórmula que se utiliza para invertir probabilidades condicionales:\n\\[P(E|F) = \\frac{P(F|E)P(E)}{P(F)},\\]\nque es una consecuencia fácil de la definición de probabilidad condicional. Es útil conocerla porque facilita resolver varios problemas de probabilidad que en un principio parecen difíciles.\n\nEjemplo\nSupongamos que tenemos una baraja de 8 cartas, 4 negras y 4 blancas. Extraemos sucesivamente dos cartas a azar. Sea \\(N_1=\\) la primera carta es negra y \\(B_2=\\) la segunda carta es blanca. ¿Cuál es la probabilidad condicional de que la primera carta haya sido negra si nos dicen que la segunda fue blanca?\nAunque ya resolvimos problemas como este, parece confuso en un principio. Sin embargo, podemos calcular:\n\\[P(N_1|B_2) = \\frac{P(B_2|N_1)P(N_1)}{P(B_2)}\\]\nSabemos que \\(P(B_2|N_1) = 4/7\\), y que \\(P(N_1)=1/2\\) y \\(P(B_2) = 1/2\\), de modo que\n\\[P(N_1|B_2) = 4/7 > 1/2\\]\nes decir, si sabemos que la segunda carta fue blanca, eso hace más probable que la primera carta haya sido negra.\n\n\nEjemplo (parte 1)\nSupongamos que una aseguradora cree que hay dos tipos de personas: unos con más riesgo de tener accidentes y otros con menos riesgo. Los datos muestran que una persona con riesgo alto tendrá un accidente en algún momento del año con probabilidad 0.04, y esta probabilidad baja a 0.01 para una persona de riesgo bajo. Si 10% de la población tiene riesgo alto, ¿cuál es la probabilidad de que un asegurado nuevo tenga un accidente un año después de comprar su póliza? (Nota: no sabemos si la persona nueva es de riesgo alto o bajo).\nPuedes resolver son simulación, o usar la ley de probabilidad total. Si \\(A\\) es el evento de tener un accidente, \\(R_A\\) es el evento de que la persona es de riesgo alto y \\(R_B\\) es el evento que la persona es de riesgo bajo, entonces\n\\[P(A) = P(A|R_A)P(R_A) + P(A|R_B)P(R_B)\\]\npues \\(R_A\\) y \\(R_B\\) cubren todas las posibilidades. Entonces\n\\[P(A) = 0.04(0.10) + (0.01)(0.90) = 0.004 + 0.009 = 0.013\\]\nEs decir, su probabilidad es de 1.3% de tener una accidente en el primer año.\n\n\nEjemplo (parte 2)\nAhora vemos que un cliente tuvo un accidente en su primer año. ¿Cuál es la probabilidad de que sea un cliente de riesgo alto?\nLa pregunta es de probabilidad condicional, porque ya tenemos información. Queremos calcular\n\\[P(R_A| A)\\]\nSi usamos la regla de Bayes obtenemos\n\\[P(R_A|A)= \\frac{P(A|R_A)P(R_A)}{P(A)}\\]\nSustituimos los datos que tenemos\n\\[P(R_A|A)= \\frac{0.04(0.10)}{P(A)}\\]\ny \\(P(A)\\) que calculamos en el ejercicio anterior:\n\\[P(R_A|A)= \\frac{0.04(0.10)}{0.013} \\approx 0.3077\\]\nDe manera que al principio la probabilidad no condicionada de ser de alto riesgo era de 10%, y cuando tiene un accidente esta probabilidad se triplica.\n\n\nEjemplo\nSupongamos que en un concurso de TV tenemos tres puertas y debemos escoger una. Atrás de una de ellas hay un premio, y no hay nada detrás de las otras dos. Escogemos una de las puertas.\nAhora el conductor (que sabe dónde está el premio), abre una puerta vacía, y nos pregunta si queremos cambiar o no de puerta. ¿Cuál es la mejor estrategia, cambiar o quedarnos con la que escogimos al principio?\nVeamos la estrategia de quedarnos con la puerta que escogimos. Sin perdida de generalidad, suponemos que escogemos la puerta 1.\nAhora observamos que el conductor abre la puerta 2.\nSea \\(E_1=\\) el premio está en la puerta 1, y \\(A_2=\\) el conductor abre la puerta 2, donde no hay premio. Por la regla de bayes:\n\\[P(E_1 | A_2) = \\frac{P(A_2 | E_1) P(E_1)}{P(A_2)}\\]\nSabemos que \\(P(E_1)= 1/3\\), y que \\(P(A_2|E_1)=1/2\\) (pues el conductor pudo abrir la puerta 2 o 3). Adicionalmente\n\\[P(A_2) = P(A_2|E_1)P(E_1) + P(A_2|E_1^c)P(E_1^c) = (1/2)(1/3) + (1/2) (2/3) = 1/2\\]\npues \\(P(A_2|E_1^c) = 1/2\\), es decir, si el premio no está en la puerta 1, la probabilidad de que abrir la puerta 2 es la probabilidad de que el premio esté en la puerta 2 dado que no está en la puerta 1\nSustituyendo,\n\\[P(E_1 | A_2) = \\frac{(1/2)(1/3)}{1/2} = 1/3\\]\nAsi que si cambiamos, la probabilidad de ganar es de 2/3.\nSimula para verificar tus resultados."
  },
  {
    "objectID": "prob-condicional.html#independencia",
    "href": "prob-condicional.html#independencia",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.9 Independencia",
    "text": "9.9 Independencia\nCuando tenemos dos eventos, y tenemos que \\(P(E|F) > P(E)\\) o \\(P(F|E) > P(F)\\) (checa que una implica la otra), decimos que los eventos tienen dependencia positiva: cuando sabemos que uno ocurre, la probabiidad del otro aumenta.\nEn algunos casos \\(P(E|F) = P(E)\\) y \\(P(F|E) = P(F)\\), lo cual sucede cuando \\(P(EF)=P(F)P(E)\\). En este caso decimos que los eventos \\(E\\) y \\(F\\) son independientes. Nótese que esto no quiere decir que no haya ninguna conexión entre \\(E\\) y \\(F\\) (puede ser que la ocurrencia de \\(F\\) cambie las maneras en que puede ocurrir \\(E\\)), sólo que la probabilidad de uno no cambia al condicionar al otro.\n\nEjemplos\n\nMuestra que si sabemos que sacar un corazón de una baraja de 52 cartas es independiente de sacar un as\nMuestra que en nuestro modelo equiprobable de dos volados, el resultado de un volado es independiente del resultado del otro.\nEl evento “la suma de la tirada de dos dados” es independiente del resultado del primer dado."
  },
  {
    "objectID": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "href": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.10 Independencia de más de dos eventos",
    "text": "9.10 Independencia de más de dos eventos\nNótese que cuando los eventos \\(E\\) y \\(F\\) son indpendientes, por definición\n\\[P(E \\,y\\, F) = P(E) P(F)\\]\nDecimos que los eventos \\(E\\), \\(F\\) y \\(G\\) son independientes cuando\n\n\\(P(E \\,y \\,F \\, y \\, G) = P(E)P(F)P(G)\\)\n\\(P(E \\,y \\,F ) = P(E)P(F)\\)\n\\(P(E \\,y \\, G) = P(E)P(G)\\)\n\\(P(F \\, y \\, G) = P(F)P(G)\\)\n\ny así sucesivamente para un número mayor de eventos: si los eventos son independientes, la probabilidad de que ocurran cualquier subconjunto de ellos es es el producto de la probabilidades de que cada uno de ellos ocurra.\nEn general, si \\(E_1, E_2, \\ldots, E_n\\) son eventos independientes, entonces\n\\[P(E_1\\, y \\, E_2 \\, y \\cdots \\,y E_n) = P(E_1)P(E_2)\\cdots P(E_n)\\]\n\nEjercicio\nHacemos un número indefinido de pruebas independientes, y cada una de ellas puede resultar en éxito con probabilidad \\(p\\) y fracaso con probabilidad \\(1-p\\). Calcula la probabilidad de que 1) al menos un éxito suceda en la primeras 20 pruebas. 2) todas las pruebas sean éxito y 3) exactamente 5 de las 2o pruebas sean éxito.\n\n\nEjemplo: número de seises\nConstruye un modelo para el número de seises en dos tiradas de dados. Escribimos \\(X\\)= número de seises que obtenemos en dos tiradas de dado.\nLos resultados posibles son 0, 1 y 2 seises. Para calcular la probablidad de tirar dos seises hacemos:\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2 | S_1) P(S_1).\\]\nahora, si suponemos que el primer tiro no afecta de ninguna manera el resultado del segundo tiro, entonces \\(P(S_2|S_1) = P(S_2)\\), y la fórmula es\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2) P(S_1) = (1/6)(1/6) = 1/36.\\]\nUsando el mismo argumento podemos calcular de la probabilidad de obtener ningún seis es\n\\[P(X=0) = P(S_2^c \\, y \\, S_1^c ) = P(S_2^c)P(S_1^c) = (5/6)(5/6) = 25/36\\]\nLa probabilidad restante se puede calcularse directamente, o notar que como 0, 1 y 2 son los únicos posibles resultados, entonces\n\\[P(X=1) = 1- P(X=0) - P(X=2) = 1 - 25/36 - 1/36 = 10/36\\]\nCheca tus resultados usando simulación.\nObservación: en muchos casos, la independencia se construye como un supuesto para construir modelos más complejos, cuando este supuesto es adecuado. Un ejemplo es cuando tomamos muestras de una población: si tomamos cada muestra independientemente de las otras, analizar los resultados es mucho más fácil que cuando hay esquemas complejos de dependencias entre los datos que xtraemos."
  },
  {
    "objectID": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "href": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "title": "9  Probabilidad condicional e independencia",
    "section": "9.11 Intro a estimación por máxima verosimilitud",
    "text": "9.11 Intro a estimación por máxima verosimilitud\nUno de los procedimientos más estándar para estimar cantidades desconocidas es el método de máxima verosimilitud. Los estimadores de máxima verosimilitud tienen propiedades convenientes, y dan en general resultados razonables siempre y cuando los supuestos sean razonables.\nMáxima verosimilitud es un proceso intuitivo, y consiste en aprender o estimar valores de parámetros desconocidos suponiendo para los datos su explicación más probable. Para esto, usando supuestos y modelos requeriremos calcular la probabilidad de un conjunto de observaciones.\n\nEjemplo 1\nSupongamos que una máquina produce dos tipos de bolsas de 25 galletas: la mitad de las veces produce una bolsa con 5 galletas de avena y 20 de chispas de chocolate, y la otra mitad produce galletas con 23 galletas de avena y 2 de chispas de chocolate.\nTomamos una bolsa, y no sabemos qué tipo de bolsa es (parámetro desconocido). Extraemos al azar una de las galletas, y es de chispas de chocolate (observación).\nPor máxima verosimilitud, inferimos que la bolsa que estamos considerando tiene 5 galletas de avena. Esto es porque es más probable observar una galleta de chispas en las bolsas que contienen 5 galletas de avena que en las bolsas que contienen 23 galletas de avena. Podemos cuantificar la probabilidad que “acertemos” en nuestra inferencia.\n\nCómo se aprecia en el ejemplo anterior, el esquema general es:\n\nExiste un proceso del que podemos obtener observaciones de algún sistema o población real.\nTenemos un modelo probabilístico que dice cómo se producen esas observaciones a partir del sistema o población real.\nUsualmente este modelo tiene algunas cantidades que no conocemos, que rigen el proceso y cómo se relaciona el proceso con las observaciones.\n\nNuestro propósito es:\n\nExtraemos observaciones del proceso:\n\n\\[x_1, x_2, \\ldots, x_n\\]\n\nQueremos aprender de los parámetros desconocidos del proceso para calcular cantidades de interés acerca del sistema o población real\n\nEn principio, los modelos que consideramos pueden ser complicados y tener varias partes o parámetros. Veamos primero un ejemplo clásico con un solo parámetro, y cómo lo resolveríamos usando máxima verosimilitud.\nNota: Cuando decimos muestra en general nos referimos a observaciones independientes obtenidas del mismo proceso (ver la sección anterior para ver qué significa que sea independientes). Este esquema es un supuesto que simplifica mucho los cálculos, como discutimos antes. Muchas veces este supuesto sale del diseño de la muestra o del estudio, pero en todo caso es importante considerar si es razonable o no para nuestro problema particular.\n\n\nEjemplo\nSupongamos que queremos saber qué proporción de registros de una base de datos tiene algún error menor de captura. No podemos revisar todos los registros, así que tomamos una muestra de 8 registros, escogiendo uno por uno al azar de manera independiente. Revisamos los 8 registros, y obtenemos los siguientes datos:\n\\[x_1 = 0, x_2 = 1, x_3 = 0, x_4 = 0, x_5 =1, x_6 =0, x_7 =0, x_8 =0\\]\ndonde 1 indica un error menor. Encontramos dos errores menores. ¿Cómo estimamos el número de registros con errores leves en la base de datos?\nYa sabemos una respuesta razonable para nuestro estimador puntual, que sería \\(\\hat{p}=2/8=0.25\\). Veamos cómo se obtendría por máxima verosimilitud.\nSegún el proceso con el que se construyó la muestra, debemos dar una probabilidad de observar los 2 errores en 8 registros. Supongamos que en realidad existe una proporción \\(p\\) de que un registro tenga un error. Entonces calculamos\nProbabilidad de observar la muestra:\n\\[P(X_1 = 0, X_2 = 1, X_3 = 0, X_4 = 0, X_5 =1, X_6 =0, X_7 =0, X_8 =0)\\]\nes igual a\n\\[P(X_1 = 0)P(X_2 = 1)P(X_3 = 0)P( X_4 = 0)P(X_5 =1)P(X_6 =0)P(X_7 =0)P(X_8 =0)\\]\npues la probabilidad de que cada observación sea 0 o 1 no depende de las observaciones restantes (la muestra se extrajo de manera independiente).\nEsta ultima cantidad tiene un parámetro que no conocemos: la proporcion \\(p\\) de registros con errores. Así que lo denotamos como una cantidad desconocida \\(p\\). Nótese entonces que \\(P(X_2=1) = p\\), \\(P(X_3=0) = 1-p\\) y así sucesivamente, así que la cantidad de arriba es igual a\n\\[p(1-p)p(1-p)(1-p)p(1-p)(1-p)(1-p) \\]\nque se simplifica a\n\\[ L(p) = p^2(1-p)^6\\]\nAhora la idea es encontrar la p que maximiza la probabilidad de lo que observamos. En este caso se puede hacer con cálculo, pero vamos a ver una gráfica de esta función y cómo resolverla de manera numérica.\n\n\nCódigo\nverosimilitud <- function(p){\n  p^2 * (1-p)^6\n}\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosimilitud))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.25, color = \"red\") +\n  xlab(\"p\")\n\n\n\n\n\nNótese que esta gráfica:\n\nDepende de los datos, que pensamos fijos.\nCuando cambiamos la \\(p\\), la probabilidad de observar la muestra cambia. Nos interesa ver las regiones donde la probabilidad es relativamente alta.\nEl máximo está en 0.25.\nAsí que el estimador de máxima verosimilitud es \\(\\hat{p} = 0.25\\)\n\n\nObsérvese que para hacer esto usamos:\n\nUn modelo teórico de cómo es la población con parámetros y\nInformación de como se extrajo la muestra,\n\ny resolvimos el problema de estimación convirtiéndolo en uno de optimización.\nProbamos esta idea con un proceso más complejo:\n\n\n9.11.1 Ejemplo 2\nSupongamos que una máquina puede estar funcionando correctamente o no en cada corrida. Cada corrida se producen 500 productos, y se muestrean 10 para detectar defectos. Cuando la máquina funciona correctamente, la tasa de defectos es de 3%. Cuando la máquina no está funcionando correctamente la tasa de defectos es de 10%\nSupongamos que escogemos al azar 11 corridas, y obervamos los siguientes datos de número de defectuosos:\n\\[1, 0, 0, 3 ,0, 0, 0, 2, 1, 0, 0\\]\nLa pregunta es: ¿qué porcentaje del tiempo la máquina está funcionando correctamente?\nPrimero pensemos en una corrida. La probabilidad de observar una sucesión particular de \\(r\\) defectos es\n\\[0.03^r(0.97)^{(10-r)}\\]\ncuando la máquina está funcionando correctamente.\nSi la máquina está fallando, la misma probabilidad es\n\\[0.2^r(0.8)^{(10-r)}\\]\nAhora supongamos que la máquina trabaja correctamente en una proporción \\(p\\) de las corridas. Entonces la probabilidad de observar \\(r\\) fallas se calcula promediando (probabilidad total) sobre las probabilidades de que la máquina esté funcionando bien o no:\n\\[0.03^r(0.97)^{(10-r)}p + 0.2^r(0.8)^{(10-r)}(1-p)\\]\nY esta es nuestra función de verosimilitud para una observación.\nSuponemos que las \\(r_1,r_2, \\ldots, r_{11}\\) observaciones son independientes (por ejemplo, después de cada corrida la máquina se prepara de una manera estándar, y es como si el proceso comenzara otra vez). Entonces tenemos que multiplicar estas probabilidades para cada observación \\(r_1\\):\n\n\nCódigo\ncalc_verosim <- function(r){\n  q_func <- 0.03^r*(0.97)^(10-r)\n  q_falla <- 0.2^r*(0.8)^(10-r)\n  function(p){\n    #nota: esta no es la mejor manera de calcularlo, hay \n    # que usar logaritmos.\n    prod(p*q_func + (1-p)*q_falla)\n  }\n}\nverosim <- calc_verosim(c(1, 0, 0, 3, 0, 0, 0, 2, 1, 0, 0))\nverosim(0.1)\n\n\n[1] 2.692087e-14\n\n\n\n\nCódigo\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosim))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.8, color = \"red\") +\n  xlab(\"prop funcionado\")\n\n\n\n\n\nY nuestra estimación puntual sería de alrededor de 80%.\n\n\nAspectos numéricos\nCuando calculamos la verosimilitud arriba, nótese que estamos multiplicando números que pueden ser muy chicos (por ejemplo \\(p^6\\), etc). Esto puede producir desbordes numéricos fácilmente. Por ejemplo para un tamaño de muestra de 1000, podríamos tener que calcular\n\n\nCódigo\np <- 0.1\nproba <- (p ^ 800)*(1-p)^200\nproba\n\n\n[1] 0\n\n\nEn estos casos, es mejor hacer los cálculos en la escala logarítmica. El logaritmo convierte productos en sumas, y baja exponentes multiplicando. Si calculamos en escala logaritmica la cantidad de arriba, no tenemos problema:\n\n\nCódigo\nlog_proba <- 800 * log(p) + 200 * log(1-p)\nlog_proba\n\n\n[1] -1863.14\n\n\nAhora notemos que\n\nMaximizar la verosimilitud es lo mismo que maximizar la log-verosimilitud, pues el logaritmo es una función creciente. Si \\(x_{max}\\) es el máximo de \\(f\\), tenemos que \\(f(x_{max})>f(x)\\) para cualquier \\(x\\), entonces tomando logaritmo, \\[log(f(x_{max}))>log(f(x))\\] para cualquier \\(x\\), pues el logaritmo respeta la desigualdad por ser creciente.\nUsualmente usamos la log verosimilitud para encontrar estimador de máxima verosimilitud\nHay razónes teóricas y de interpretación por las que también es conveniente hacer esto.\n\n\n\n\n\nRoss, Sheldon M. 1998. A First Course in Probability. Fifth. Upper Saddle River, N.J.: Prentice Hall."
  },
  {
    "objectID": "modelos-continuos.html",
    "href": "modelos-continuos.html",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "",
    "text": "En la parte anterior consideramos un númeroo fijo de resultados numéricos de experimentos aleatorios, por ejemplo, cuando \\(X\\) el resultado de una tirada de dado. En este caso, un modelo de probabilidad para \\(X\\) asigna una probabilidad dada a cada posible resultado, por ejemplo\n\\[P(X=1) = 1/6\\]\ne igualmente \\(P(X=2)=\\cdots = P(X=6) = 1/6\\). En muchos casos, la cantidad \\(X\\) que nos interesa puede tomar valores numéricos arbitrarios, y en este esquema no está claro cómo asignaríamos probabilidades."
  },
  {
    "objectID": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "href": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.1 Modelo equiprobable o uniforme",
    "text": "10.1 Modelo equiprobable o uniforme\nLos modelos más simple para una medición continua \\(X\\) son los modelos uniforme.\nPara nuestra ruleta, por ejemplo, \\(X\\) puede tomar valores en el intervalo \\([0, 360)\\). Si la ruleta es justa, entonces la probabilidad de que la flecha caiga en cualquier sector \\([a,b]\\) debe ser igual. Una manera de lograr esto usar como probabilidad la proporción de la longitud de \\([a,b]\\) con respecto al total de \\([0, 360)\\):\n\\(P(X\\in [a,b]) = \\frac{b-a}{360}.\\)\n\nDiscute por qué esta asignación de probabilidades satisface las tres reglas básicas de probabilidad (axiomas) que presentamos anteriormente.\nEste es el equivalente continuos para espacios equiprobables con un número finito de resultados.\n\n::: {.cell type=‘comentario’}\n\nSupongamos que una variable aleatoria puede tomar valores en el intervalo \\([L,U]\\). La variable aleatoria es uniforme en \\([L,U]\\) cuando\n\\[P(X \\in [a,b]) = \\frac{b-a}{L-U}\\]\n</div>\\EndKnitrBlock{comentario}\n:::\n\n10.1.1 Ejemplo: ruleta sesgada\nAhora supongamos que nuestra ruleta no está del todo balanceada. Por ejemplo, podría ser estuviera colgada en una pared, y al girar la flecha es un poco más probable que la flecha apunte hacia el piso en lugar de hacia el cielo.\nEn este caso, si la dirección hacia arriba es 90 grados y hacia abajo es 270 grados, quisiéramos por ejemplo que\n\\[P(260 < X <280) < P(80 < X < 100)\\]\nY nótese que debe ser posible asignar probabilidades a cualquier sector de la ruleta con el nuevo modelo que propongamos. ¿Cómo podríamos modificar nuestra asignación de probabilidades?\nUna de las maneras más fáciles es pensando que nuestra probabilidad se obtiene integrando una funcion constante:\n\nSi \\([a,b]\\) es un sector de la ruleta con \\(a<b\\), podríamos poner\n\n\\[P(X\\in [a,b]) = \\int_a^b \\frac{1}{360} \\,dx = \\frac{b-a}{360}\\]\nDe forma que si \\(f(x)= 1/360\\) para valores \\(0 \\leq x < 360\\), nuestra probabilidad se escribe como la integral\n\\[P(X\\in [a,b]) = \\int_a^b f(x) \\,dx \\]\n\nEn este caso, probabilidad es área bajo la curva \\(f(x)=1/360\\) que se calcula integrando sobre el intervalo de interés\n\nPara generalizar la idea es la siguiente:\n\nUsamos la fórmula anterior, pero modificamos o perturbamos la función \\(f(x) = 1/360\\) para que \\(f\\) sea un poco más alta alrededor de 270 grados (abajo), y un poco más baja alrededor de 90 grados (arriba).\nLo único que necesitamos es que \\(f(x)\\) no puede tomar valores negativos (por que si no obtendríamos probabilidades negativas en algunos sectores), y la integral sobre la ruleta completa debe ser uno:\n\n\\[P(X\\in [0, 360]) = \\int_0^{360} f(x)\\,dx = 1\\]\nPodríamos utilizar por ejemplo:\n\n\nCódigo\nf_dens <- function(x){\n    x_rad <- 2 * pi * x / 360\n    (1/360) +  0.0002 * cos(x_rad - 3 * pi  / 2)\n}\ngraf_1_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"uniforme\",\n                   f = 1 / 360) \ngraf_2_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"colgada\") %>% \n    mutate(f = f_dens(angulo))\ngraf_tbl <- bind_rows(graf_1_tbl, graf_2_tbl)\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1)\n\n\n\n\n\nEl cálculo se hace ahora con área bajo la curva. Para calcular la probabilidad\n\\[P(X\\in [50, 130]),\\]\nintegramos la función \\(f\\) correspondiente, que corresponde a calcular área bajo la curva:\n\n\nCódigo\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1) +\n    geom_area(aes(x = ifelse(angulo > 50 & angulo < 130, angulo, 0)), \n              fill=\"salmon\", alpha = 0.5)\n\n\n\n\n\nY ahora vemos que para la versión perturbada, más de la probabilidad se concentra alrededor de 270 grados que alrededor de 90. Por las propiedades de la integral, todas las propiedades usuales de probabilidad se cumplen."
  },
  {
    "objectID": "modelos-continuos.html#funciones-de-densidad",
    "href": "modelos-continuos.html#funciones-de-densidad",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.2 Funciones de densidad",
    "text": "10.2 Funciones de densidad\nCuando trabajamos con mediciones de tipo continuo, es más conveniente definir asignaciones de probabilidad utilizando funciones de densidad de probabilidad:\n\n\n\nUna función \\(f(x)\\) no negativa cuya integral es igual a 1 es una función de densidad de probabilidad. Las probabilidades asociadas se calculan integrando:\n\\[P(X\\in [a,b]) = \\int_a^b f(x)\\,dx\\]\nEn este caso decimos que \\(f\\) es la función de densidad de probabilidad asociada a la variable aleatoria \\(X\\). A este tipo de variables aleatorias les llamamos continuas."
  },
  {
    "objectID": "modelos-continuos.html#ejemplo-densidad-triangular",
    "href": "modelos-continuos.html#ejemplo-densidad-triangular",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.3 Ejemplo: densidad triangular",
    "text": "10.3 Ejemplo: densidad triangular\nSupongamos que tenemos una variable aleatoria que tiene mediana 2, y puede tomar valores entre 0 y 4. Podríamos definir una densidad como sigue: Si \\(x\\) está entre 0 y 2, entonces\n\\[f(x) = \\frac{x}{4}\\]\ny si \\(x\\) está entre 2 y 4, entonces\n\\[f(x) = 1 - \\frac{x}{4}\\]\n\n\nCódigo\ndens_triangular <- function(x){\n    (x > 0) * (x < 4) * ifelse(x < 2, x/4, 1 - x/4)\n}\ntriangular_tbl <- tibble(x = seq(-1, 5, 0.001)) %>% \n    mutate(f = dens_triangular(x)) \nggplot(triangular_tbl, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\n\nEjemplo\nSupongamos que una variable \\(X\\) tiene mediana 2 y rango de 0 a 4, con densidad triangular. ¿Cuál es la probabilidad \\(P(X>1)\\)?\nSolución: Por reglas usuales de probabilidad, \\(P(X>1) = P(1<X<2) + P(X\\geq2)\\). Tenemos que \\(P(X\\geq 2) = 0.5\\). Ahora usamos la fórmula de la densidad triangular para obtener\n\\[P(1<X<2) = \\int_{1}^{2} f(x)\\,dx = \\int_1^2 \\frac{x}{4}\\,dx =\n\\left [\\frac{x^2}{8}\\right ]_1^2 = 1/2 - 1/8 = 3/8 = 0.375\\]\nde modo que\n\\[P(X<1) = 0.375 +0.500 = 0.875\\]\nEn general, podemos dar una fórmula para una densidad triangular en el intervalo \\([A,B]\\) con mediana en \\((A + B)/2\\). ¿Cómo sería la fórmula?"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "href": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.4 Cuantiles de variables aleatorias",
    "text": "10.4 Cuantiles de variables aleatorias\nAntes vimos la definición de cuantiles para datos numéricos. Podemos definirlos también para variables aleatorias numéricas:\n::: {.cell type=‘comentario’}\n\nSea \\(p\\in (0,1)\\). El cuantil-\\(p\\) de la variable \\(X\\) con función de densidad \\(f(x)\\) es el valor \\(x(p)\\) tal que\n\\[\\int_{-\\infty}^{x(p)} f(x)\\,dx = p\\]\n</div>\\EndKnitrBlock{comentario}\n:::\nObservación: nótese que usamos como límite inferior \\(-\\infty\\) para indicar que integramos \\(f\\) sobre toda la densidad que esté a la izquierda de \\(x(p)\\).\n\nEjemplo: densidad triangular\nSupongamos que \\(X\\) tiene la densidad triangular mostrada arriba. Calcula el cuartil inferior y superior (es decir, los cuantiles 0.25 y 0.75). Para el cuartil superior, por ejemplo, buscamos al \\(x(0.75)\\) de la siguiente gráfica:\n\n\nCódigo\nsource(\"R/triangular.R\")\nggplot(triangular_tbl, aes(x = x, y = f)) +\n        geom_line() +\n    geom_area(aes(x = ifelse(x > 0 & x < qtri(0.75, 0, 4), x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0, 0.7)) +\n    annotate(\"text\", x = qtri(0.75, 0, 4), y = 0.03, label = \"x(0.75)\") +\n    annotate(\"point\", x = qtri(0.75, 0, 4), y = 0.0) \n\n\n\n\n\nComenzaremos por el cuartil inferior. Buscamos una \\(x(0.25)\\) tal que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = 0.25\\]\nSabemos que \\(x(0.25)< 2\\), pues la integral hasta 2 es 0.5, así que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = \\int_0^{x(0.25)} x/4 \\,dx = \\left [ x^2/8\\right]_0^{x(0.25)} = (x(0.25))^2/8\\]\nSi queremos que este valor sea igual a 0.25, entonces despejando obtenemos\n\\[x(0.25) = \\sqrt{0.25(8)} = \\sqrt{2}\\approx 1.4142\\]\nAhora podríamos calcular la otra integral, pero por simetría podemos concluir que\n\\[x(0.75) = 2 + (2 - 1.4142) \\approx 2.5858\\]\ny concluimos que los cuartiles inferiores y superiores son aproximadamente 1.41 y 2.59\n\n\nEjercicio: densidad uniforme\nCalcula la mediana, y los percentiles 0.10 y 0.90 de una variable uniforme en \\([0, 10]\\)."
  },
  {
    "objectID": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "href": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.5 Comparando cuantiles teóricos y empíricos",
    "text": "10.5 Comparando cuantiles teóricos y empíricos\nLos cuantiles que vimos en la parte de descriptivos para datos numéricos se llaman usualmente cuantiles empíricos. Estos cuantiles podemos compararlos con cuantiles teóricos para ver qué tan similares son, y si el modelo teórico describe adecuadamente los datos.\n\n10.5.1 Ejemplo: distribución uniforme\nSimularemos 500 datos uniformes en \\([0, 10]\\):\n\n\nCódigo\nx_sim_u <- runif(500, 0, 10)\n\n\nPodríamos calcular algunos cuantiles empíricos:\n\n\nCódigo\nquantile(x_sim_u, c(0.10, 0.50, 0.90))\n\n\n      10%       50%       90% \n0.8740116 4.7480174 8.7635375 \n\n\nPor el ejercicio anterior sabemos cuáles son los cuantiles teóricos correspondientes a una uniforme en \\([0,10]\\). Podemos calcularlos también como sigue:\n\n\nCódigo\nqunif(c(0.10, 0.5, 0.90), 0, 10)\n\n\n[1] 1 5 9\n\n\nY vemos que son muy similares los cuantiles empíricos y teóricos. Una mejor manera de considerar esta similitud es graficando todos los cuantiles empíricos y comparándolos con los teóricos:\n\n\nCódigo\nggplot(tibble(x = x_sim_u), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = stats::qunif, dparams = list(min = 0, max = 10)) +\n    xlab(\"Cuantiles Teóricos U(0,10)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\n\nY vemos que la forma de las dos distribuciones es muy similar: los cuantiles empíricos son muy similares a los teóricos. Existen algunas fluctuaciones debidas al muestreo aleatorio.\n\n\n10.5.2 Ejemplo: distribución triangular\nRepetimos para la distribución triangular. Los cuantiles que calculamos arriba son:\n\n\nCódigo\nqtri(c(0.25, 0.75), a = 0, b = 4)\n\n\n[1] 1.414214 2.585786\n\n\n\n\nCódigo\nx_sim_tri <- rtri(500, 0, 4)\nggplot(tibble(x = x_sim_tri), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = qtri, dparams = list(a = 0, b = 4)) +\n    xlab(\"Cuantiles Teóricos triangular(0,4)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\n\nNótese que otra vez, los cuantiles teóricos se alinean bien con los teóricos."
  },
  {
    "objectID": "modelos-continuos.html#histogramas-y-densidades",
    "href": "modelos-continuos.html#histogramas-y-densidades",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.6 Histogramas y densidades",
    "text": "10.6 Histogramas y densidades\nPara el análisis de datos usual, las gráficas cuantil-cuantil tienden a ser útiles para entender si unos datos se comportan según alguna densidad teórica. Sin embargo, muchas veces se usan histogramas, como en las siguientes gráficas:\n\n\nCódigo\nhist_1 <- ggplot(tibble(x = x_sim_u),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 10, 0.01)) %>% \n                  mutate(f = dunif(x, 0, 10)),\n              aes(x = x, y = f), colour = \"red\")\nhist_2 <- ggplot(tibble(x = x_sim_tri),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.25, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 4, 0.01)) %>% \n                  mutate(f = dtri(x, 0, 4)),\n              aes(x = x, y = f), colour = \"red\")\nhist_1 + hist_2\n\n\n\n\n\nNótese la escala vertical de estos histogramas, que no es simplemente el conteo de casos que caen en cada intervalo del histograma. Para poder comparar los conteos con las densidades correspondientes, es necesario observar lo siguiente:\nSi \\(I = [a,b]\\) es un intervalo del histograma, según la densidad (teórica), la probabilidad de que un dato \\(x\\) caiga en \\(I\\) es\n\\[P(x\\in I) = \\int_{a}^b f(x)\\,dx \\approx f(a) (b-a)\\]\nLa última aproximación se debe a que en un intervalo chico \\([a,b]\\), el área bajo la curva de \\(f(x)\\) es aproximadamente igual a la base (el ancho del intervalo) por la altura en un punto de la curva (\\(f(a)\\), aunque también podríamos usar \\(f(\\frac{a+b}{2})\\) por ejemplo).\nSi tenemos \\(n\\) observaciones, esperamos entonces que caigan \\(nP(X\\in I)\\) en el intervalo \\(I\\), de forma que si \\(n([a,b])\\) es el número de observaciones que caen en \\(I = [a,b]\\), esperamos\n\\[\\frac{n([a,b])}{n} \\approx f(a)(b-a),\\]\ny despejando obtenemos\n\\[f(a)\\approx \\frac{n([a, b])}{n(b-a)}.\\]\nEsto implica que para aproximar la densidad, es necesario dividir las frecuencias relativas entre el ancho de los intervalos correspondientes, y de ahí la escala vertical de las gráficas de arriba.\nObservación: las gráficas de cuantiles son generalmente más prácticas para evaluar el ajuste a un modelo teórico, aunque son menos comunes."
  },
  {
    "objectID": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "href": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.7 Más descriptivos: media y desviación estándar",
    "text": "10.7 Más descriptivos: media y desviación estándar\nPodemos utilizar cuantiles para describir modelos teóricos y también conjuntos de datos (por ejemplo, mediana para centralidad y diferencia entre cuantiles 0.9 y 0.10 para dispersión), y funcionan bien en general. Sin embargo, para modelos teóricos y conjuntos de datos particulares muchas veces es más conveniente usar medidas como la media y desviación estándar.\n\n10.7.1 Media teórica y empírica\nSabemos que la media de un conjunto de datos \\(x_1,x_2,\\ldots,x_n\\) está dada por\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots x_n}{n}.\\]\nAhora consideremos qué pasa con esta cantidad cuando las \\(x_i's\\) son observaciones independientes de una distribución con densidad teórica \\(f(x)\\). Utilizaremos simulaciones de la distribución triangular que vimos arriba\n\n\nCódigo\ndatos <- tibble(x = rtri(5000, 0, 4))\nggplot(datos, aes(x = x)) + stat_bin(breaks = seq(0, 4, 0.25))\n\n\n\n\n\nPodemos aproximar la media de estos datos promediando los valores iniciales de los intervalos de corte ponderado por el número de casos. Lo hacemos así:\n\n\nCódigo\nparticion <- seq(0, 4, 0.25)\niniciales <- head(particion, -1) # quitar último\nagrupados_cubeta <- datos %>% \n  mutate(inicial = cut(x, breaks = particion, labels = iniciales)) %>%  \n  mutate(inicial = as.numeric(as.character(inicial))) %>% \n  group_by(inicial) %>%\n  count() %>% \n  ungroup() \nagrupados_cubeta\n\n\n# A tibble: 16 × 2\n   inicial     n\n     <dbl> <int>\n 1    0       40\n 2    0.25   121\n 3    0.5    212\n 4    0.75   273\n 5    1      360\n 6    1.25   439\n 7    1.5    488\n 8    1.75   575\n 9    2      558\n10    2.25   507\n11    2.5    439\n12    2.75   354\n13    3      269\n14    3.25   210\n15    3.5    122\n16    3.75    33\n\n\nY calculamos la aproximación a la media como sigue:\n\n\nCódigo\nagrupados_cubeta %>% \n  summarise(media_aprox = sum(inicial * n) / sum(n))\n\n\n# A tibble: 1 × 1\n  media_aprox\n        <dbl>\n1        1.87\n\n\nApliquemos esta idea cuando tenemos una densidad \\(f(x)\\). Dividimos el rango de la densidad en cubetas, y aproximamos la densidad por rangos, por ejemplo:\n\n\nCódigo\nparticion <- seq(0, 4, 0.25)\nvalor <- dtri(particion, 0, 4)\napprox_tbl <- tibble(x = particion, densidad = valor)\ndensidad_tri <- tibble(x = seq(0, 4, 0.01)) %>% \n  mutate(densidad = dtri(x, 0, 4))\nggplot(densidad_tri) + \n  ylab('f(x)') + \n  geom_line(aes(x = x, y= densidad), alpha = 0.8) +\n  geom_step(data = approx_tbl, aes(x = x, y = densidad), colour = \"red\") +\n  theme_minimal() \n\n\n\n\n\nY repetimos el mismo proceso: ponderamos los valores iniciales por la altura de la densidad:\n\n\nCódigo\napprox_tbl %>% \n  summarise(media_approx = sum(x * densidad) / sum(densidad))\n\n\n# A tibble: 1 × 1\n  media_approx\n         <dbl>\n1            2\n\n\nY esta es una aproximación a la media de esta distribución.\nNótese que la cantidad que estamos calculando es\n\\[\\sum_i x_i f(x_i) \\Delta\\]\ndonde \\(\\Delta\\) es igual a\n\n\nCódigo\napprox_tbl %>% summarise(suma_densidad = 1 / sum(densidad))\n\n\n# A tibble: 1 × 1\n  suma_densidad\n          <dbl>\n1          0.25\n\n\nque es el ancho del intervalo de las particiones. Recordamos por cálculo que esta es una la aproximación a la siguiente integral:\n\\[\\sum_i x_i f(x_i) \\Delta \\approx  \\int xf(x)\\,dx\\]\nDe modo que para pasar de media de los datos \\(\\bar{x}\\) a la media de \\(\\mu_f\\) de una distribución la equivalencia es:\n$$\n{x} = x_i xf(x) = \n$$ donde\n\nUsamos la densidad en lugar de frecuencias relativas para ponderar\nUsamos integral en lugar de suma\n\nEs decir, cuando tenemos una densidad teórica continua, es necesario integrar en lugar de sumar para calcular su media.\n\n\n10.7.2 Varianza y desviación estándar\nOtra cantidad importante es la varianza de una muestra. Es una medida de dispersión, y se calcula como\n$$\n^2 = _{i=1}^n (x_i - {x})^2\n$$\nNótese que cuando los datos están altamente concentrados alrededor del valor de la media, la varianza es chica, y cuando hay más dispersión alrededor de la media, la varianza es grande.\nLa desviación estándar es la raíz cuadrada de esta cantidad:\n$$\n = \n$$ que tiene la ventaja de que está en las mismas unidades que la variable original.\nSiguiendo el mismo patrón que arriba, tenemos que integrar en lugar de sumar, y ponderar por la densidad en lugar de la frecuencia: El equivalente en una distribución teórica es también una integral, y la varianza está definida por\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x) \\,dx\\]\nLa desviación estándar \\(\\sigma\\) es la raíz cuadrada de esta cantidad.\n\\[\\sigma = \\sqrt{\\int (x - \\mu)^2 f(x)\\,dx}\\]\nEn resumen la densidad indica la frecuencia relativa de datos que esperamos observar alrededor de cada punto, y\n\nResúmenes de una distribución teórica (como cuantiles, media, varianza, etc.) se calculan integrando ponderado por la densidad.\nResúmenes de una distribución empírica se calculan sumando ponderando por la frecuencia relativa.\n\n\n\nNotación\nPara la media de una variable aleatoria \\(X\\) con densidad \\(f\\), utilizamos la notación siguiente:\n\\[\\mu = \\int x f(x)\\,dx = E(X)\\]\ny decimos también que \\(E(X)\\) es el valor esperado de \\(X\\). Igualmente, la varianza podemos escribirla como el valor esperado de la variable \\((X-\\mu)^2\\):\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x)  = E\\left ( (X-\\mu)^2 \\right)\\]\nAunque esto requiere de un teorema simple (el teorema del estadístico inconsciente) que establece que para cualquier función \\(h\\), si \\(Y=h(X)\\) y \\(f\\) es la función de densidad de \\(X\\), entonces\n\\[E(Y) = E(h(X)) = \\int h(x) f(x) \\,dx\\]\n\n\n10.7.3 ¿Cuándo usar media y desviación estándar?\nAlgunos modelos probabilísticos son más fáciles de tratar analíticamente usando media y varianza. En esos casos, conviene usar estas medidas. Esto es especialmente cierto cuando las distribuciones son simétricas y no tienen colas muy largas.\nEn cuanto a datos observados, conviene usar media y varianza cuando pretendemos modelarlos con densidades como las del párrafo anterior. En este caso, la interpretación de estos valores se hace a través de la forma de la densidad. Es importante checar (por ejemplo usando gráficas de cuantiles) que esas densidades describen apropiadamente a los datos.\n\n\nEjercicio\n\nCalcula media y desviación estándar para una densidad triangular (0,4) y otra (0,10)\nCalcula media y desviación estándar para una densidad uniforme en (0,4) y otra uniforme en (0, 10)\nContrasta tus resultados. ¿Las medias ocurren en el lugar que esperabas? ¿Qué distribuciones presentan más dispersión de acuerdo a la desviacion estándar?"
  },
  {
    "objectID": "modelos-continuos.html#la-distribución-normal",
    "href": "modelos-continuos.html#la-distribución-normal",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.8 La distribución normal",
    "text": "10.8 La distribución normal\nLa distribución normal es una que aparece naturalmente en al teoría de probabilidad.\n\n10.8.1 Promedio de variables\nConsideremos que tiramos 40 dados justos de 6 lados, y consideramos su promedio \\(\\bar{X}\\) como resultado de nuestro experimento aleatorio. ¿Cómo se ve la distribución de probabilidades de esta variable \\(\\bar{X}\\)?\nComenzamos haciendo simulacion:\n\n\nCódigo\nsimular_bolsa <- function(num_dados = 40){\n    dados <- sample(1:6, num_dados, replace = TRUE)\n    media <- mean(dados)\n    media\n}\nsimular_bolsa()\n\n\n[1] 3.075\n\n\nVeamos cómo se ven los resultados si hacemos este experimento un número grande de veces:\n\n\nCódigo\nset.seed(23)\nsims_dados <- map_dbl(1:10000, ~ simular_bolsa())\nggplot(tibble(resultado = sims_dados), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.1)\n\n\n\n\n\nY notamos una forma de “campana” interesante. Esto se explica porque típicamente tendremos tiros bajos y altos, de modo que muchos resultados de este experimento se concentran alrededor de la media de un dado (3.5). Además, existen fluctuaciones aleatorios, y a veces tenemos un poco más de tiros altos o de tiros bajos, de forma que existe dispersión alrededor de 3.5.\nSin embargo, estas desviaciones de 3.5 no pueden ser muy grandes: por ejemplo, para tener un promedio de 1, todas las tiradas de los 40 dados tendrían que dar 1, y eso es muy poco probable. Igualmente, para que el promedio fuera cercano a 6, la gran mayoría de los 40 dados deberían de dar 6, lo cual otra vez es muy poco probable. Esto explica al menos la forma general de la forma de las colas derecha e izquierda de esta distribución.\nLos dados podrían ser diferentes (por ejemplo, un poco cargados a 1 o 6, o más cargados a valores centrales), y los argumentos de arriba también se cumplirían. Lo que es sorprendente es que, independientemente de cómo sean las particularidades de los dados, la forma analítica de esta distribución que acabamos de mostrar es la misma.\nEsta forma está descrita por la densidad normal estándar:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\]\ncuya gráfica presentamos a continuación:\n\n\nCódigo\ntibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = (1/sqrt(2*pi)) * exp(-x^2 / 2)) %>% \nggplot(aes(x = x, y = f)) + geom_line()\n\n\n\n\n\nA una variable \\(Z\\) que tiene esta densidad le llamamos una variable con distribución normal estándar. Comparemos cuantiles en nuestro ejemplo:\n\n\nCódigo\nggplot(tibble(resultado = sims_dados),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 40 dados\")\n\n\n\n\n\nY notamos que los cuantiles no corresponden, pero el espaciamiento entre los cuantiles de los datos y los teóricos de la normal estándar es el mismo. Quiere decir que estas dos distribuciones tienen la misma forma, aunque estén escaladas y centradas en distintos valores.\nProbemos con promedios de 20 observaciones triangulares en \\((0,1)\\) por ejemplo. El resultado es el mismo:\n\n\nCódigo\nset.seed(23)\nsims_tri <- map_dbl(1:10000, ~ mean(rtri(20, 0 ,1)))\nggplot(tibble(resultado = sims_tri), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\nCódigo\nggplot(tibble(resultado = sims_tri),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 20 triangulares (0,1)\")\n\n\n\n\n\nOtra vez, la forma general es la misma, aún cuando los datos están centrados y escalados de manera distinta."
  },
  {
    "objectID": "modelos-continuos.html#la-densidad-normal-estándar",
    "href": "modelos-continuos.html#la-densidad-normal-estándar",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.9 La densidad normal estándar",
    "text": "10.9 La densidad normal estándar\nComo expicamos, la densidad normal estándar está dada por\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}},\\]\ncuya gráfica es como sigue:\n\n\nCódigo\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\nLas probabilidades asociadas a una normal estándar se calculan integrando esta curva (que tiene que hacerse de forma numérica). Por ejemplo, para calcular\n\\[P(Z < 1.5),\\]\npodemos usar\n\n\nCódigo\npnorm(1.5)\n\n\n[1] 0.9331928\n\n\nQue es el área bajo la curva mostrada abajo:\n\n\nCódigo\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.005)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line() +\n    geom_area(aes(x = ifelse(x > -3.5 & x < 1.5, x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0,0.4)) +\n    scale_x_continuous(breaks = seq(-3.5, 3.5, 0.5))\n\n\n\n\n\nEsta es la forma de la densidad estándar. Podemos centrar esta campana en otro valor \\(\\mu\\) y aumentar la dispersión por un factor \\(\\sigma\\). Si \\(Z\\) es una variable normal estándar, la variable\n\\[X = \\mu + \\sigma Z\\]\nes una variable normal con parámetros \\((\\mu, \\sigma)\\), o de manera más compacta, decimos que \\(X\\) es \\(N(\\mu, \\sigma)\\). La distribución normal estándar es \\(N(0,1)\\).\nPor ejemplo, si escogemos \\(\\mu=5\\) y \\(\\sigma = 0.5\\), obtenemos:\n\n\nCódigo\nnormal_graf <- tibble(x = seq(3, 7, 0.01)) %>% \n    mutate(f = dnorm(x, 5, 0.5))\nggplot(normal_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\nPodemos mostrar juntas estas dos distribuciones:\n\n\nCódigo\ndensidad_normal <- tibble(x = seq(3, 7, 0.1)) %>% \n  mutate(densidad = dnorm(x, 5, 0.5))\ndensidad_normal_estandar <- tibble(x = seq(-3, 3, 0.1)) %>% \n  mutate(densidad = dnorm(x))\ng_2 <- ggplot(densidad_normal_estandar, aes(x = x, y = densidad)) + geom_line()\ng_3 <- g_2 + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_1 <- ggplot(densidad_normal, aes(x = x, y = densidad)) + geom_line() + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_3 + g_1\n\n\n\n\n\nSe puede demostrar que:\n\n\n\nDistribución normal\n\nLa distribución normal estándar \\(N(0,1)\\) tiene media 0 y desviación estándar 1\nLa distribución normal \\(N(\\mu,\\sigma)\\) tiene media \\(\\mu\\) y desviación estándar \\(\\sigma\\)"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "href": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.10 Cuantiles y concentración de la densidad normal",
    "text": "10.10 Cuantiles y concentración de la densidad normal\nCon un poco de cálculo podemos ver qué tan fuertemente se concentra la densidad alrededor de la media para una distribución normal. La regla es la siguiente:\n\n68% de la densidad se concentra en el intervalo \\([\\mu-\\sigma, \\mu+\\sigma]\\)\n95% de la densidad se concentra en el intervalo \\([\\mu-2\\sigma, \\mu+2\\sigma]\\)\n99.7% de la densidad se concentra en el intervalo \\([\\mu-3\\sigma, \\mu+3\\sigma]\\)\n\n\n\nCódigo\ngrafica_concentracion <- function(mu, sigma, z){\n  x <- seq(mu - 3.1 * sigma, mu + 3.1 * sigma, 0.01)\n  valor <- dnorm(x, mu, sigma)\n  datos <- tibble(x = x, `f(x)`=valor)\n  texto <- round(100*(pnorm(z) - pnorm(-z)), 1)\n  texto <- paste0(texto, \"%\")\n  ggplot(datos, aes(x = x, y = `f(x)`)) +\n    geom_area(data = filter(datos, x < mu + z*sigma, x > mu - z*sigma), \n      fill = \"salmon\") +\n        geom_line() +\n    annotate(\"text\", x = mu, y = 0.1, label = texto) +\n    scale_x_continuous(breaks = mu + sigma*seq(-3, 3, 1)) +\n    theme_minimal() + ylab(\"\") \n}\ng_68 <- grafica_concentracion(10, 2, 1)\ng_95 <- grafica_concentracion(10, 2, 2)\ng_997 <- grafica_concentracion(10, 2, 3)\npaneles <- g_68 + g_95 + g_997\npaneles + plot_annotation(title = \"Concentración alrededor de la media (normal)\")\n\n\n\n\n\nNota: esto aplica para cualquier densidad normal, independientemente de los parámetros.\nObsérvese que esto nos da una interpretación natural de la desviación estándar de una distribución normal en términos de percentiles de los datos, y la manera usual con la que entendemos la desviación estándar de la distribución normal."
  },
  {
    "objectID": "modelos-continuos.html#el-teorema-central-del-límite",
    "href": "modelos-continuos.html#el-teorema-central-del-límite",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.11 El teorema central del límite",
    "text": "10.11 El teorema central del límite\nUna de las razones por las que el modelo normal es tan importante es el siguiente resultado:\n::: {.cell type=‘comentario’}\n\nTeorema central del límite\nSi \\(X_1,X_2,\\ldots, X_n\\) son variables aleatorias independientes con media \\(\\mu\\) y desviación estándar \\(\\sigma\\) con una densidad \\(f(x)\\):\n\n\\(S_n = X_1 + X_2 + \\cdots X_n\\) es aproximadamente normal cuando \\(n\\) es suficientemente grande\n\n:::\nMuchas cantidades de interés en la estadística se pueden definir como sumas o promedios de un número grande de variables aleatorias (por ejempo, cuando queremos estimar el total de ingreso de los hogares, estatura promedio en una población, etc.) Los percentiles de una muestra grande también cumplen un teorema central del límite de este tipo.\nLa aproximación del teorema central del límite mejora cuando \\(n\\) es más grande. Aunque una regla de dedo dice que \\(n\\geq 30\\) es suficiente para muchas distribuciones, puede ser que sea necesaria usar una \\(n\\) más grande.\n\nEsto nos permite, por ejemplo, considerar nuestro primera técnica de estimación por intervalos:\n\nObservamos una muestra grande \\(x_1,\\ldots, x_n\\) de datos de una población (no necesariamente con distribución normal). Supongamos que buscamos estimar la media \\(\\mu\\) de la población con un intervalo.\nEstimamos la media con\n\n\\[\\bar{x} = \\frac{1}{n}(x_1+\\cdots + x_n) = \\frac{1}{n}\\sum_i x_i,\\]\n\nPor el teorema del límite central, {x} es aproximadamente normal, y su media es \\(\\mu\\). Esto implica que\n\n\\[P(\\mu - 2\\sigma  \\leq \\bar{x} \\leq\\mu + 2\\sigma)\\approx 0.95\\]\nDespejando \\(\\mu\\) obtenemos\n\\[P(\\bar{x} - 2\\sigma  \\leq \\mu  \\leq\\bar{x} + 2\\sigma)\\approx 0.95\\]\nFinalmente, no conocemos \\(\\sigma\\), pero la estimamos con\n\\[\\hat{\\sigma}^2 = \\frac{1}{n}((x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2) = \\frac{1}{n}\\sum_i(x_i - \\bar{x})^2\\]\nY aproximamos sustituyendo nuestra estimación:\n\\[P(\\bar{x} - 2\\hat{\\sigma } \\leq \\mu  \\leq\\bar{x} + 2\\hat{\\sigma})\\approx 0.95\\]\nEsto nos da un intervalo (llamado el intervalo de Wald) con 95% de confianza para la media poblacional:\n\\[[\\bar{x} - 2\\hat{\\sigma }  , \\bar{x} + 2\\hat{\\sigma}]\\]\nNotas:\n\nPor otras razones técnicas, a veces se usa \\(s^2 = \\frac{1}{n-1}\\sum_i (x_i-\\bar{x})^2\\) en lugar de \\(\\hat{\\sigma}^2\\). Si la muestra es grande esto no es importante.\nEstos intervalos tienen cobertura nominal de 95%, sin embargo, puede variar dependiendo del tamaño de muestra y la forma de la distribución teórica (poblacional). Existen métodos como el bootstrap donde podemos checar qué tan razonable es hacer esta aproximación. También se puede hacer simulación modelando la distribución \\(f(x)\\)."
  },
  {
    "objectID": "modelos-continuos.html#otras-densidades-comunes",
    "href": "modelos-continuos.html#otras-densidades-comunes",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.12 Otras densidades comunes",
    "text": "10.12 Otras densidades comunes\nComo vimos arriba, consideraciones teóricas hacen razonable suponer que una variable aleatoria tiene cierta distribución: por ejemplo, si una variable aleatoria es suma de muchas perturbaciones independientes, la suma o promedio resultante puede modelarse como una distribución normal.\nOtras consideraciones teóricas sugieren otro tipo formas útiles de densidades. Un primer ejemplo es la distribución exponencial.\n\n10.12.1 Variables aleatorias exponenciales\nSupongamos que estamos modelando tiempos a la ocurrencia \\(X\\) de un evento (por ejemplo en análisis de supervivencia en estudios clínicos). Esta es una variable que toma valores en los números positivos. ¿Cómo podría ser su forma?\nConsideremos por ejemplo que el tiempo de espera no tienen memoria. Es decir: si hay esperamos un periodo de \\(t\\) días por ejemplo, la distribución del tiempo restante que tenemos que esperar no depende de \\(s\\). En términos de probabilidad, podríamos escribir:\n\\[P(X > s + t | X > t) = P(X > s)\\]\nEsto se lee: dado que el evento ocurre en más de \\(t\\) días, la probabilidad de que tarde al menos otros \\(s\\) días no dependen de \\(t\\). A la función \\(S(t) = P(X>t)\\) muchas veces se le llama la función de supervivencia.\nEste es un modelo base útil, que después puede extenderse a procesos donde los eventos ocurren aceleradamente (envejecen), o donde los eventos dan evidencia de robustez (los que han sobrevivido hasta cierto tiempo se espera que duren más que lo que inicialmente), o quizá una combinación de los dos dependiendo de el valor de \\(t\\).\nCon el supuesto de falta de memoria, la ecuación de arriba se cumple, y entonces (¿por qué?):\n\\[P(X > s + t) = P(X > s) P(X > t),\\]\nasí que\n\\[\\frac{1}{s}(P(X > s+ t) - P(X>t)) = \\frac{1}{s}(P(X>s) - 1)P(X>t).\\]\nQue podemos reescribir usando la densidad \\(f(x)\\) como\n\\[\\frac{1}{s}\\int_t^{t+s} f(x)\\,dx = P(X>t)\\frac{1}{s}\\int_0^s f(x)\\,dx\\]\nConforme \\(s\\) se hace más chica, el lado izquierdo converge a \\(f(t)\\). El lado derecho, por otra parte, converge a \\(f(0)\\), y obtenemos\n\\[f(t) = f(0)\\int_t^\\infty f(x)\\,dx,\\]\ny ahora derivamos de ambos lados para obtener\n\\[f'(t) = -f(0)f(t)\\]\nLa única función que satisface esta propiedad (su derivada es proporcional a ella misma) es\n\\[f(t) = \\lambda e^{-\\lambda t}\\]\nPuedes checar que efectivamente esta densidad cumple que la ecuación anterior. A esta densidad le llamamos la densidad exponencial con tasa \\(\\lambda\\).\n\n\nCódigo\nlambda <- 1\nsim_exp <- rexp(1000, rate = lambda)\nggplot(tibble(x = sim_exp), aes(x = x)) +\n    geom_histogram(boundary = 0)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEjercicio: demuestra que una variable aleatoria exponencial con tasa \\(\\lambda\\) tiene media \\(E(X) = 1/\\lambda\\) y $Var(X) = 1 / ^2\nEl parámetro \\(\\lambda\\) se llama tasa por la interpretación de tiempo de espera que mostramos arriba. Supongamos que \\(\\lambda = 10\\). Eso quiere decir que esperamos observar el evento en \\(1/\\lambda = 1/10\\) minutos, por ejemplo, o lo que es lo mismo, a una tasa de \\(\\lambda = 10\\) eventos por minuto.\nNótese finalmente que todas las variables exponenciales son escalamientos de una variable exponencial con \\(\\lambda = 1\\). Esto es porque si \\(Y=kX\\), y \\(X\\) es exponencial con \\(\\lambda= 1\\) entonces:\n\\[P(Y>t)=P(kX > t) = P(X > t/k) = \\int_{t/k}^\\infty e^{-x}\\,dx = e^{-t/k}\\]\nDe modo que derivando, encontramos que la densidad de \\(Y\\) es \\(\\frac{1}{k} e^{-t/k}\\), que es una exponencial con tasa \\(\\lambda = 1/k\\).\n\n\n10.12.2 Ejemplo: exponencial\nSupongamos que un tipo de focos tienen tiempos de vida exponencial con una vida media de 10 años. ¿Cuál es la probabilidad de que un foco dure más de 15 años? Si tenemos un foco que ya vivió 10 años, cuál es la probabilidad de que dure otros 15 años.\nTenemos que la vida de un foco es una variable \\(X\\) exponencial con parámetro \\(\\lambda = 1/10\\). La probabilidad de que dure más de 15 años es entonces\n\\[P(X>15) = \\int_{15}^\\infty \\frac{1}{10} e^{x/10}\\,dx\\]\nPodemos calcular a mano, o usar rutinas usuales de R:\n\n\nCódigo\npexp(15, rate = 1/10, lower.tail = FALSE)\n\n\n[1] 0.2231302\n\n\n\n\n10.12.3 Variables aleatorias gamma\nEsta es otra familia que extiende la familia de distribuciones exponenciales. La forma analítica de una densidad gamma con parámetro de forma \\(k>0\\) y tasa \\(\\lambda\\)\n\\[f(x) = C x^{k-1} e^{-\\lambda x}\\]\ndonde la constante \\(C\\) de normalización depende de \\(k\\) y \\(\\lambda\\).\nAbajo vemos datos simulados de densidades Gamma con distintas combinaciones de parámetros:\n\n\nCódigo\nparams_tbl <- crossing(k = c(1, 2, 5, 10), lambda = c(1/4, 1/2))\nsims_tbl <- params_tbl %>% \n    mutate(sims = map2(k, lambda, \n                       ~ rgamma(10000, shape = .x, rate = .y))) %>% \n    unnest(cols = sims)\n\n\n\n\nCódigo\nggplot(sims_tbl, aes(x = sims)) + \n    geom_histogram(boundary = 0, bins = 50) +\n    facet_grid(k~lambda)\n\n\n\n\n\nNótese que cada columna es un rescalamiento de la otra, pero las densidades de los renglones tienen distinta forma. Puedes ver aquí parámetros como esperanza, varianza de esta estas distribuciones, junto con otras propiedades y aplicaciones."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "href": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.13 Modelos conjuntos de probabilidad",
    "text": "10.13 Modelos conjuntos de probabilidad\nUsualmente no nos interesa una sola variable aleatoria, sino varias. Nos interesa entender cómo están relacionadas o cómo depende una de otra.\nPor ejemplo, ¿cuál es la mediana de peso para un infante de 2 meses, y qué tan diferente es de la mediana de peso para un infante de 5 meses? ¿qué relación hay entre temperatura y presión atmosférica? ¿qué relación hay entre creencias religiosas y afiliación política? En todos estos casos quisiéramos describir de distintas formas cómo ser relaciona una cantidad aleatoria con otra.\nAl principio de este curso, vimos algunas técnicas descriptivas para mostrar y explorar estas relaciones. Por ejemplo:\n\n¿Cómo cambian las preferencias de forma de tomar té dependiendo del tipo de té una persona acostumbra tomar? ¿Qué tan probable es que alguien que toma té negro use azúcar vs alguien que toma té verde? (relación entre dos variables categóricas o discretas)\n¿Cómo cambian los precios medios de las casas dependiendo del vecindario donde se ubican? (describir la dependencia de una variable numérica si sabemos el valor de una variable categórica)\n¿Cómo cambia la mediana y los cuartiles de peso de un infante dependiendo de los meses desde que nació (describir cómo depende una variable numérica de otra variable numérica)\n\nEn esta parte veremos una introducción cómo se formalizan estos conceptos en modelos probabilísticos."
  },
  {
    "objectID": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "href": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.14 Estaturas: modelando relaciones de dependencia",
    "text": "10.14 Estaturas: modelando relaciones de dependencia\nSupongamos que \\(X\\) es la edad de una persona entre 4 y 15 años. y \\(Y\\) es su estatura. La relación entre \\(X\\) y \\(Y\\) no es determinística, pues existe variación en el crecimiento para las personas por distintas razones. Por esta razón, no tiene mucho sentido dar una relación como \\(Y = 80 + 5.5 X\\), por ejemplo, pues esta relación no se cumple para prácticamente ninguna persona.\nTiene más sentido, sin embargo, decir cómo es la distribución condicional de \\(Y\\) dado que conocemos \\(X\\). Por ejemplo, podríamos hacer la hipótesis de que la mediana de estatura para una persona de edad \\(X\\) está dada por\n\\[med(Y|X) = 80 + 5.5 X\\]\nNótese que escribimos la mediana condicional de \\(Y\\) dado que conocemos el valor de \\(X\\). También podríamos escribir la media condicional de \\(Y\\) dada \\(X\\) como\n\\[E(Y|X) = 80 + 5.5 X\\]\nY estas dos cantidades tienen sentido.\nEstas cantidades claramente no determinan la variabilidad que hay de la estatura cuando conocemos \\(X\\). Podríamos entonces también especificar por ejemplo la desviación estándar condicional:\n\\[\\sigma(Y|X) = 3\\sqrt{X} \\]\nGeneralmente estas relaciones se estiman empíricamente con datos observados, pero para este ejemplo utilizaremos estos modelos fijos.\nSimulamos algunos datos con estas propiedades:\n\n\nCódigo\nedades <- runif(800, 2, 15) # edad distribuida uniforme, o grupos de edad del mismo tamaño\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rnorm(n(), mean = media, sd = desv_est))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point()\n\n\n\n\n\n\nObsérvese cómo en efecto la estatura esperada aumenta con la edad (condicional a la edad), y que la dispersión de estatura aumenta conforme la edad aumenta.\n\nNótese que si usamos un suavizador, podemos estimar la media condicional de nuestro modelo, que en este caso está cercana a la fórmula que establecimos en nuestro modelo:\n\n\nCódigo\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_smooth(se = FALSE)\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nPodemos estimar cuantiles también como vimos en secciones anteriores:\n\n\nCódigo\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 20, quantiles = c(0.10, 0.5, 0.90)) \n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 20)\n\n\n\n\n\nY observamos igualmente que la dispersión para el grupo de 15 años es cercana al doble que la dispersión para el grupo de 4 años."
  },
  {
    "objectID": "modelos-continuos.html#distribuciones-condicionales",
    "href": "modelos-continuos.html#distribuciones-condicionales",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.15 Distribuciones condicionales",
    "text": "10.15 Distribuciones condicionales\nTodo el trabajo de arriba de modelación teórica consiste entonces en definir distribuciones condicionales. En el ejemplo anterior,\n\nDimos una distribución para \\(X\\), que en este caso la tomamos uniforme en [4, 15], pues suponemos que esta es la estructura de nuestra población (hay el aproximadamente el mismo número de personas de cada edad)\nDimos una distribución para \\(Y\\) condicionada a \\(X\\). En este caso, establecimos que \\(Y|X\\) es normal con media \\(80 + 5.5X\\) y desviación estándar \\(3 * sqrt(X)\\).\n\nEstas dos partes dan un modelo conjunto para \\(X\\) y \\(Y\\): sabemos que está completamente determinado pues pudimos simular del modelo. Otra manera de entender esto es que cualquier probabilidad que involucra a \\(X\\) y \\(Y\\) puede ser calculada con la regla del producto. Aunque no entraremos en detalles, la densidad conjunta de \\(X\\) y \\(Y\\) puede definirse en este caso como\n\\[f(x,y) = f_X(x)f_{Y|X}(y|x)\\]\nSabemos que \\(f_X(x) = 1/(15- 3)\\) para \\(x\\) entre 4 y 15 años, y la forma de \\(f_{Y|X}(y|x)\\) sabemos que es normal con media y varianza conocida (en términos de x). Esta conjunta puede ser integrada sobre cualquier región (al menos en teoría) para calcular la probabilidad de interés"
  },
  {
    "objectID": "modelos-continuos.html#estaturas-variación-gamma",
    "href": "modelos-continuos.html#estaturas-variación-gamma",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.16 Estaturas: variación gamma",
    "text": "10.16 Estaturas: variación gamma\nPodemos juntar estos bloques de densidades condicionales para construir otro tipo de modelos. Por ejemplo, supondremos que la estatura dada la edad es una distribución gamma \\((k,\\lambda)\\) y la misma media y desviación estándar que vimos arriba. Como la media de una gamma de este tipo es \\(\\mu = k/\\lambda\\) y su desviación estándar es \\(\\sigma = \\sqrt{k}/\\lambda\\), podemos despejar \\(k\\) y \\(lambda\\) y hacer:\n\n\nCódigo\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\nEste modelo es muy similar al normal. Sin embargo, podríamos intentar otras variaciones si cambiamos la magnitud de la desviación estándar en relación a la media, por ejemplo:\n\n\nCódigo\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 30 + 10*edad, desv_est = 7 * (edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(medicion_y = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = medicion_y)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\nEste caso claramente no serviría para modelar estaturas, pero podemos ver cómo introdujimos asimetría considerable en las distribuciones condicionales de y dado x una vez que especificamos la media y la varianza condicionales."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "href": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.17 Modelos conjuntos para factor categórico",
    "text": "10.17 Modelos conjuntos para factor categórico\nSupongamos ahora que la variable \\(Y\\) es numérica y la variable \\(X\\) es categórica. En este caso, un modelo conjunto está definido por las probabilidades \\(P(X=x)\\) junto con densidades condicionales \\(Y|X\\).\n\n10.17.1 Ejemplo: cuentas y propinas\nSupongamos que \\(X\\) es la hora del día (comida y cena) y que \\(Y\\) es el tamaño de la cuenta.\nPodríamos establecer por ejemplo, que \\(Y|X=comida\\) es Gamma con media 10 dólares y desviacion estándar 10 dólares. Sin embargo, \\(Y|X=cena\\) es Gamma con media 25 dólares y desviación estándar 20 dólares. Simulamos y analizamos:\n\n\nCódigo\ndatos_tbl <- \n   tibble(hora = sample(c(\"comida\", \"cena\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(hora == \"comida\", 10, 25),\n          desv_est = ifelse(hora == \"comida\", 10, 20)) %>% \n   mutate(cuenta = rgamma(n(), shape = (media / desv_est)^2,\n                               rate = media / (desv_est^2)))\nggplot(datos_tbl, aes(x = hora, y = cuenta)) +\n   geom_boxplot()\n\n\n\n\n\nY esta gráfica busca mostrar una estimación de las distribuciones condicionales de cuenta dado el turno donde el cliente consumió."
  },
  {
    "objectID": "modelos-continuos.html#modelos-multivariados",
    "href": "modelos-continuos.html#modelos-multivariados",
    "title": "10  Modelos probabilísticos para variables continuas",
    "section": "10.18 Modelos multivariados",
    "text": "10.18 Modelos multivariados\nAhora consideremos que queremos construir un modelo conjunto para tres variables \\(X, Y\\) y \\(Z\\). La forma en que procedemos dependerá del problema particular, pero podemos usar la regla del producto como guía. Por ejemplo, podríamos dar una distribución para \\(Z\\), luego una densidad condicional de Y dado \\(Z\\), y finalmente una condicional de \\(Y\\) dada tanto \\(X\\) como \\(Z\\).\n\n\nCódigo\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    Z -> X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\n\nEn este caso, cada variable aleatorio es nodo, y representa una distribución condicional dado sus antecesores. Esta gráfica por ejemplo, muestra una manera de escribir con la regla del producto un modelo conjunto, pero podríamos cambiar de posición los nodos dependiendo de nuestro conocimiento y el problema que queremos resolver.\nEn algunos casos, es posible simplificar la construcción del modelo eliminando algunas aristas. Supongamos por ejemplo que\n\nX es la edad de la persona\nZ es “M” of “F”\nY es su estatura\n\nEn este caso, no es necesario especificar la condicional de \\(X\\) dado \\(Z\\), pues estas dos son variables independientes. Pondríamos entonces simplemente\n\n\nCódigo\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\n\nY solo necesitamos especificar las distribuciones de \\(X\\), de \\(Z\\), y la condicional de \\(Y\\) dado \\(Z\\). Siguiendo nuestro ejemplo anterior, consideraremos a\n\n\\(X\\) como uniforme en \\([4,15]\\) (que es nuestro rango de edad de interés)\n\\(Z\\) es \\(M\\) con probabilidad 0.5 y \\(F\\) con probabilidad 0.5\n\nY podríamos especificar ahora: la condicional de \\(Y\\) (estatura) es normal con los siguientes parámetros:\n\nSi \\(X\\) es la edad y \\(Z=\"F\"\\), entonces la media es \\(70 + 6.5 X\\)\nSi \\(X\\) es la edad y \\(Z=\"M\"\\), entonces la media es \\(80 + 4.5 X\\)\nLa desviación estándar sólo depende de \\(X\\), y es igual a \\(4\\sqrt{X}\\).\n\nSimulamos ahora de este modelo probabilístico:\n\n\nCódigo\ndatos_tbl <- tibble(x = runif(1000, 4, 15)) %>%\n   # independientemente simulamos M o F\n   mutate(z = sample(c(\"m\", \"f\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(z==\"f\", 70 + 6.5 * x, 80 + 4.5 * x)) %>% \n   mutate(desv_est = 4 * sqrt(x)) %>% \n   mutate(estatura = rnorm(n(), media, desv_est))\ndatos_tbl %>% head(20) %>% kable()\n\n\n\n\n \n  \n    x \n    z \n    media \n    desv_est \n    estatura \n  \n \n\n  \n    13.917312 \n    m \n    142.62790 \n    14.922366 \n    106.17567 \n  \n  \n    9.655767 \n    m \n    123.45095 \n    12.429492 \n    118.95931 \n  \n  \n    4.178553 \n    m \n    98.80349 \n    8.176603 \n    102.90009 \n  \n  \n    4.085706 \n    m \n    98.38568 \n    8.085252 \n    99.27880 \n  \n  \n    7.510502 \n    f \n    118.81826 \n    10.962118 \n    120.93123 \n  \n  \n    9.506507 \n    m \n    122.77928 \n    12.333049 \n    131.72703 \n  \n  \n    8.311886 \n    f \n    124.02726 \n    11.532137 \n    127.80079 \n  \n  \n    10.336809 \n    f \n    137.18926 \n    12.860363 \n    138.18963 \n  \n  \n    8.578672 \n    m \n    118.60402 \n    11.715748 \n    133.69638 \n  \n  \n    11.500391 \n    f \n    144.75254 \n    13.564891 \n    121.21221 \n  \n  \n    6.668017 \n    f \n    113.34211 \n    10.329002 \n    123.89877 \n  \n  \n    12.935783 \n    f \n    154.08259 \n    14.386540 \n    135.22746 \n  \n  \n    14.904618 \n    m \n    147.07078 \n    15.442600 \n    128.11113 \n  \n  \n    11.579196 \n    m \n    132.10638 \n    13.611287 \n    143.13437 \n  \n  \n    14.921303 \n    m \n    147.14586 \n    15.451241 \n    162.54670 \n  \n  \n    8.863920 \n    m \n    119.88764 \n    11.908934 \n    114.67713 \n  \n  \n    8.487825 \n    m \n    118.19521 \n    11.653549 \n    105.22789 \n  \n  \n    4.221525 \n    m \n    98.99686 \n    8.218540 \n    94.52403 \n  \n  \n    6.803164 \n    m \n    110.61424 \n    10.433150 \n    108.54392 \n  \n  \n    14.458625 \n    m \n    145.06381 \n    15.209799 \n    145.96106 \n  \n\n\n\n\n\nY hacemos algunas gráficas descriptivas:\n\n\nCódigo\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point()\n\n\n\n\n\n\nDiscute qué otras cosas podrías cambiar en este modelo probabilístico para que fuera más flexible o más simple. ¿Cómo ajustarías un modelo así a datos reales?\n\n\n\nCódigo\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point() +\n   facet_wrap(~z) +\n   geom_quantile(method = \"rqss\", lambda = 10,\n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\n\n\n\nEn la modelación probabilística generalmente usamos estos mecanismos (dependencia condicional, independencia) y estos bloques (distribuciones de probabilidad dadas en términos de parámetros) para obtener estimaciones de parámetros de interés.\nLas decisiones de cómo usar estos mecanismos y bloques se desprenden de conocimiento de dominio, alcances del análisis, y siempre están sujetos a revisión dependiendo del tipo de desajustes que presenten frente a los datos reales."
  },
  {
    "objectID": "inferencia-modelos.html",
    "href": "inferencia-modelos.html",
    "title": "11  Inferencia con modelos probabilísticos",
    "section": "",
    "text": "Intro"
  },
  {
    "objectID": "inferencia-bayesiana.html",
    "href": "inferencia-bayesiana.html",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La inferencia bayesiana es el proceso de ajustar un modelo de probabilidad a un conjunto de datos y resumir el resultado mediante una distribución de probabilidad sobre los parámetros del modelo y sobre cantidades no observadas, como predicciones para nuevas observaciones. En éste capítulo desarrollaremos con ejemplos sencillos las ideas básicas de la inferencia bayesiana."
  },
  {
    "objectID": "intro-bayesiana-1.html",
    "href": "intro-bayesiana-1.html",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "",
    "text": "En las secciones anteriores estudiamos métodos de remuestreo para estimar parámetros, y cuantificar la incertidumbre qué tenemos acerca de valores poblacionales.\nOtra manera de hacer inferencia es usando modelos probabilísticos para los datos: asumir que los datos tienen cierto comportamiento (por ejemplo, provienen de distribuciones normales), y usar métodos como máxima verosimilitud para hacer estimaciones junto con su incertidumbre.\nUna tercera opción es utilizar inferencia bayesiana, que tiene objetivos similares.\nEl concepto probabilístico básico que utilizamos para construir estos modelos y la inferencia es el de probabilidad condicional: la probabilidad de que ocurran ciertos eventos dada la información disponible del fenómeno que nos interesa."
  },
  {
    "objectID": "intro-bayesiana-1.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "href": "intro-bayesiana-1.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Un primer ejemplo completo de inferencia bayesiana",
    "text": "Un primer ejemplo completo de inferencia bayesiana\nConsideremos el siguiente problema simple: Nos dan una moneda, y solo sabemos que la moneda puede tener probabilidad \\(3/5\\) de tirar sol (está cargada a sol) o puede ser una moneda cargada a águila, con probabilidad \\(2/5\\) de tirar sol.\nVamos a lanzar la moneda dos veces y observamos su resultado (águila o sol). Queremos decir algo acerca de qué tan probable es que hayamos tirado la moneda cargada a sol o la moneda cargada a águila.\nEn este caso, tenemos dos variables: \\(X\\), que cuenta el número de soles obtenidos en el experimento aleatorio, y \\(\\theta\\), que da la probabilidad de que un volado resulte en sol (por ejemplo, si la moneda es justa entonces \\(\\theta = 0.5\\)).\n¿Qué cantidades podríamos usar para evaluar qué moneda es la que estamos usando? Si hacemos el experimento, y tiramos la moneda 2 veces, podríamos considerar la probabilidad\n\\[P(\\theta = 0.4 | X = x)\\]\ndonde \\(x\\) es el número de soles que obtuvimos en el experimento. Esta es la probabilidad condicional de que estemos tirando la moneda con probabilidad de sol 2/5 dado que observamos \\(x\\) soles. Por ejemplo, si tiramos 2 soles, deberíamos calcular\n\\[P(\\theta=0.4|X=2).\\]\n¿Cómo calculamos esta probabilidad? ¿Qué sentido tiene?\nUsando reglas de probabildad (regla de Bayes en particular), podríamos calcular\n\\[P(\\theta=0.4|X=2) = \\frac{P(X=2 | \\theta = 0.4) P(\\theta =0.4)}{P(X=2)}\\]\nNota que en el numerador uno de los factores, \\(P(X=2 | \\theta = 0.4),\\) es la verosimilitud. Así que primero necesitamos la verosimilitud:\n\\[P(X=2|\\theta = 0.4) = (0.4)^2 = 0.16.\\]\nLa novedad es que ahora tenemos que considerar la probabilidad \\(P(\\theta = 0.4)\\). Esta cantidad no la habíamos encontrado antes. Tenemos que pensar entonces que este parámetro es una cantidad aleatoria, y puede tomar dos valores \\(\\theta=0.4\\) ó \\(\\theta = 0.6\\).\nConsiderar esta cantidad como aleatoria requiere pensar, en este caso, en cómo se escogió la moneda, o qué sabemos acerca de las monedas que se usan para este experimento (conocimiento de dominio). Supongamos que en este caso, nos dicen que la moneda se escoge al azar de una bolsa donde hay una proporción similar de los dos tipos de moneda (0.4 ó 0.6). Es decir el espacio parametral es \\(\\Theta = \\{0.4, 0.6\\},\\) y las probabilidades asociadas a cada posibilidad son las mismas. Tenemos\n\\[P(\\theta = 0.4) = P(\\theta = 0.6) =0.5,\\]\nque representa la probabilidad de escoger de manera aleatoria la moneda con una carga en particular.\nAhora queremos calcular \\(P(X=2)\\), pero con el trabajo que hicimos esto es fácil. Pues requiere usar reglas de probabilidad usuales para hacerlo. Podemos utilizar probabilidad total \\[\\begin{align}\nP(X) &= \\sum_{\\theta \\in \\Theta} P(X, \\theta)\\\\\n&= \\sum_{\\theta \\in \\Theta} P(X\\, |\\, \\theta) P(\\theta),\n\\end{align}\\] lo cual en nuestro ejemplo se traduce en escribir\n\\[ P(X=2) = P(X=2|\\theta = 0.4)P(\\theta = 0.4) + P(X=2|\\theta=0.6)P(\\theta =0.6),\\]\npor lo que obtenemos\n\\[P(X=2) = 0.16(0.5) + 0.36(0.5) = 0.26.\\]\nFinalmente la probabilidad de haber escogido la moneda con carga \\(2/5\\) dado que observamos dos soles en el lanzamiento es\n\\[P(\\theta=0.4|X=2) = \\frac{0.16(0.5)}{0.26} \\approx  0.31.\\]\nEs decir, la probabilidad posterior de que estemos tirando la moneda \\(2/5\\) baja de 0.5 (nuestra información inicial) a 0.31.\nEste es un ejemplo completo, aunque muy simple, de inferencia bayesiana. La estrategia de inferencia bayesiana implica tomar decisiones basadas en las probabilidades posteriores.\n¿Cuál sería la estimación de máxima verosimilitud para este problema? ¿Cómo cuantificaríamos la incertidumbre en la estimación de máxima verosimilitud?\nFinalmente, podríamos hacer predicciones usando la posterior predictiva. Si \\({X}_{nv}\\) es una nueva tirada adicional de la moneda que estamos usando, nos interesaría saber:\n\\[P({X}_{nv}=\\mathsf{sol}\\, | \\, X=2)\\]\nNotemos que un volado adicional es un resultado binario. Por lo que podemos calcular observando que \\(P({X}_{nv}|X=2, \\theta)\\) es una variable Bernoulli con probabilidad \\(\\theta\\), que puede valer 0.4 ó 0.6. Como tenemos las probabilidades posteriores \\(P(\\theta|X=2)\\) podemos usar probabilidad total, condicionado en \\(X=2\\): \\[\\begin{align*}\nP({X}_{nv}=\\mathsf{sol}\\, | \\, X=2) & = \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}, \\theta \\, | \\, X=2) & \\text{(probabilidad total)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta , X=2) P(\\theta \\, | \\, X=2) & \\text{(probabilidad condicional)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta ) P(\\theta \\, | \\, X=2), & \\text{(independencia condicional)}\n\\end{align*}\\]\nlo que nos da el siguiente cálculo\n\\[P(X_{nv}=\\mathsf{sol}\\, |\\, \\theta=0.4) \\,  P(\\theta=0.4|X=2) \\,  +\\, P(X_{nv}=\\mathsf{sol}|\\theta = 0.6) \\, P(\\theta =0.6|X=2)\\]\nEs decir, promediamos ponderando con las probabilidades posteriores. Por lo tanto obtenemos\n\\[P(X_{nv} = \\mathsf{sol}|X=2) =  0.4 ( 0.31) + 0.6 (0.69) = 0.538.\\]\n\nObservación 0\nNótese que en contraste con máxima verosimilitud, en este ejemplo cuantificamos con probabilidad condicional la incertidumbre de los parámetros que no conocemos. En máxima verosimilitud esta probabilidad no tiene mucho sentido, pues nunca consideramos el parámetro desconocido como una cantidad aleatoria.\n\n\nObservación 1\nNótese el factor \\(P(X=2)\\) en la probabilidad posterior puede entenderse como un factor de normalización. Notemos que los denominadores en la distribución posterior son\n\\[P(X=2 | \\theta = 0.4) P(\\theta =0.4) = 0.16(0.5) = 0.08,\\]\ny\n\\[P(X=2 | \\theta = 0.6) P(\\theta =0.6) = 0.36(0.5) = 0.18.\\]\nLas probabilidades posteriores son proporcionales a estas dos cantidades, y como deben sumar uno, entonces normalizando estos dos números (dividiendo entre su suma) obtenemos las probabilidades.\n\n\nObservación 2\nLa nomenclatura que usamos es la siguiente:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, o modelo de los datos.\nA \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior.\n\nPara utilizar inferencia bayesiana, hay que hacer supuestos para definir las primeras dos partes del modelo. La parte de iniciales o previas está ausente de enfoques como máxima verosimlitud usual.\n\n\nObservación 3\n¿Cómo decidimos las probabilidades iniciales, por ejemplo \\(P(\\theta=0.4)\\) ?\nQuizá es un supuesto y no tenemos razón para pensar que se hace de otra manera. O quizá conocemos el mecanismo concreto con el que se selecciona la moneda. Discutiremos esto más adelante.\n\n\nObservación 4\n¿Cómo decidimos el modelo de los datos? Aquí típicamente también tenemos que hacer algunos supuestos, aunque algunos de estos pueden estar basados en el diseño del estudio, por ejemplo. Igual que cuando usamos máxima verosimilitud, es necesario checar que nuestro modelo ajusta razonablemente a los datos.\n\n\nEjercicio\nCambia distintos parámetros del número de soles observados, las probabilidades de sol de las monedas, y las probabilidades iniciales de selección de las monedas.\n\n\nCódigo\nn_volados <- 2\n# posible valores del parámetro desconocido\ntheta = c(0.4, 0.6)\n# probabilidades iniciales\nprobs_inicial <- tibble(moneda = c(1, 2),\n                        theta = theta,\n                        prob_inicial = c(0.5, 0.5))\nprobs_inicial\n\n\n# A tibble: 2 × 3\n  moneda theta prob_inicial\n   <dbl> <dbl>        <dbl>\n1      1   0.4          0.5\n2      2   0.6          0.5\n\n\nCódigo\n# verosimilitud\ncrear_verosim <- function(no_soles){\n    verosim <- function(theta){\n      # prob de observar no_soles en 2 volados con probabilidad de sol theta\n      dbinom(no_soles, 2, theta)\n    }\n    verosim\n}\n# evaluar verosimilitud\nverosim <- crear_verosim(2)\n# ahora usamos regla de bayes para hacer tabla de probabilidades\ntabla_inferencia <- probs_inicial |>\n  mutate(verosimilitud = map_dbl(theta, verosim)) |>\n  mutate(inicial_x_verosim = prob_inicial * verosimilitud) |>\n  # normalizar\n  mutate(prob_posterior = inicial_x_verosim / sum(inicial_x_verosim))\ntabla_inferencia |>\n  mutate(moneda_obs = moneda) |>\n  select(moneda_obs, theta, prob_inicial, verosimilitud, prob_posterior)\n\n\n# A tibble: 2 × 5\n  moneda_obs theta prob_inicial verosimilitud prob_posterior\n       <dbl> <dbl>        <dbl>         <dbl>          <dbl>\n1          1   0.4          0.5          0.16          0.308\n2          2   0.6          0.5          0.36          0.692\n\n\n\n¿Qué pasa cuando el número de soles es 0? ¿Cómo cambian las probabilidades posteriores de cada moneda?\nIncrementa el número de volados, por ejemplo a 10. ¿Qué pasa si observaste 8 soles, por ejemplo? ¿Y si observaste 0?\n¿Qué pasa si cambias las probabilidades iniciales (por ejemplo incrementas la probabilidad inicial de la moneda 1 a 0.9)?\n\nJustifica las siguientes aseveraciones (para este ejemplo):\n\nLas probabilidades posteriores o finales son una especie de punto intermedio entre verosimilitud y probablidades iniciales.\nSi tenemos pocas observaciones, las probabilidades posteriores son similares a las iniciales.\nCuando tenemos muchos datos, las probabilidades posteriores están más concentradas, y no es tan importante la inicial.\nSi la inicial está muy concentrada en algún valor, la posterior requiere de muchas observaciones para que se pueda concentrar en otros valores diferentes a los de la inicial.\n\nAhora resumimos los elementos básicos de la inferencia bayesiana, que son relativamente simples:\nInferencia bayesiana. Con la notación de arriba: - Como \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, proceso generador de datos o modelo de los datos. - El factor \\(P(\\theta)\\) le llamamos la distribución inicial o previa. - La distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior Hacemos inferencia usando la ecuación\n\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\\]\nque también escribimos:\n\\[P(\\theta | X) \\propto P(X | \\theta) P(\\theta)\\]\ndonde \\(\\propto\\) significa “proporcional a”. No ponemos \\(P(X)\\) pues como vimos arriba, es una constante de normalización.\nEn estadística Bayesiana, las probablidades posteriores \\(P(\\theta|X)\\) dan toda la información que necesitamos para hacer inferencia. ¿Cuándo damos probablidad alta a un parámetro particular \\(\\theta\\)? Cuando su verosimilitud es alta y/o cuando su probabilidad inicial es alta. De este modo, la posterior combina la información inicial que tenemos acerca de los parámetros con la información en la muestra acerca de los parámetros (verosimilitud). Podemos ilustrar como sigue:"
  },
  {
    "objectID": "intro-bayesiana-1.html#ejemplo-estimando-una-proporción",
    "href": "intro-bayesiana-1.html#ejemplo-estimando-una-proporción",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Ejemplo: estimando una proporción",
    "text": "Ejemplo: estimando una proporción\nRegresamos ahora a nuestro problema de estimar una proporción \\(\\theta\\) de una población dada usando una muestra \\(X_1,X_2,\\ldots, X_n\\) de variables Bernoulli () extraídas de modo independiente. Ya sabemos calcular la verosimilitud (el modelo de los datos):\n\\[P(X_1=x_1,X_2 =x_2,\\ldots, X_n=x_n|\\theta) = \\theta^k(1-\\theta)^{n-k},\\]\ndonde \\(k = x_1 + x_2 +\\cdots + x_k\\) es el número de éxitos que observamos.\nAhora necesitamos una distribución inicial o previa \\(P(\\theta)\\). Aunque esta distribución puede tener cualquier forma, supongamos que nuestro conocimiento actual podemos resumirlo con una distribución \\(\\mathsf{Beta}(3, 3)\\):\n\\[P(\\theta) \\propto \\theta^2(1-\\theta)^2.\\]\nLa constante de normalización es 1/30, pero no la requerimos. Podemos simular para examinar su forma:\n\n\nCódigo\nsim_inicial <- tibble(theta = rbeta(10000, 3, 3))\nggplot(sim_inicial) + geom_histogram(aes(x = theta, y = ..density..), bins = 15)\n\n\n\n\n\n\n\n\n\nDe modo que nuestra información inicial es que la proporción puede tomar cualquier valor entre 0 y 1, pero es probable que tome un valor no tan lejano de 0.5. Por ejemplo, con probabilidad 0.95 creemos que \\(\\theta\\) está en el intervalo\n\n\nCódigo\nquantile(sim_inicial$theta, c(0.025, 0.975)) |> round(2)\n\n\n 2.5% 97.5% \n 0.15  0.85 \n\n\nEs difícil justificar en abstracto por qué escogeriamos una inicial con esta forma. Aunque esto los detallaremos más adelante, puedes pensar, por el momento, que alguien observó algunos casos de esta población, y quizá vio tres éxitos y tres fracasos. Esto sugeriría que es poco probable que la probablidad \\(\\theta\\) sea muy cercana a 0 o muy cercana a 1.\nAhora podemos construir nuestra posterior. Tenemos que\n\\[P(\\theta| X_1=x_1, \\ldots, X_n=x_n) \\propto P(X_1 = x_1,\\ldots X_n=x_n | \\theta)P(\\theta) = \\theta^{k+2}(1-\\theta)^{n-k + 2}\\]\ndonde la constante de normalización no depende de \\(\\theta\\). Como \\(\\theta\\) es un parámetro continuo, la expresión de la derecha nos debe dar una densidad posterior.\nSupongamos entonces que hicimos la prueba con \\(n = 30\\) (número de prueba) y observamos 19 éxitos. Tendríamos entonces\n\\[P(\\theta | S_n = 19) \\propto \\theta^{19 + 2} (1-\\theta)^{30 - 19 +2} = \\theta^{21}(1-\\theta)^{13}\\]\nLa cantidad de la derecha, una vez que normalizemos por el número \\(P(X=19)\\), nos dará una densidad posterior (tal cual, esta expresion no integra a 1). Podemos obtenerla usando cálculo, pero recordamos que una distribución \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) tiene como fórmula\n\\[\\frac{1}{B(a,b)} \\theta^{a-1}(1-\\theta)^{b-1}\\]\nConcluimos entonces que la posterior tiene una distribución \\(\\mathsf{Beta}(22, 14)\\). Podemos simular de la posterior usando código estándar para ver cómo luce:\n\n\nCódigo\nsim_inicial <- sim_inicial |> mutate(dist = \"inicial\")\nsim_posterior <- tibble(theta = rbeta(10000, 22, 14)) |> mutate(dist = \"posterior\")\nsims <- bind_rows(sim_inicial, sim_posterior)\nggplot(sims, aes(x = theta, fill = dist)) +\n  geom_histogram(aes(x = theta), bins = 30, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\n\n\nLa posterior nos dice cuáles son las posibilidades de dónde puede estar el parámetro \\(\\theta\\). Nótese que ahora excluye prácticamente valores más chicos que 0.25 o mayores que 0.9. Esta distribución posterior es el objeto con el que hacemos inferencia: nos dice dónde es creíble que esté el parámetro \\(\\theta\\).\nPodemos resumir de varias maneras. Por ejemplo, si queremos un estimador puntual usamos la media posterior:\n\n\nCódigo\nsims |> group_by(dist) |>\n  summarise(theta_hat = mean(theta) |> round(3))\n\n\n# A tibble: 2 × 2\n  dist      theta_hat\n  <chr>         <dbl>\n1 inicial       0.499\n2 posterior     0.611\n\n\nNota que el estimador de máxima verosimilitud es \\(\\hat{p} = 19/30 = 0.63\\), que es ligeramente diferente de la media posterior. ¿Por qué?\nY podemos construir intervalos de percentiles, que en esta situación suelen llamarse intervalos de credibilidad, por ejemplo:\n\n\nCódigo\nf <- c(0.025, 0.975)\nsims |> group_by(dist) |>\n  summarise(cuantiles = quantile(theta, f) |> round(2), f = f) |>\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n\n# A tibble: 2 × 3\n# Groups:   dist [2]\n  dist      `0.025` `0.975`\n  <chr>       <dbl>   <dbl>\n1 inicial      0.15    0.85\n2 posterior    0.45    0.76\n\n\nEl segundo renglón nos da un intervalo posterior para \\(\\theta\\) de credibilidad 95%. En inferencia bayesiana esto sustituye a los intervalos de confianza.\n\nEl intervalo de la inicial expresa nuestras creencias a priori acerca de \\(\\theta\\). Este intervalo es muy amplio (va de 0.15 a 0.85)\nEl intervalo de la posterior actualiza nuestras creencias acerca de \\(\\theta\\) una vez que observamos los datos, y es considerablemente más angosto y por lo tanto informativo.\n\nObservaciones:\n\nNótese que escogimos una forma analítica fácil para la inicial, pues resultó así que la posterior es una distribución beta. No siempre es así, y veremos qué hacer cuando nuestra inicial no es de un tipo “conveniente”.\nComo tenemos la forma analítica de la posterior, es posible hacer los cálculos de la media posterior, por ejemplo, integrando la densidad posterior a mano. Esto generalmente no es factible, y en este ejemplo preferimos hacer una aproximación numérica. En este caso particular es posible usando cálculo, y sabemos que la media de una \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) es \\(a/(a+b)\\), de modo que nuestra media posterior es\n\n\\[\\hat{\\mu} = (19 + 2)/(30 + 4) = 21/34 = 0.617 \\]\nque podemos interpretar como sigue: para calcular la media posterior, a nuestras \\(n\\) pruebas iniciales agregamos 4 pruebas adicionales fijas, con 2 éxitos y 2 fracasos, y calculamos la proporción usual de éxitos.\nRepite el análisis considerando en general \\(n\\) pruebas, con \\(k\\) éxitos. Utiliza la misma distribución inicial.\n\nLo mismo aplica para el intervalo de 95% (¿cómo se calcularía integrando?). También puedes usar la aproximación de R, por ejemplo:\n\n\n\nCódigo\nqbeta(0.025, shape1 = 22, shape2 = 14) |> round(2)\n\n\n[1] 0.45\n\n\nCódigo\nqbeta(0.975, shape1 = 22, shape2 = 14) |> round(2)\n\n\n[1] 0.76"
  },
  {
    "objectID": "intro-bayesiana-1.html#ejemplo-observaciones-uniformes",
    "href": "intro-bayesiana-1.html#ejemplo-observaciones-uniformes",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Ejemplo: observaciones uniformes",
    "text": "Ejemplo: observaciones uniformes\nAhora regresamos al problema de estimación del máximo de una distribución uniforme. En este caso, consideraremos un problema más concreto. Supongamos que hay una lotería (tipo tradicional) en México y no sabemos cuántos números hay. Obtendremos una muestra iid de \\(n\\) números, ya haremos una aproximación continua, suponiendo que\n\\[X_i \\sim U[0,\\theta]\\]\nLa verosimilitud es entonces\n\\[P(X_1,\\ldots, X_n|\\theta) = \\theta^{-n},\\]\ncuando \\(\\theta\\) es mayor que todas las \\(X_i\\), y cero en otro caso. Necesitaremos una inicial \\(P(\\theta)\\).\nPor la forma que tiene la verosimilitud, podemos intentar una distribución Pareto, que tiene la forma\n\\[P(\\theta) = \\frac{\\alpha \\theta_0^\\alpha}{\\theta^{\\alpha + 1}}\\]\ncon soporte en \\([\\theta_0,\\infty]\\). Tenemos que escoger entonces el mínimo \\(\\theta_0\\) y el parámetro \\(\\alpha\\). En primer lugar, como sabemos que es una lotería nacional, creemos que no puede haber menos de unos 300 mil números, así que \\(\\theta_0 = 300\\). La función acumulada de la pareto es \\(1- (300/\\theta)^\\alpha\\), así que el cuantil 99% es\n\n\nCódigo\nalpha <- 1.1\n(300/(0.01)^(1/alpha))\n\n\n[1] 19738\n\n\nes decir, alrededor de 20 millones de números. Creemos que es un poco probable que el número de boletos sea mayor a esta cota. Nótese ahora que la posterior cumple (multiplicando verosimilitud por inicial):\n\\[P(\\theta|X_1,\\ldots, X_n |\\theta) \\propto \\theta^{-(n + 2.1)}\\]\npara \\(\\theta\\) mayor que el máximo de las \\(X_n\\)’s y 300, y cero en otro caso. Esta distribución es pareto con \\(\\theta_0' = \\max\\{300, X_1,\\ldots, X_n\\}\\) y \\(\\alpha = n + 1.1\\)\nUna vez planteado nuestro modelo, veamos los datos. Obtuvimos la siguiente muestra de números:\n\n\nCódigo\nloteria_tbl <- read_csv(\"datos/nums_loteria_avion.csv\", col_names = c(\"id\", \"numero\")) |>\n  mutate(numero = as.integer(numero))\nset.seed(334)\nmuestra_loteria <- sample_n(loteria_tbl, 25) |>\n  mutate(numero = numero/1000)\nmuestra_loteria |> as.data.frame() |> head()\n\n\n  id   numero\n1 87  348.341\n2  5 5851.982\n3 40 1891.786\n4 51 1815.455\n5 14 5732.907\n6 48 3158.414\n\n\nPodemos simular de una Pareto como sigue:\n\n\nCódigo\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n\n\nSimulamos de la inicial:\n\n\nCódigo\nsims_pareto_inicial <- tibble(\n  theta = rpareto(20000, 300, 1.1 ),\n  dist = \"inicial\")\n\n\nY con los datos de la muestra, simulamos de la posterior:\n\n\nCódigo\nsims_pareto_posterior <- tibble(\n  theta = rpareto(20000,\n                  max(c(300, muestra_loteria$numero)),\n                  nrow(muestra_loteria) + 1.1),\n  dist = \"posterior\")\nsims_theta <- bind_rows(sims_pareto_inicial, sims_pareto_posterior)\nggplot(sims_theta) +\n  geom_histogram(aes(x = theta, fill = dist),\n                 bins = 70, alpha = 0.5, position = \"identity\",\n                 boundary = max(muestra_loteria$numero))  +\n  xlim(0, 15000) + scale_y_sqrt() +\n  geom_rug(data = muestra_loteria, aes(x = numero))\n\n\n\n\n\n\n\n\n\nNótese que cortamos algunos valores de la inicial en la cola derecha: un defecto de esta distribución inicial, con una cola tan larga a la derecha, es que pone cierto peso en valores que son poco creíbles y la vuelve poco apropiada para este problema. Regresamos más adelante a este problema.\nSi obtenemos percentiles, obtenemos el intervalo\n\n\nCódigo\nf <- c(0.025, 0.5, 0.975)\nsims_theta |> group_by(dist) |>\n  summarise(cuantiles = quantile(theta, f) |> round(2), f = f) |>\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n\n# A tibble: 2 × 4\n# Groups:   dist [2]\n  dist      `0.025` `0.5` `0.975`\n  <chr>       <dbl> <dbl>   <dbl>\n1 inicial      307.  569.   8449.\n2 posterior   5858. 6010.   6732.\n\n\nEstimamos entre 5.8 millones y 6.7 millones de boletos. El máximo en la muestra es de\n\n\nCódigo\nmax(muestra_loteria$numero)\n\n\n[1] 5851.982\n\n\nEscoger la distribución pareto como inicial es conveniente y nos permitió resolver el problema sin dificultad, pero por su forma vemos que no necesariamente es apropiada para el problema por lo que señalamos arriba. Nos gustaría, por ejemplo, poner una inicial como la siguiente\n\n\nCódigo\nqplot(rgamma(2000, 5, 0.001), geom=\"histogram\", bins = 20) +\n  scale_x_continuous(breaks = seq(1000, 15000, by = 2000))\n\n\n\n\n\n\n\n\n\nSin embargo, los cálculos no son tan simples en este caso, pues la posterior no tiene un forma reconocible. Tendremos que usar otras estrategias de simulación para ejemplos como este (Monte Carlo por medio de Cadenas de Markov, que veremos más adelante)."
  },
  {
    "objectID": "intro-bayesiana-1.html#probabilidad-a-priori",
    "href": "intro-bayesiana-1.html#probabilidad-a-priori",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Probabilidad a priori",
    "text": "Probabilidad a priori\nLa inferencia bayesiana es conceptualmente simple: siempre hay que calcular la posterior a partir de verosimilitud (modelo de datos) y distribución inicial o a priori. Sin embargo, una crítica usual que se hace de la inferencia bayesiana es precisamente que hay que tener esa información inicial, y que distintos analistas llegan a distintos resultados si tienen información inicial distinta.\nEso realmente no es un defecto, es una ventaja de la inferencia bayesiana. Los datos y los problemas que queremos resolver no viven en un vacío donde podemos creer que la estatura de las personas, por ejemplo, puede variar de 0 a mil kilómetros, el número de boletos de una lotería puede ir de 2 o 3 boletos o también quizá 500 millones de boletos, o la proporción de personas infectadas de una enfermedad puede ser de unos cuantos hasta miles de millones.\n\nEn todos estos casos tenemos cierta información inicial que podemos usar para informar nuestras estimaciones. Esta información debe usarse.\nAntes de tener datos, las probabilidades iniciales deben ser examinadas en términos del conocimiento de expertos.\nLas probabilidades iniciales son supuestos que hacemos acerca del problema de interés, y también están sujetas a críticas y confrontación con datos.\nPoner iniciales “no informativas” en todos los parámetros no necesariamente es buena idea. En la siguiente gráfica mostramos dos distribuciones iniciales para porcentaje de votos en modelos bayesianos para el conteo rápido. El de la derecha no usa iniciales informativas en los parámetros, lo que resulta en valores informativos para las cantidades que al final queremos estimar:\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\nSupongamos que queremos estimar la estatura de los cantantes de tesitura tenor con una muestra iid de tenores de Estados Unidos. Usaremos el modelo normal de forma que \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma^2)\\).\nUna vez decidido el modelo, tenemos que poner distribución inicial para los parámetros \\((\\mu, \\sigma^2)\\).\nComenzamos con \\(\\sigma^2\\). Checando fuentes oficiales, en la población general la desviación estándar es alrededor de 7 centímetros. Puede ser que la variabilidad de los tenores sea algo menor o mayor. Tenemos varias opciones para la distribución inicial. En este ejemplo, usaremos una gamma.\nSimulamos para calcular cuantiles probando con distintas iniciales. Esperamos que esta información sea consiste con el conocimiento que tenemos. Por ejemplo, una inicial como esta no tiene sentido:\n\n\nCódigo\nsigma <- rgamma(10000, 0.65, 1/20)\nquantile(sigma, c(0.05, 0.5, 0.95))\n\n\n        5%        50%        95% \n 0.1508418  7.4707260 46.0530573 \n\n\nporque estamos diciendo que es posible que los tenores varíen hasta en 2 *50 = 100 centímetros de estatura como población alrededor de su media. Eso es físicamente imposible.\n\n\nCódigo\nsigma_inicial <- rgamma(10000, 2, 1/4)\nquantile(sigma_inicial, c(0.05, 0.5, 0.95))\n\n\n       5%       50%       95% \n 1.424524  6.668293 18.786815 \n\n\nEsta distribución inicial es más razonable. Quizá puede haber el doble de variación que en población, y dudamos de que sean tan uniformes como para tener menos de 1cm de desviación estándar. Un histograma de la distribución inicial, que es informativa y consistente con lo que sabemos de los humanos en cuanto a su estatura:\n\n\nCódigo\nqplot(x = sigma_inicial, geom = \"histogram\")\n\n\n\n\n\n\n\n\n\nComenzamos con \\(\\mu\\). Sabemos, por ejemplo, que con alta probabilidad la media debe ser algún número entre 1.60 y 1.80. Podemos investigar: la media nacional en estados unidos está alrededor de 1.75, y el percentil 90% es 1.82. Esto es variabilidad en la población: debe ser muy poco probable, por ejemplo, que la media de tenores sea 1.82 Quizá los cantantes tienden a ser un poco más altos o bajos que la población general, así que podríamos agregar algo de dispersión.\nPodemos establecer parámetros y simular de la marginal a partir de las fórmulas de arriba para entender cómo se ve la inicial de \\(\\mu\\):\n\n\nCódigo\nmu_0 <- 175 # valor medio de inicial\ns_0 <- 5 # cuánta concentración en la inicial\nmu <- rnorm(1000, mu_0, s_0)\nquantile(mu, c(0.05, 0.5, 0.95))\n\n\n      5%      50%      95% \n166.5934 174.9179 183.3338 \n\n\nQue consideramos un rango en el que con alta probabilidad debe estar la media poblacional de los cantantes tenores.\nPodemos checar nuestros supuestos simulando posibles muestras usando sólo nuestra información previa:\n\n\nCódigo\nsimular_modelo_inicial <- function(n, pars){\n  mu_0 <- pars[1]\n  s_0 <- pars[2]\n  a_sigma <- pars[3]\n  b_sigma <- pars[4]\n  # simular media\n  sigma <- rgamma(1, a_sigma, b_sigma)\n  mu <- rnorm(1, mu_0, s_0)\n  # simular sigma\n  rnorm(n, mu, sigma)\n}\nsimular_parametros <- function(n, pars){\n  mu_0 <- pars[1]\n  s_0 <- pars[2]\n  a_sigma <- pars[3]\n  b_sigma <- pars[4]\n  # simular media\n  tibble(\n    mu = rnorm(n, mu_0, s_0),\n    sigma = rgamma(n, a_sigma, b_sigma)\n  )\n}\nset.seed(34161)\nm_0 <- 175\nsigma_0 <- 5\na_sigma <- 2\nb_sigma <- 1/4\nsims_tbl <- tibble(rep = 1:20) |>\n  mutate(estatura = map(rep, ~ simular_modelo_inicial(500, \n    c(mu_0, sigma_0, a_sigma, b_sigma)))) |>\n  unnest(cols = c(estatura))\nggplot(sims_tbl, aes(x = estatura)) + geom_histogram() +\n  facet_wrap(~ rep) +\n  geom_vline(xintercept = c(150, 180), colour = \"red\") + theme_light()\n\n\n\n\n\n\n\n\n\nPusimos líneas de referencia en 150 y 180. Vemos que nuestras iniciales no producen simulaciones totalmente fuera del contexto, y parecen cubrir apropiadamente el espacio de posiblidades para estaturas de los tenores. Quizá hay algunas realizaciones poco creíbles, pero no extremadamente. En este punto, podemos regresar y ajustar la inicial para \\(\\sigma\\), que parece tomar valores demasiado grandes (produciendo por ejemplo una simulación con estatura de 220 y 120, que deberían ser menos probables).\n\nEste paso se llama el chequeo predictivo a priori. Estamos probando que el modelo y la inicial produzca valores consistentes con el conocimiento de dominio. Este análisis puede hacerse más detallado mostrando valores de poblaciones relacionadas o datos relevantes.\n\nUna vez que estamos satisfechos con le modelo inicial, podemos considerar los datos para producir las distribuciones posteriores. En este caso, nuestra muestra es:\n\n\nCódigo\nset.seed(3413)\ncantantes <- lattice::singer |>\n  mutate(estatura_cm = round(2.54 * height)) |>\n  filter(str_detect(voice.part, \"Tenor\")) |>\n  sample_n(20)\ncantantes\n\n\n    height voice.part estatura_cm\n139     70    Tenor 1         178\n150     68    Tenor 2         173\n140     65    Tenor 1         165\n132     66    Tenor 1         168\n152     69    Tenor 2         175\n141     72    Tenor 1         183\n161     71    Tenor 2         180\n156     71    Tenor 2         180\n158     71    Tenor 2         180\n164     69    Tenor 2         175\n147     68    Tenor 1         173\n130     72    Tenor 1         183\n162     71    Tenor 2         180\n134     74    Tenor 1         188\n170     69    Tenor 2         175\n167     68    Tenor 2         173\n149     64    Tenor 1         163\n143     68    Tenor 1         173\n157     69    Tenor 2         175\n153     71    Tenor 2         180\n\n\n\nEn este caso usaremos métodos numéricos para simular de la posterior (métodos MCMC, o Markov Chain Monte Carlo).\n\nNuestro modelo en Stan se escribe como sigue, según nuestra discusión de arriba:\n\n\nCódigo\nlibrary(cmdstanr)\nmod_estaturas <- cmdstan_model(\"stan/cantantes.stan\")\nprint(mod_estaturas)\n\n\n\n\nCódigo\ncat(readLines(\"stan/cantantes.stan\"), sep = '\\n')\n\n\ndata {\n  int<lower=0> N;\n  vector[N] estaturas;\n}\n\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n\nmodel {\n  // verosimilitud\n  estaturas ~ normal(mu, sigma);\n  // iniciales\n  mu ~ normal(175, 5);\n  sigma ~ gamma(2.0, 0.5);\n}\n\ngenerated quantities {\n  vector[N] estaturas_sim;\n  // generar una muestra bajo el modelo ajustado\n  for(i in 1:N){\n    estaturas_sim[i] = normal_rng(mu, sigma);\n  }\n}\n\n\n¿Cómo se ve nuestra posterior comparada con la inicial? Podemos hacer simulaciones:\n\n\nCódigo\ny <- cantantes$estatura_cm\nN <- length(y)\ndatos_lista <- list(N = N, estaturas = y)\najuste <- mod_estaturas$sample(data = datos_lista, refresh = 1000)\nresumen <- ajuste$summary(c(\"mu\", \"sigma\"))\nwrite_rds(resumen, file = \"datos/resumen-cantantes.rds\")\n\n\n\n\nCódigo\nresumen <- read_rds(\"datos/resumen-cantantes.rds\")\nresumen\n\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 mu       176.   176.   1.38  1.32  174.   178.    1.00    3044.    2421.\n2 sigma      6.21   6.10 0.981 0.902   4.86   8.01  1.00    2361.    2184.\n\n\nY vemos que nuestra posterior es consistente con la información inicial que usamos, hemos aprendido considerablemente de la muestra. La posterior se ve como sigue. Hemos marcado también las medias posteriores de cada parámetro: media y desviación estándar.\n\n\nCódigo\nsims_post <- ajuste$draws(c(\"mu\", \"sigma\"), format = \"df\")\nwrite_rds(sims_post, file = \"datos/cantantes-sim.rds\")\n\n\n\n\nCódigo\nsims_post <- read_rds(\"datos/cantantes-sim.rds\")\nhead(sims_post)\n\n\n# A tibble: 6 × 5\n     mu sigma .chain .iteration .draw\n  <dbl> <dbl>  <int>      <int> <int>\n1  176.  6.30      1          1     1\n2  176.  6.30      1          2     2\n3  176.  7.44      1          3     3\n4  176.  5.51      1          4     4\n5  176.  7.05      1          5     5\n6  176.  4.97      1          6     6\n\n\n\n\nCódigo\nsims_inicial <- \n  simular_parametros(4000, c(mu_0, sigma_0, a_sigma, b_sigma))\nsims <- bind_rows(sims_inicial |> mutate(tipo = \"inicial\"),\n                  sims_post |> mutate(tipo = \"posterior\"))\nggplot(sims, aes(x = mu, y = sigma, colour = tipo)) +\n  geom_point() + coord_equal()\n\n\n\n\n\n\n\n\n\nPodemos construir intervalos creíbles del 90% para estos dos parámetros, por ejemplo haciendo intervalos de percentiles\n\n\nCódigo\nf <- c(0.05, 0.5, 0.95)\nsims |>\n  pivot_longer(cols = mu:sigma, names_to = \"parametro\") |>\n  group_by(tipo, parametro) |>\n  summarise(cuantil = quantile(value, f) |> round(1), f= f) |>\n  pivot_wider(names_from = f, values_from = cuantil)\n\n\n# A tibble: 4 × 5\n# Groups:   tipo, parametro [4]\n  tipo      parametro `0.05` `0.5` `0.95`\n  <chr>     <chr>      <dbl> <dbl>  <dbl>\n1 inicial   mu         167   175.   183. \n2 inicial   sigma        1.4   6.7   19.5\n3 posterior mu         174.  176.   178. \n4 posterior sigma        4.9   6.1    8  \n\n\nComo comparación, los estimadores de máxima verosimlitud son\n\n\nCódigo\nmedia_mv <- mean(cantantes$estatura_cm)\nsigma_mv <- mean((cantantes$estatura_cm - media_mv)^2) |> sqrt()\nc(media_mv, sigma_mv)\n\n\n[1] 176   6\n\n\nAhora solo resta checar que el modelo es razonable. Veremos más adelante cómo hacer esto, usando la distribución predictiva posterior."
  },
  {
    "objectID": "intro-bayesiana-1.html#pasos-de-un-análisis-de-datos-bayesiano",
    "href": "intro-bayesiana-1.html#pasos-de-un-análisis-de-datos-bayesiano",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Pasos de un análisis de datos bayesiano",
    "text": "Pasos de un análisis de datos bayesiano\nComo vimos en los ejemplos, en general un análisis de datos bayesiano sigue los siguientes pasos: - Identificar los datos releventes a nuestra pregunta de investigación, el tipo de datos que vamos a describir, que variables queremos estimar. - Definir el modelo descriptivo para los datos. La forma matemática y los parámetros deben ser apropiados para los objetivos del análisis. - Especificar la distribución inicial de los parámetros. Verificar que el modelo inicial produce datos consistentes con el conocimiento de dominio. - Utilizar inferencia bayesiana para reubicar la credibilidad a lo largo de los posibles valores de los parámetros. - Verificar que la distribución posterior replique los datos de manera razonable, de no ser el caso considerar otros modelos descriptivos para los datos."
  },
  {
    "objectID": "intro-bayesiana-1.html#verificación-predictiva-posterior",
    "href": "intro-bayesiana-1.html#verificación-predictiva-posterior",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Verificación predictiva posterior",
    "text": "Verificación predictiva posterior\nUna vez que ajustamos un modelo bayesiano, podemos simular nuevas observaciones a partir del modelo. Esto tiene dos utilidades:\n\nHacer predicciones acerca de datos no observados.\nConfirmar que nuevas producidas simuladas con el modelo son similares a las que de hecho observamos. Esto nos permite confirmar la calidad del ajuste del modelo, y se llama verificación predictiva posterior.\n\nSupongamos que tenemos la posterior \\(p(\\theta | x)\\). Podemos generar una nueva replicación de los datos como sigue:\nLa distribución predictiva posterior genera nuevas observaciones a partir de la información observada. La denotamos como \\(p(\\tilde{x}|x)\\). Para simular de ella: - Muestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\). - Simulamos del modelo de las observaciones \\(\\tilde{x} \\sim p(\\tilde{x}|\\tilde{\\theta})\\). - Repetimos el proceso hasta obtener una muestra grande. - Usamos este método para producir, por ejemplo, intervalos de predicción para nuevos datos. Si queremos una replicación de las observaciones de la predictiva posterior, - Muestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\). - Simulamos del modelo de las observaciones \\(\\tilde{x}_1, \\tilde{x}_2,\\ldots, \\tilde{x}_n \\sim p(\\tilde{x}|\\tilde{\\theta})\\), done \\(n\\) es el tamaño de muestra de la muestra original \\(x\\). - Usamos este método para producir conjuntos de datos simulados que comparamos con los observados para verificar nuestro modelo.\n\nEjemplo: estaturas de tenores\nEn este ejemplo, usaremos la posterior predictiva para checar nuestro modelo. Vamos a crear varias muestras, del mismo tamaño que la original, según nuestra predictiva posterior, y compararemos estas muestras con la observada.\n\n\nCódigo\nset.seed(82223)\nmuestras_sim <- ajuste$draws(\"estaturas_sim\", format = \"df\") |> \n  as_tibble() |> \n  pivot_longer(contains(\"estaturas_sim\"), names_to = \"variable\", \n               values_to = \"estatura\") |>\n  select(.n = .draw, estatura_cm = estatura) |> \n  filter(.n %in% sample(1:4000, 19)) |>\n  nest(c(estatura_cm)) |> \n  mutate(.n = min_rank(.n)) |> \n  unnest()\nwrite_rds(muestras_sim, file = \"datos/muestras-cantantes.rds\")\n\n\n\n\nCódigo\nmuestras_sim <- read_rds(\"datos/muestras-cantantes.rds\")\n\n\nPodemos simular varias muestras y hacer una prueba de lineup:\n\n\nCódigo\nlibrary(nullabor)\nset.seed(44165)\npos <- sample(1:20, 1)\nlineup_tbl <- lineup(true = cantantes |> select(estatura_cm),\n                     samples = muestras_sim, pos = pos)\nggplot(lineup_tbl, aes(x = estatura_cm)) + geom_histogram(binwidth = 2.5) +\n  facet_wrap(~.sample)\n\n\n\n\n\n\n\n\n\nCon este tipo de gráficas podemos checar desajustes potenciales de nuestro modelo.\n\n¿Puedes encontrar los datos verdaderos? ¿Cuántos seleccionaron los datos correctos?\nPrueba hacer pruebas con una gráfica de cuantiles. ¿Qué problema ves y cómo lo resolverías?\n\n\n\nEjemplo: modelo Poisson\nSupongamos que pensamos el modelo para las observaciones es Poisson con parámetro \\(\\lambda\\). Pondremos como inicial para \\(\\lambda\\) una exponencial con media 10.\nNótese que la posterior está dada por\n\\[p(\\lambda|x_1,\\ldots, x_n) \\propto e^{-n\\lambda}\\lambda^{\\sum_i x_i} e^{-0.1\\lambda} = \\lambda^{n\\overline{x}}e^{-\\lambda(n + 0.1)}\\]\nque es una distribución gamma con parámetros \\((n\\overline{x} + 1, n+0.1)\\)\nAhora supongamos que observamos la siguiente muestra, ajustamos nuestro modelo y hacemos replicaciones posteriores de los datos observados:\n\n\nCódigo\nx <- rnbinom(250, mu = 20, size = 3)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) |>\n  bind_rows() |>\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\n\nY vemos claramente que nuestro modelo no explica apropiadamente la variación de los datos observados. Contrasta con:\n\n\nCódigo\nset.seed(223)\nx <- rpois(250, 15)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) |>\n  bind_rows() |>\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\n\nY verificamos que en este caso el ajuste del modelo es apropiado."
  },
  {
    "objectID": "intro-bayesiana-1.html#predicción",
    "href": "intro-bayesiana-1.html#predicción",
    "title": "12  Introducción a inferencia bayesiana",
    "section": "Predicción",
    "text": "Predicción\nCuando queremos hacer predicciones particulares acerca de datos que observemos en el futuro, también podemos usar la posterior predictiva.\n\nEn el caso de los cantantes, lo usamos para replicar nuevas muestras y checar el desempeño del modelo.\n\n\nEjemplo: posterior predictiva de Pareto-Uniforme.\nLa posterior predictiva del modelo Pareto-Uniforme no tiene un nombre estándar, pero podemos aproximarla usando simulación. Usando los mismos datos del ejercicio de la lotería, haríamos:\n\n\nCódigo\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n# Simulamos de la posterior de los parámetros\nlim_inf_post <- max(c(300, muestra_loteria$numero))\nk_posterior <- nrow(muestra_loteria) + 1.1\nsims_pareto_posterior <- tibble(\n  theta = rpareto(100000, lim_inf_post, k_posterior))\n# Simulamos una observación para cada una de las anteriores:\nsims_post_pred <- sims_pareto_posterior |>\n  mutate(x_pred = map_dbl(theta, ~ runif(1, 0, .x)))\n# Graficamos\nggplot(sims_post_pred, aes(x = x_pred)) +\n  geom_histogram(binwidth = 50) +\n  geom_vline(xintercept = lim_inf_post, colour = \"red\")\n\n\n\n\n\n\n\n\n\nQue es una mezcla de una uniforme con una Pareto."
  },
  {
    "objectID": "intro-bayesiana-2.html",
    "href": "intro-bayesiana-2.html",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "",
    "text": "El enfoque bayesiano se puede formalizar coherentemente en términos de probabilidades subjetivas, y como vimos, esta es una fortaleza del enfoque bayesiano.\nEn la práctica, sin embargo, muchas veces puede ser difícil argumentar en términos exclusivos de probabilidad subjetiva, aunque hagamos los esfuerzos apropiados para incorporar la totalidad de información que distintos actores involucrados pueden tener.\nConsideremos, por ejemplo, que INEGI produjera un intervalo creíble del 95% para el ingreso mediano de los hogares de México. Aún cuando nuestra metodología sea transparente y correctamente informada, algunos investigadores interesados puede ser que tengan recelo en usar esta información, y quizá preferirían hacer estimaciones propias. Esto restaría valor al trabajo cuidadoso que pusimos en nuestras estimaciones oficiales.\nPor otra parte, el enfoque frecuentista provee de ciertas garantías mínimas para la utilización de las estimaciones, que no dependen de la interpretación subjetiva de la probabilidad, sino de las propiedades del muestreo. Consideremos la cobertura de los intervalos de confianza:\nLos intervalos creíbles en principio no tienen por qué cumplir esta propiedad, pero consideramos que en la práctica es una garantía mínima que deberían cumplir.\nEl enfoque resultante se llama bayesiano calibrado, Little (2011) . La idea es seguir el enfoque bayesiano usual para construir nuestras estimaciones, pero verificar hasta donde sea posible que los intervalos resultantes satisfacen alguna garantía frecuentista básica.\nObservación. checar que la cobertura real es similar a la nominal es importante en los dos enfoques: frecuentista y bayesiano. Los intervalos frecuentistas, como hemos visto, generalmente son aproximados, y por lo tanto no cumplen automáticamente esta propiedad de calibración."
  },
  {
    "objectID": "intro-bayesiana-2.html#enfoque-bayesiano-y-frecuentista",
    "href": "intro-bayesiana-2.html#enfoque-bayesiano-y-frecuentista",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Enfoque bayesiano y frecuentista",
    "text": "Enfoque bayesiano y frecuentista\nLos métodos estadísticos clásicos toman el punto de vista frecuentista y se basa en los siguientes puntos (Wasserman (2013)):\n\nLa probabilidad se interpreta como un límite de frecuencias relativas, donde las probabilidades son propiedades objetivas en el mundo real.\nEn un modelo, los parámetros son constantes fijas (desconocidas). Como consecuencia, no se pueden realizar afirmaciones probabilísticas útiles en relación a éstos.\nLos procedimientos estadísticos deben diseñarse con el objetivo de tener propiedades frecuentistas bien definidas. Por ejemplo, un intervalo de confianza del \\(95\\%\\) debe contener el verdadero valor del parámetro con frecuencia límite de al menos el \\(95\\%\\).\n\nEn contraste, el acercamiento Bayesiano muchas veces se describe por los siguientes postulados:\n\nLa probabilidad describe grados de creencia, no frecuencias limite. Como tal uno puede hacer afirmaciones probabilísticas acerca de muchas cosas y no solo datos sujetos a variabilidad aleatoria. Por ejemplo, puedo decir: “La probabilidad de que Einstein tomara una taza de té el primero de agosto de \\(1948\\)” es \\(0.35\\), esto no hace referencia a ninguna frecuencia relativa sino que refleja la certeza que yo tengo de que la proposición sea verdadera.\nPodemos hacer afirmaciones probabilísticas de parámetros.\nPodemos hacer inferencia de un parámetro \\(\\theta\\) por medio de distribuciones de probabilidad. Las inferencias como estimaciones puntuales y estimaciones de intervalos se pueden extraer de dicha distribución.\n\nFinalmente, en el enfoque bayesiano calibrado (Little (2011)):\n\nUsamos el enfoque bayesiano para modelar y hacer afirmaciones probabilísticas de los parámetros.\nBuscamos cumplir las garantías frecuentistas del inciso 3)."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-estimación-de-una-proporción",
    "href": "intro-bayesiana-2.html#ejemplo-estimación-de-una-proporción",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Ejemplo: estimación de una proporción",
    "text": "Ejemplo: estimación de una proporción\nRecordamos nuestro problema de estimación de una proporcion \\(\\theta\\). Usando la distribución inicial \\(p(\\theta)\\sim \\mathsf{Beta}(2,2)\\), y la verosimilitud estándar binomial, vimos que la posterior cuando observamos \\(k\\) éxitos es \\[p(\\theta|k) \\sim \\mathsf{Beta}(k + 2, n - k + 2)\\].\nLa media posterior es\n\\[\\frac{k + 2}{n + 4} \\]\nque podemos interpretar como: agrega 2 éxitos y 2 fracasos a los datos observados y calcula la proporción de éxitos. Un intervalo posterior de credibilidad del 95% se calcula encontrando los cuantiles 0.025 y 0.975 de una \\(\\mathsf{Beta}(k + 2, n - k + 2)\\)\n\\[I_a = \\left [q_{0.025}(k+2, n+4), q_{0.975}(k+2, n+4)\\right ]\\]\nQue compararemos con el intervalo usual de Wald: si \\(\\hat{\\theta} = \\frac{k}{n}\\), entonces\n\\[I_w = \\left [\\hat{\\theta} - 2 \\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}, \\hat{\\theta} + 2 \\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}\\right]\\]\n¿Cómo podemos comparar la calibración de estos dos intervalos? Nominalmente, deben tener cobertura de 95%. Hagamos un ejercicio de simulación para distintos tamaños de muestra \\(n\\) y posibles valores \\(\\theta\\in (0,1)\\):\n\n\nCódigo\nset.seed(332)\nsimular_muestras <- function(M, n, p){\n  k = rbinom(M, n, p)\n  tibble(rep = 1:M, n = n, p = p, k = k)\n}\nintervalo_wald <- function(n, k){\n  p_hat <- k / n\n  ee_hat <- sqrt(p_hat * (1 - p_hat) / n)\n  tibble(inf = p_hat - 2 * ee_hat, sup = p_hat + 2 * ee_hat)\n}\nintervalo_bayes <- function(n, k, a = 2, b = 2){\n  a <- k + a\n  b <- n - k + b\n  tibble(inf = qbeta(0.025, a, b), sup = qbeta(0.975, a, b))\n}\nset.seed(812)\nejemplo <- simular_muestras(5, 20, 0.4)\n\n\n\n\nCódigo\nejemplo %>% mutate(intervalo = intervalo_wald(n, k)) %>% pull(intervalo) %>% \n  bind_cols(ejemplo) %>% select(-rep)\n\n\n# A tibble: 5 × 5\n     inf   sup     n     p     k\n   <dbl> <dbl> <dbl> <dbl> <int>\n1 0.0211 0.379    20   0.4     4\n2 0.228  0.672    20   0.4     9\n3 0.276  0.724    20   0.4    10\n4 0.228  0.672    20   0.4     9\n5 0.137  0.563    20   0.4     7\n\n\n\n\nCódigo\nejemplo %>% mutate(intervalo = intervalo_bayes(n, k)) %>% pull(intervalo) %>% \n  bind_cols(ejemplo) %>% select(-rep)\n\n\n# A tibble: 5 × 5\n    inf   sup     n     p     k\n  <dbl> <dbl> <dbl> <dbl> <int>\n1 0.102 0.437    20   0.4     4\n2 0.268 0.655    20   0.4     9\n3 0.306 0.694    20   0.4    10\n4 0.268 0.655    20   0.4     9\n5 0.197 0.573    20   0.4     7\n\n\n¿Cuáles de estos intervalos cubren al verdadero valor? Nótese que no podemos descalificar a ningún método por no cubrir una vez. Es fácil producir un intervalo con 100% de cobertura: (0,1). Pero no nos informa dónde es probable que esté el parámetro.\nSin embargo, podemos checar la cobertura frecuentista haciendo una cantidad grande de simulaciones:\n\n\nCódigo\nparametros <- crossing(n = c(5, 10, 30, 60, 100, 400), \n                       p = c(0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.05, 0.07, 0.1, 0.15))\nset.seed(2343)\n# simulaciones\nsimulaciones <- parametros %>% \n  mutate(muestra = map2(n, p, ~ simular_muestras(50000, .x, .y) %>% select(rep, k))) %>% \n  unnest(muestra)\n# calcular_cobertura\ncalcular_cobertura <- function(simulaciones, construir_intervalo){\n  # nombre de función\n  intervalo_nombre <- substitute(construir_intervalo) %>% as.character()\n  cobertura_tbl <- simulaciones %>% \n    mutate(intervalo  = construir_intervalo(n, k)) %>%\n    pull(intervalo) %>% \n    bind_cols(simulaciones) %>% \n    mutate(cubre = p >= inf & p <= sup) %>% \n    group_by(n, p) %>% \n    summarise(cobertura = mean(cubre), long_media = mean(sup - inf))\n  cobertura_tbl %>% mutate(tipo = intervalo_nombre)\n}\n\n\n\n\nCódigo\ncobertura_wald <- calcular_cobertura(simulaciones, intervalo_wald)\ncobertura_wald\n\n\n# A tibble: 66 × 5\n# Groups:   n [6]\n       n     p cobertura long_media tipo          \n   <dbl> <dbl>     <dbl>      <dbl> <chr>         \n 1     5 0.01     0.0483     0.0347 intervalo_wald\n 2     5 0.015    0.0733     0.0527 intervalo_wald\n 3     5 0.02     0.0954     0.0689 intervalo_wald\n 4     5 0.025    0.119      0.0862 intervalo_wald\n 5     5 0.03     0.140      0.102  intervalo_wald\n 6     5 0.035    0.165      0.120  intervalo_wald\n 7     5 0.04     0.187      0.137  intervalo_wald\n 8     5 0.05     0.227      0.167  intervalo_wald\n 9     5 0.07     0.299      0.223  intervalo_wald\n10     5 0.1      0.398      0.303  intervalo_wald\n# … with 56 more rows\n\n\n\n\nCódigo\ngraficar_cobertura <- function(cobertura_tbl){\n  ggplot(cobertura_tbl, aes(x = p, y = cobertura, colour = tipo)) +\n  geom_hline(yintercept = 0.95, colour = \"black\") +\n  geom_line() + geom_point() +\n  facet_wrap(~n) +\n  ylim(0, 1) \n}\ncobertura_wald %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\n\nLa cobertura real es mucho más baja que la nominal en muchos casos, especialmente cuando la \\(p\\) es baja y \\(n\\) es chica. Pero incluso para muestras relativamente grandes (100), la cobertura es mala si \\(p\\) es chica.\nAhora probamos nuestro método alternativo:\n\n\nCódigo\ncobertura_bayes <- calcular_cobertura(simulaciones, intervalo_bayes)\n\n\n\n\nCódigo\nbind_rows(cobertura_wald, cobertura_bayes) %>% \n  mutate(tipo = factor(tipo, levels = c('intervalo_wald', 'intervalo_bayes'))) %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\n\nY vemos que en general el intervalo de Bayes es superior al de Wald, en sentido de que su cobertura real es más cercana a la nominal. El caso donde fallan los dos es para muestras muy chicas \\(n=5, 10\\), con probabilidades de éxito chicas \\(p\\leq 0.02\\).\n\nSin embargo, si tenemos información previa acerca del tamaño de la proporción que estamos estimando, es posible obtener buena calibración con el método bayesiano.\n\nEn este caso particular, tenemos argumentos frecuentistas para utilizar el método bayesiano. Por ejemplo, si el INEGI utilizara estos intervalos creíbles, un análisis de calibración de este tipo sostendría esa decisión."
  },
  {
    "objectID": "intro-bayesiana-2.html#intervalos-de-agresti-coull",
    "href": "intro-bayesiana-2.html#intervalos-de-agresti-coull",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Intervalos de Agresti-Coull",
    "text": "Intervalos de Agresti-Coull\nUn método intermedio que se usa para obtener mejores intervalos cuando estimamos proporciones es el siguiente:\n\nAgregar dos 1’s y dos 0’s a los datos.\nUtilizar el método de Wald con estos datos modificados.\n\n\n\nCódigo\nintervalo_agresti_coull <- function(n, k){\n  p_hat <- (k + 2)/ (n + 4)\n  ee_hat <- sqrt(p_hat * (1 - p_hat) / n)\n  tibble(inf = p_hat - 2 * ee_hat, sup = p_hat + 2 * ee_hat)\n}\ncobertura_ac <- calcular_cobertura(simulaciones, intervalo_agresti_coull)\n\n\n\n\nCódigo\nbind_rows(cobertura_wald, cobertura_bayes, cobertura_ac) %>% \n  mutate(tipo = factor(tipo, levels = c('intervalo_wald', 'intervalo_bayes', 'intervalo_agresti_coull'))) %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\n\nQue tiende a ser demasiado conservador para proporciones chicas:\n\n\nCódigo\ngraficar_cobertura(cobertura_ac) +\n  ylim(c(0.9, 1))\n\n\n\n\n\n\n\n\n\nConclusión 1: Los intervalos de Agresti-Coull son una buena alternativa para estimar proporciones como sustituto de los intervalos clásicos de Wald, aunque tienden a ser muy conservadores para muestras chicas\nIdealmente podemos utilizar un método bayesiano pues normalmente tenemos información inicial acerca de las proporciones que queremos estimar."
  },
  {
    "objectID": "intro-bayesiana-2.html#incorporando-información-inicial",
    "href": "intro-bayesiana-2.html#incorporando-información-inicial",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Incorporando información inicial",
    "text": "Incorporando información inicial\nNótese que generalmente tenemos información acerca de la cantidad que queremos estimar: por ejemplo, que proporción de visitantes de un sitio web compra algo (usualmente muy baja, menos de 2%), qué proporción de personas tiene diabetes tipo 1 (una proporción muy baja, menos de 1 por millar), o qué proporción de hogares tienen ingresos trimestrales mayores a 150 mil pesos (menos de %5 con alta probabilidad).\nEn este caso, tenemos que ajustar nuestra inicial. Por ejemplo, para el problema de ingresos, podríamos usar una \\(\\mathsf{Beta}(2, 100)\\), cuyos cuantiles son:\n\n\nCódigo\n# uno de cada 100\na <- 2\nb <- 100\nbeta_sims <- rbeta(5000, a, b)\nquantile(beta_sims, c(0.01, 0.05, 0.50, 0.90, 0.99)) %>% round(3)\n\n\n   1%    5%   50%   90%   99% \n0.001 0.004 0.016 0.039 0.067 \n\n\n\n\nCódigo\nqplot(beta_sims)\n\n\n\n\n\n\n\n\n\nVeamos cómo se ven los intervalos bayesianos producidos con esta inicial:\n\n\nCódigo\ncrear_intervalo_bayes <- function(a, b){\n  intervalo_fun <- function(n, k){\n    a_post <- k + a\n    b_post <- n - k + b\n    tibble(inf = qbeta(0.025, a_post, b_post), sup = qbeta(0.975, a_post, b_post))\n  }\n  intervalo_fun\n}\nintervalo_bayes_2 <- crear_intervalo_bayes(a, b)\n\n\n\n\nCódigo\ncobertura_bayes <- calcular_cobertura(simulaciones,\n                                      intervalo_bayes_2)\n\n\n\n\nCódigo\ngraficar_cobertura(bind_rows(cobertura_bayes, cobertura_ac) %>% filter(p < 0.05)) +\n  ylim(c(0.5, 1))\n\n\n\n\n\n\n\n\n\nY vemos que la calibración es similar. Notemos sin embargo que la longitud del del intervalo bayesiano es mucho menor que el de Agresti-Coull cuando la muestra es chica:\n\n\nCódigo\nggplot(bind_rows(cobertura_bayes, cobertura_ac), \n       aes(x = p, y = long_media, colour = tipo)) +\n  geom_point() + facet_wrap(~n) \n\n\n\n\n\n\n\n\n\nCuando la muestra es chica, los intervalos de bayes son similares a los iniciales, y mucho más cortos que los de Agresti-Coull. Para muestras intermedias (50-100) los intervalos bayesianos son más informativos que los de Agresti-Coull, con calibración similar, y representan aprendizaje por encima de lo que sabíamos en la inicial. Para muestras grandes, obtenemos resultados simililares.\nPor ejemplo:\n\n\nCódigo\nset.seed(2131)\nk <- rbinom(1, 50, 0.03)\nk\n\n\n[1] 4\n\n\nCódigo\nintervalo_agresti_coull(50, k) %>% round(3)\n\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.022   0.2\n\n\nes un intervalo muy grande que puede incluir valores negativos. En contraste, el intervalo bayesiano es:\n\n\nCódigo\nintervalo_bayes_2(50, k) %>% round(3)\n\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.015 0.076\n\n\nAún quitando valores negativos, los intervalos de Agresti-Coull son mucho más anchos. La aproximación bayesiana, entonces, utiliza información previa para dar un resultado considerablemente más informativo, con calibración similar a Agresti-Coull.\n¿Aprendimos algo? Comparemos la posterior con la inicial:\n\n\nCódigo\nbeta_sims_inicial <- tibble(prop = rbeta(5000, a, b), dist = \"inicial\")\nbeta_sims_posterior <- tibble(prop = rbeta(5000, a + k, b + 50), dist = \"posterior\")\nbind_rows(beta_sims_inicial, beta_sims_posterior) %>% \n  ggplot(aes(x = prop, fill = dist)) +\n    geom_histogram(alpha = 0.5, position = \"identity\") \n\n\n\n\n\n\n\n\n\nDonde vemos que no aprendimos mucho en este caso, pero nuestras creencias sí cambiaron en comparación con la inicial.\nConclusión 2: con el enfoque bayesiano podemos obtener intervalos informativos con calibración razonable, incluso con información inicial que no es muy precisa. Los intervalos de Agresti-Coull son poco informativos para muestras chicas y/o proporciones chicas.\n\nEjemplo: porporción de hogares de ingresos grandes\nUsaremos los datos de ENIGH como ejemplo (ignorando el diseño, pero es posible hacer todas las estimaciones correctamente) para estimar el porcentaje de hogares que tienen ingreso corriente de más de 150 mil pesos al trimestre. Suponemos que la muestra del enigh es la población, y tomaremos una muestra iid de esta población. Usamos la misma inicial que mostramos arriba, que es una Beta con parámetros\n\n\nCódigo\nc(a,b)\n\n\n[1]   2 100\n\n\n\n\nCódigo\nset.seed(2521)\nmuestra_enigh <- read_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n  select(ing_cor) %>% \n  sample_n(120) %>% \n  mutate(mas_150mil = ing_cor > 150000)\n\n\nUn intervalo de 95% es entonces\n\n\nCódigo\nk <- sum(muestra_enigh$mas_150mil)\nk\n\n\n[1] 3\n\n\nCódigo\nintervalo_bayes_2(120, sum(muestra_enigh$mas_150mil)) %>% round(3)\n\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.007 0.046\n\n\nLa media posterior es\n\n\nCódigo\nprop_post <- (a + k) / (120 + b)\nprop_post\n\n\n[1] 0.02272727\n\n\nEl estimador de máxima verosimilitud es\n\n\nCódigo\nk / 120\n\n\n[1] 0.025\n\n\n¿Cuál es la verdadera proporción?\n\n\nCódigo\nread_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n  select(ing_cor) %>% \n  mutate(mas_150mil = ing_cor > 150000) %>% \n  summarise(prop_pob = mean(mas_150mil))\n\n\n# A tibble: 1 × 1\n  prop_pob\n     <dbl>\n1   0.0277\n\n\nEn este caso, nuestro intervalo cubre a la proporción poblacional."
  },
  {
    "objectID": "intro-bayesiana-2.html#inferencia-bayesiana-y-regularización",
    "href": "intro-bayesiana-2.html#inferencia-bayesiana-y-regularización",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Inferencia bayesiana y regularización",
    "text": "Inferencia bayesiana y regularización\nComo hemos visto en análisis y modelos anteriores, la posterior que usamos para hacer inferencia combina aspectos de la inicial con la verosimilitud (los datos). Una manera de ver esta combinación y sus beneficios es pensando en término de regularización de estimaciones.\n\nEn las muestras hay variación. Algunas muestras particulares nos dan estimaciones de máxima verosimilitud pobres de los parámetros de interés (estimaciones ruidosas).\nCuando esas estimaciones pobres están en una zona de baja probabilidad de la inicial, la estimación posterior tiende a moverse (o encogerse) hacia las zonas de alta probabilidad de la inicial.\nEsto filtra ruido en las estimaciones.\nEl mecanismo resulta en una reducción del error cuadrático medio, mediante una reducción de la varianza de los estimadores (aunque quizá el sesgo aumente).\n\nEsta es una técnica poderosa, especialmente para problemas complejos donde tenemos pocos datos para cada parámetro. En general, excluímos resultados que no concuerdan con el conocimiento previo, y esto resulta en mayor precisión en las estimaciones."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-modelo-normal-y-estaturas",
    "href": "intro-bayesiana-2.html#ejemplo-modelo-normal-y-estaturas",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Ejemplo: modelo normal y estaturas",
    "text": "Ejemplo: modelo normal y estaturas\nHaremos un experimento donde simularemos muestras de los datos de cantantes. Usaremos el modelo normal-gamma inverso que discutimos anteriormente, con la información inicial que elicitamos. ¿Cómo se compara la estimación de máxima verosimilitud con la media posterior?\n\n\nCódigo\n# inicial para media, ver sección anterior para discusión (normal)\nmu_0 <- 175\nn_0 <- 5\n# inicial para sigma^2 (gamma inversa)\na <- 3\nb <- 140\n\n\nPara este ejemplo chico, usaremos muestras de tamaño 5:\n\n\nCódigo\nset.seed(3413)\n# ver sección anterior para explicación de esta función\ncalcular_pars_posterior <- function(x, pars_inicial){\n  # iniciales\n  mu_0 <- pars_inicial[1]\n  n_0 <- pars_inicial[2]\n  a_0 <- pars_inicial[3]\n  b_0 <- pars_inicial[4]\n  # muestra\n  n <- length(x)\n  media <- mean(x)\n  S2 <- sum((x - media)^2)\n  # sigma post\n  a_1 <- a_0 + 0.5 * n\n  b_1 <- b_0 + 0.5 * S2 + 0.5 * (n * n_0) / (n + n_0) * (media - mu_0)^2\n  # posterior mu\n  mu_1 <- (n_0 * mu_0 + n * media) / (n + n_0)\n  n_1 <- n + n_0\n  c(mu_1, n_1, a_1, b_1)\n}\n\n\nY también de la sección anterior:\n\n\nCódigo\nsim_params <- function(m, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular sigmas\n  sims <- tibble(tau = rgamma(m, a, b)) %>% \n    mutate(sigma = 1 / sqrt(tau))\n  # simular mu\n  sims <- sims %>% mutate(mu = rnorm(m, mu_0, sigma / sqrt(n_0)))\n  sims\n}\n\n\n\n\nCódigo\n# simular muestras y calcular medias posteriores\nsimular_muestra <- function(rep, mu_0, n_0, a_0, b_0){\n  cantantes <- lattice::singer %>% \n    mutate(estatura_cm = 2.54 * height) %>% \n    filter(str_detect(voice.part, \"Tenor\")) %>% \n    sample_n(5, replace = FALSE)\n  pars_posterior <- calcular_pars_posterior(cantantes$estatura_cm,\n                                            c(mu_0, n_0, a_0, b_0))\n  medias_post <- \n    sim_params(1000, pars_posterior) %>% \n    summarise(across(everything(), mean)) %>% \n    select(mu, sigma)\n  media <- mean(cantantes$estatura_cm)\n  est_mv <- c(\"mu\" = media,\n              \"sigma\" = sqrt(mean((cantantes$estatura_cm - media)^2)))\n  bind_rows(medias_post, est_mv) %>% \n    mutate(rep = rep, tipo = c(\"media_post\", \"max_verosim\")) %>% \n    pivot_longer(mu:sigma, names_to = \"parametro\", values_to = \"estimador\")\n}\n\n\n\n\nCódigo\npoblacion <- lattice::singer %>% \n  mutate(estatura_cm = 2.54 * height) %>% \n  filter(str_detect(voice.part, \"Tenor\")) %>% \n  summarise(mu = mean(estatura_cm), sigma = sd(estatura_cm)) %>% \n  pivot_longer(mu:sigma, names_to = \"parametro\", values_to = \"valor_pob\")\n\n\n\n\nCódigo\nerrores <- map(1:2000, ~ simular_muestra(.x, mu_0, n_0, a, b)) %>%\n  bind_rows() %>% left_join(poblacion) %>% \n  mutate(error = (estimador - valor_pob))\nggplot(errores, aes(x = error, fill = tipo)) +\n  geom_histogram(bins = 20, position = \"identity\", alpha = 0.5) + facet_wrap(~parametro)\n\n\n\n\n\n\n\n\n\nVemos claramente que la estimación de la desviación estándar de nuestro modelo es claramente superior a la de máxima verosimilitud. En resumen:\n\n\nCódigo\nerrores %>% \n  group_by(tipo, parametro) %>% \n  summarise(recm = sqrt(mean(error^2)) %>% round(2)) %>% \n  arrange(parametro)\n\n\n# A tibble: 4 × 3\n# Groups:   tipo [2]\n  tipo        parametro  recm\n  <chr>       <chr>     <dbl>\n1 max_verosim mu         2.85\n2 media_post  mu         1.55\n3 max_verosim sigma      2.45\n4 media_post  sigma      1.04\n\n\nObtenemos una ganancia considerable en cuanto a la estimación de la desviación estandar de esta población. Los estimadores de la media superior son superiores a los de máxima verosimilitud en términos de error cuadrático medio.\nPodemos graficar las dos estimaciones, muestra a muestra, para entender cómo sucede esto:\n\n\nCódigo\nerrores %>% \n  select(-error) %>% \n  pivot_wider(names_from = tipo, values_from = estimador) %>% \n  filter(parametro == \"sigma\") %>% \nggplot(aes(x = max_verosim, y = media_post)) +\n  geom_abline(colour = \"red\") +\n  geom_hline(yintercept = sqrt(b/(a - 1)), lty = 2, color = 'black') + \n  geom_point() +\n  labs(subtitle = \"Estimación de sigma\") +\n  xlab(\"Estimador MV de sigma\") +\n  ylab(\"Media posterior de sigma\") +\n  coord_fixed() + \n  geom_segment(aes(x = 13, y = 11, xend = 13, yend = sqrt(b/(a - 1))), \n               colour='red', size=1, arrow =arrow(length = unit(0.5, \"cm\"))) + \n  geom_segment(aes(x = .5, y = 6, xend = .5, yend = sqrt(b/(a - 1))), \n               colour='red', size=1, arrow =arrow(length = unit(0.5, \"cm\")))\n\n\n\n\n\n\n\n\n\nNótese como estimaciones demasiado bajas o demasiada altas son contraídas hacia valores más consistentes con la inicial, lo cual resulta en menor error. El valor esperado de \\(\\sigma\\) bajo la distribución inicial se muestra como una horizontal punteada."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-estimación-de-proporciones",
    "href": "intro-bayesiana-2.html#ejemplo-estimación-de-proporciones",
    "title": "13  Calibración bayesiana y Regularización",
    "section": "Ejemplo: estimación de proporciones",
    "text": "Ejemplo: estimación de proporciones\nAhora repetimos el ejercicio\n\n\nCódigo\n# inicial\na <- 2\nb <- 100\nqbeta(c(0.01, 0.99), a, b)\n\n\n[1] 0.001477084 0.063921446\n\n\nCódigo\n# datos\ndatos <- read_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n    select(ing_cor)\n# estimaciones\nobtener_estimados <- function(datos){\n  muestra_enigh <-  datos %>% \n    sample_n(120) %>% \n    mutate(mas_150mil = ing_cor > 150000)\n  k <- sum(muestra_enigh$mas_150mil)\n  tibble(k = k, est_mv = k/120, media_post = (a + k) / (120 + b), pob = 0.02769)\n}\nestimadores_sim <- map(1:200, ~obtener_estimados(datos)) %>% \n  bind_rows() \n# calculo de errores\nerror_cm <- estimadores_sim %>% \n  summarise(error_mv = sqrt(mean((est_mv - pob)^2)),\n         error_post = sqrt(mean((media_post - pob)^2)))\nerror_cm\n\n\n# A tibble: 1 × 2\n  error_mv error_post\n     <dbl>      <dbl>\n1   0.0147    0.00928\n\n\nPodemos ver claramente que las medias posteriores están encogidas hacia valores más chicos (donde la inicial tiene densidad alta) comparadas con las estimaciones de máxima verosimilitud:\n\n\nCódigo\nestimadores_sim_ag <- estimadores_sim %>% \n  group_by(k, est_mv, media_post) %>% \n  summarise(n = n())\nggplot(estimadores_sim_ag, aes(x = est_mv, media_post, size = n)) + geom_point() +\n  geom_abline()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLittle, Roderick. 2011. «Calibrated Bayes, for Statistics in General, and Missing Data in Particular». Statist. Sci. 26 (2): 162-74. https://doi.org/10.1214/10-STS318.\n\n\nWasserman, Larry. 2013. All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  }
]