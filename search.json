[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pensamiento estadístico",
    "section": "",
    "text": "En casi todas las soluciones basadas en datos, los científicos de datos ejercen el pensamiento estadístico al diseñar estrategias de recopilación de datos, obtener evidencia para la toma de decisiones y construir modelos para predecir tendencias futuras.\nEste curso busca explicar con los principios básicos de la estadística, y su papel en el análisis de datos. Nuestro punto de vista es uno de fundamentos, con menos énfasis en recetas o técnicas particulares.\nEn particular, estudiaremos básicos de probabilidad, métodos basados en remuestreo, y en la parte final la filosofía de la inferencia bayesiana y las técnicas de modelado bayesiano utilizando estudios de casos ilustrativos.\n\n\n\n\n\nLicencia Creative Commons\n\n\nEste trabajo está bajo una licencia: Attribution-NonCommercial 4.0 International\nEres libre de adaptarlo para propósitos no comerciales otorgando el crédito correspondiente."
  },
  {
    "objectID": "est-cd-1.html",
    "href": "est-cd-1.html",
    "title": "Estadística y ciencia de datos",
    "section": "",
    "text": "El término “ciencia de datos” surgió recientemente, y hay discusión acerca de qué tan apropiado es el término, si es correcto llamarla “ciencia”, y en general, en cómo definirla exactamente. En este curso tomamos el punto de vista de que:\nDesde este punto de vista, el estándar de validez más importante en la ciencia de datos (Tukey (1962)) es su funcionamiento en la práctica, y no la adherencia a argumentos teóricos, matemáticos o estadísticos.\nIgualmente puede ser difícil definir qué es la estadística (algunos la ven como una parte o rama de las matemáticas, en un extremo, y otros la consideran algo más cercano al análisis de datos). En cualquier caso:"
  },
  {
    "objectID": "est-cd-1.html#preguntas-y-datos",
    "href": "est-cd-1.html#preguntas-y-datos",
    "title": "Estadística y ciencia de datos",
    "section": "Preguntas y datos",
    "text": "Preguntas y datos\nCuando observamos un conjunto de datos, independientemente de su tamaño, el paso inicial más importante es entender bajo qué proceso se generan los datos.\n\nA grandes rasgos, cuanto más sepamos de este proceso, mejor podemos contestar preguntas de interés.\nEn muchos casos, tendremos que hacer algunos supuestos de cómo se generan estos datos para dar respuestas (condicionales a esos supuestos).\n\n\nEjemplo: nacimientos\nComenzamos con un ejemplo de análisis exploratorio. Consideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016. Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño, pueden representarse de manera razonablemente efectiva en una visualización de serie de tiempo\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\nnacimientos <- read_rds(\"datos/nacimientos/natalidad.rds\") |>\n   ungroup() |> \n   filter(year(fecha) == 2016, month(fecha) <= 3)\n\n\nExaminamos partes del contenido de la tabla:\n\n\nCódigo\ntab_1 <- nacimientos |> \n   select(fecha, n) |> \n   slice_head(n = 5)\ntab_2 <- nacimientos |> \n   select(fecha, n) |> \n   slice_tail(n = 5)\nkable(list(tab_1, tab_2)) |> kable_styling()\n\n\n\n\n\n  \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-01-01 \n    3952 \n  \n  \n    2016-01-02 \n    4858 \n  \n  \n    2016-01-03 \n    4665 \n  \n  \n    2016-01-04 \n    5948 \n  \n  \n    2016-01-05 \n    6087 \n  \n\n\n\n \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-03-27 \n    4112 \n  \n  \n    2016-03-28 \n    5805 \n  \n  \n    2016-03-29 \n    5957 \n  \n  \n    2016-03-30 \n    5766 \n  \n  \n    2016-03-31 \n    5497 \n  \n\n\n\n \n  \n\n\n\n\n\nEn un examen rápido de estos números no vemos nada fuera de orden. Los datos tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar de manera simple como sigue:\n\n\nCódigo\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\n\nEsta es una descripción de los datos, que quizá no es muy compacta pero muestra varios aspectos importantes. En este caso notamos algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:\n\n\nCódigo\ndomingos_tbl <- nacimientos |> \n   filter(weekdays(fecha) == \"Sunday\")\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = \"salmon\") +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\n\nObservamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente menos nacimentos. ¿Por qué crees que sea esto?\nAdicionalmente a estos patrones observamos otros aspectos interesantes:\n\nEl primero de enero hay considerablemente menos nacimientos de los que esperaríamos para un viernes. ¿Por qué?\nEl primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial este primero de marzo?\n¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?\n¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?\n\nLa confirmación de estas hipótesis, dependiendo de su forma, puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados con otros días de la semana) hasta muy compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes, procesos en el registro civil, etc.) En todo caso, una descripción correcta de estos datos requiere conocer tanto hechos generales como conocimiento detallado de prácticas relacionadas con la natalidad y el registro de nacimientos.\n\nEl análisis exploratorio y descripción de los datos requiere también conocimiento de dominio: ¿qué cosas intervienen en el proceso que genera estos datos?\n\n\n\nEjemplo (cálculos renales)\nEste es un estudio real acerca de tratamientos para cálculos renales (Julious y Mullee (1994)). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.\nLa tabla original tiene 700 renglones (cada renglón es un paciente)\n\n\nCódigo\ncalculos <- read_csv(\"./datos/kidney_stone_data.csv\")\nnames(calculos) <- c(\"tratamiento\", \"tamaño\", \"éxito\")\ncalculos <- calculos |> \n   mutate(tamaño = ifelse(tamaño == \"large\", \"grandes\", \"chicos\")) |> \n   mutate(resultado = ifelse(éxito == 1, \"mejora\", \"sin_mejora\")) |> \n   select(tratamiento, tamaño, resultado)\nnrow(calculos)\n\n\n[1] 700\n\n\ny se ve como sigue (muestreamos algunos renglones):\n\n\nCódigo\ncalculos |> \n   sample_n(20) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n  \n \n\n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    B \n    chicos \n    sin_mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n\n\n\n\n\nAunque estos datos contienen información de 700 pacientes, los datos pueden resumirse sin pérdida de información contando como sigue:\n\n\nCódigo\ncalculos_agregada <- calculos |> \n   group_by(tratamiento, tamaño, resultado) |> \n   count()\ncalculos_agregada |> kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n    n \n  \n \n\n  \n    A \n    chicos \n    mejora \n    81 \n  \n  \n    A \n    chicos \n    sin_mejora \n    6 \n  \n  \n    A \n    grandes \n    mejora \n    192 \n  \n  \n    A \n    grandes \n    sin_mejora \n    71 \n  \n  \n    B \n    chicos \n    mejora \n    234 \n  \n  \n    B \n    chicos \n    sin_mejora \n    36 \n  \n  \n    B \n    grandes \n    mejora \n    55 \n  \n  \n    B \n    grandes \n    sin_mejora \n    25 \n  \n\n\n\n\n\nEste resumen no es muy informativo, pero al menos vemos qué valores aparecen en cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:\n\n\nCódigo\ncalculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, tamaño, total, prop_mejora) |> \n   arrange(tamaño) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    chicos \n    87 \n    0.93 \n  \n  \n    B \n    chicos \n    270 \n    0.87 \n  \n  \n    A \n    grandes \n    263 \n    0.73 \n  \n  \n    B \n    grandes \n    80 \n    0.69 \n  \n\n\n\n\n\nEsta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Pero es apropiada para empezar a contestar la pregunta:\n\n¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?\n\nSupongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:\n\n\nCódigo\ncalculos |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\ny parece ser que el tratamiento \\(B\\) es mejor que el \\(A\\). Esta es una paradoja (un ejemplo de la paradoja de Simpson) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar \\(B\\)? ¿Si sabe debería recetar \\(A\\)? Esta discusión parece no tener mucho sentido.\nPodemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:\n\n\nCódigo\ncalculos |> group_by(tratamiento, tamaño) |> count() |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    tamaño \n    n \n  \n \n\n  \n    A \n    chicos \n    87 \n  \n  \n    A \n    grandes \n    263 \n  \n  \n    B \n    chicos \n    270 \n  \n  \n    B \n    grandes \n    80 \n  \n\n\n\n\n\nNuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento \\(A\\) para cálculos grandes, y \\(B\\) para cálculos chicos. Esto quiere decir que en la tabla total el tratamiento \\(A\\) está en desventaja porque se usa en casos más difíciles, pero el tratamiento \\(A\\) parece ser en general mejor.\n\nUna mejor respuesta a la pregunta de qué tratamiento es mejor es la que presenta los datos desagregados\nLa tabla desagregada de asignación del tratamiento nos informa acerca de cómo se está distribuyendo el tratamiento en los pacientes.\n\nIgual que en el ejemplo anterior, los resúmenes descriptivos están acompañados de hipótesis acerca del proceso generador de datos, y esto ilumina lo que estamos observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos.\n\n\nEjemplo (cálculos renales 2)\nContrastemos el ejemplo anterior usando exactamente los mismos datos, pero con una interpretación diferente. En este caso, los tratamientos son para mejorar alguna enfermedad del corazón. Sabemos que parte del efecto de este tratamiento ocurre gracias a una baja en presión arterial de los pacientes, así que después de administrar el tratamiento, se toma la presión arterial de los pacientes. Ahora tenemos la tabla agregada y desagregada como sigue:\n\n\nCódigo\ncorazon <- calculos |> \n  select(tratamiento, presión = tamaño, resultado) |> \n  mutate(presión = ifelse(presión == \"grandes\", \"alta\", \"baja\"))\ncorazon_agregada <- corazon |> \n   group_by(tratamiento, presión, resultado) |> \n   count()\ncorazon_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, presión, total, prop_mejora) |> \n   arrange(presión) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    presión \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    alta \n    263 \n    0.73 \n  \n  \n    B \n    alta \n    80 \n    0.69 \n  \n  \n    A \n    baja \n    87 \n    0.93 \n  \n  \n    B \n    baja \n    270 \n    0.87 \n  \n\n\n\n\n\n\n\nCódigo\ncorazon |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable()\n\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\n¿Cuál creemos que es el mejor tratamiento en este caso? ¿Deberíamos usar la tabla agregada o la desagregada por presión?\n\nEn este caso, la tabla agregada es más apropiada (B es mejor tratamiento).\nLa razón es que presión en este caso es una consecuencia de tomar el tratamiento, y como las tablas muestran, B es más exitoso en bajar la presión de los pacientes.\nSi sólo comparamos dentro de los grupos de presión baja o de presión alta, ignoramos parte del efecto del tratamiento en la probabilidad de mejorar."
  },
  {
    "objectID": "est-cd-1.html#diagramas-causales",
    "href": "est-cd-1.html#diagramas-causales",
    "title": "Estadística y ciencia de datos",
    "section": "Diagramas causales",
    "text": "Diagramas causales\nPodemos utilizar diagramas causales introducidos por Judea Pearl (Pearl, Glymour, y Jewell (2016)) para explicar por qué el análisis se hace de manera diferente en cada uno de los casos de arriba. Los diagramas causales son representaciones de nuestro conocimiento de dominio acerca de cómo se relacionan de manera causal las variables de interés. En el caso de cálculos renales, podemos escribir el diagrama como sigue:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Tratamiento\" [exposure,pos=\"-3,0\"]\n  \"Resultado\" [outcome,pos=\"3,0\"]\n  \"Tamaño\" [pos=\"0,1\"]\n  \"Tamaño\"  -> \"Tratamiento\"\n    \"Tamaño\" -> \"Resultado\"\n    \"Tratamiento\" -> \"Resultado\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag() \n\n\n\n\n\n\nEl tamaño de los cálculos afecta al resultado y a la asignación del tratamiento. Es un confusor si queremos entender el efecto del tratamiento en el resultado.\nPartimos los datos según este confusor para “comparar peras con peras”.\n\nSin embargo, en el segundo ejemplo tenemos:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Tratamiento\" [exposure,pos=\"-3,0\"]\n  \"Resultado\" [outcome,pos=\"3,0\"]\n  \"Presión\" [pos=\"0,1\"]\n  \"Tratamiento\"  -> \"Presión\"\n    \"Presión\" -> \"Resultado\"\n    \"Tratamiento\" -> \"Resultado\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\n\nEn este caso, la presión es consecuencia también del tratamiento, así que es un camino por medio del cual el tratamiento produce resultados.\nComparar el tratamiento dentro de grupos de presión alta o baja estima el efecto del tratamiento que no tiene qué ver con la regulación de la presión, lo cual da una respuesta incompleta.\n\nAdicionalmente, en ambos ejemplos, estamos suponiendo que no existen otras variables confusoras que puedan afectar nuestro análisis. Qué tan correcta es esa suposición depende de que conozcamos los detalles de cómo fueron recopilados estos datos.\n\nEjemplo: prevalencia de anemia\nEn un estudio de hospitales Australia se registró que 57% de una muestra pacientes tenían anemia cuando fueron ingresados. ¿Qué podemos decir acerca de la prevalencia de anemia en la población general de Australia? Con información básica acerca del proceso generador de esta muestra podemos concluir que será difícil generalizar con estos datos a la población general. La razón es que:\n\nMuchas enfermedades graves (por ejemplo del corazón) pueden producir anemia.\nEstas enfermededes hacen más probable que alguien sea hospitalizado.\nPor lo tanto, en este estudio hay una asociación entre tener anemia y ser seleccionado para el estudio (tener anemia sube la probabilidad de ser seleccionado para el estudio)\nNuestra conclusión es que el 57% es probablemente una sobreestimación de la prevalencia de anemia en la población.\n\n\n\nCódigo\ndag_1 <- dagitty('dag{\"Selección\" [exposure, pos = \"2,1\"]\n  \"Anemia\" [outcome, pos = \"-1, 2.5\"]\n  \"Hospitalización\" [pos=\"1,2\"]\n  \"Enfermedad\" [pos=\"0, 3\"]\n  \"Hospitalización\" -> \"Selección\"\n    \"Enfermedad\" -> \"Anemia\"\n    \"Anemia\" -> \"Hospitalización\"\n    \"Enfermedad\" -> \"Hospitalización\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\nEste diagrama indica que puede ser difícil generalizar con las personas que han sido seleccionadas, porque tanto la selección como la variable de interés tienen una causa común: la existencia o no de una enfermedad en la persona. Será difícil generalizar para las personas no observadas en el estudio.\n\n\nEjemplo: colisionadores 1\nAlgunos estudios fueron publicados en la primera mitad de 2020 que notaban que el porcentaje fumadores entre los casos positivos de COVID era menor que en la población general, y se hicieron algunas interpretaciones acerca de este hecho. Estos estudios se hicieron con personas que se hicieron una prueba.\nEn este ejemplo replicaremos cómo es que podemos encontrar esta asociación en este tipo de estudios aún cuando no exista tal asociación en la población general. Usaremos datos sintéticos (simulados).\nPrimero vamos a razonar acerca del proceso generador de datos y a hacer algunos supuestos:\n\nEn primer lugar, ¿cuándo decide hacerse alguien una prueba? A principios de 2020, son principalmente personas que tienen síntomas considerables, y trabajadores de salud (tengan o no síntomas).\nSer trabajador de salud incrementa el riesgo de contagiarse.\nEn algunos países, fumar está asociado con ser trabajador de salud (no tienen la misma tasa de tabaquismo que la población general).\nSólo observamos a las personas que se hicieron una prueba.\n\nPodemos resumir cualitativamente con el siguiente diagrama:\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\ndag_1 <- dagitty('dag{\"Covid\" [outcome, pos = \"-0.5, 2.5\"]\n  \"Prueba\" [pos=\"0,-2\"]\n  \"TrabSalud\" [pos=\"0, 3\"]\n  \"Sintomas\" [pos=\"-1, 1\"]\n  \"Fumar\" [pos = \"1, 1\"]\n  \"TrabSalud\" -> \"Covid\"\n  \"TrabSalud\" -> \"Prueba\"\n  \"TrabSalud\" -> \"Fumar\"\n  \"Covid\" -> \"Sintomas\"\n  \"Sintomas\" -> \"Prueba\"\n  }')\ndag_1_tidy <- tidy_dagitty(dag_1) \ndag_1_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\nEl código para simular es el siguiente: todas las variables toman valores 0 o 1, pero con diferentes probabilidades y dependiendo de las variables que son padres en la gráfica de arriba:\n\n\nCódigo\nset.seed(821)\n#simular población\nn <- 1000000\ntrab_salud <- rbinom(n, 1, 0.01)\ncovid <- rbinom(n, 1, ifelse(trab_salud==1, 0.04, 0.01))\ndatos <- tibble(trab_salud = trab_salud, covid) |> \n  mutate(sintomas = rbernoulli(n, ifelse(covid == 1, 0.5, 0.01))) |> \n  mutate(prueba = rbernoulli(n, ifelse(trab_salud ==1, 0.99, 0.6 * sintomas + 0.01))) |> \n  mutate(fumar = rbernoulli(n, ifelse(trab_salud == 1, 0.3, 0.1))) |> \n  mutate(covid = ifelse(covid ==1, \"positivo\", \"negativo\")) |> \n  mutate(fumar = ifelse(fumar, \"fuma\", \"no_fuma\"))\n\n\nSuponemos ahora que tomamos como muestra a todas aquellas personas que se hicieron una prueba. En primer lugar, la proporción de fumadores en la muestra es un poco más alta que la población, porque los trabajadores de salud están sobrerrepresentados\n\n\nCódigo\ndatos_pruebas <- filter(datos, prueba == 1)\ntable(datos_pruebas$fumar) |> prop.table()\n\n\n\n     fuma   no_fuma \n0.1712317 0.8287683 \n\n\nY ahora vemos que están asociados fumar y salir positivo:\n\n\nCódigo\ntable(datos_pruebas$covid, datos_pruebas$fumar) |> prop.table(margin = 2) |> \n  round(2)\n\n\n          \n           fuma no_fuma\n  negativo 0.91    0.87\n  positivo 0.09    0.13\n\n\nEn la población no existe tal asociación, además de que la tasa de positivos es considerablemente más baja:\n\n\nCódigo\ntable(datos$covid, datos$fumar) |> prop.table(margin = 2) |> \n  round(3)\n\n\n          \n            fuma no_fuma\n  negativo 0.989   0.990\n  positivo 0.011   0.010\n\n\n\nEn este ejemplo, al seleccionar sólo aquellas personas que tomaron una prueba, cambia la relación entre tener covid y ser trabajador de salud, pues sólo los que tienen síntomas de la población general toman la prueba. Esto produce que una prueba negativa esté más relacionada con ser trabajador de salud, y por lo tanto, mayor probabilidad de ser fumador.\nTambién puede entenderse pensando que cuando tomamos solamente las personas que se hicieron pruebas, entonces los trabajadores de salud están sobrerrepresantados en la muestra.\n\n\n\nEjemplo: colisionador 2\n\nSupongamos que queremos entender la relación en desempeño en matemáticas y en español para estudiantes que entran a una universidad.\nEncontramos una relación negativa entre las calificaciones de los dos exámenes: parece ser que habilidad verbal se contrapone a habilidad numérica.\n\n¿Por qué tenemos que tener cuidado al interpretar esta correlación? ¿Existe esta correlación en la población general?\n\nDescubrimos que la universidad hace una calificación compuesta de español y matemáticas para que los alumnos sean aceptados en la universidad\nEsto quiere decir que para entrar es necesario al menos desempeñarse bien en alguna de las dos\n\nAhora observamos que aunque en la población general no hay tal relación, al seleccionar sólo a los alumnos de la universidad “activamos” una correlación debido al proceso de selección:\n\n\nCódigo\nset.seed(823)\ntibble(x = rnorm(2000), y = rnorm(2000)) |> \n  mutate(aceptados = x + y > 1.5 ) |> \nggplot(aes(x, y, colour = aceptados)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "est-cd-1.html#procesos-generadores-de-datos",
    "href": "est-cd-1.html#procesos-generadores-de-datos",
    "title": "Estadística y ciencia de datos",
    "section": "Procesos generadores de datos",
    "text": "Procesos generadores de datos\nNótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales y de dominio para interpretar y hacer hipótesis acerca de lo que vemos en la gráfica. Una visión descontextualizada no tiene mucha utilidad. Las explicaciones son típicamente complejas e intervienen distintos aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados.\n\n\n\n\n\n\nEl proceso generador de datos\n\n\n\nAl conjunto de esos aspectos que determinan los datos que finalmente observamos le llamamos el proceso generador de datos. Para datos que observamos “naturalmente” este proceso generalmente es complicado.\n\n\nEn la Ciencia de Datos buscamos entender las partes importantes del proceso generador\n\nLa descripción correcta de los datos se logra con ese entendimiento de dominio y del proceso generador.\nLa formulación y refinamiento de preguntas importantes y sus respuestas acerca de estos datos requiere entendimiento de dominio y del proceso generador.\nMás tarde, veremos que la inferencia estadística también depende de este entendimiento, junto con propuestas del diseño estadístico que nos permite obtener los datos necesarios para simplificar y dar certeza en el proceso de contestar las preguntas de interés.\n\nMucha parte de este trabajo no es estadístico, sino que es un esfuerzo por entender el dominio (como sugiere el título de artículo de David A. Friedman: Statistical Models and Shoe Leather)."
  },
  {
    "objectID": "est-cd-1.html#ejercicio-admisiones-de-berkeley",
    "href": "est-cd-1.html#ejercicio-admisiones-de-berkeley",
    "title": "Estadística y ciencia de datos",
    "section": "Ejercicio: admisiones de Berkeley",
    "text": "Ejercicio: admisiones de Berkeley\nConsideramos ahora los siguientes datos de admisión a distintos departamentos de Berkeley en 1975:\n\n\nCódigo\ndata(\"UCBAdmissions\")\nadm_original <- UCBAdmissions |> as_tibble() |> \n   pivot_wider(names_from = Admit, values_from = n) \nadm_original |> knitr::kable()\n\n\n\n\n \n  \n    Gender \n    Dept \n    Admitted \n    Rejected \n  \n \n\n  \n    Male \n    A \n    512 \n    313 \n  \n  \n    Female \n    A \n    89 \n    19 \n  \n  \n    Male \n    B \n    353 \n    207 \n  \n  \n    Female \n    B \n    17 \n    8 \n  \n  \n    Male \n    C \n    120 \n    205 \n  \n  \n    Female \n    C \n    202 \n    391 \n  \n  \n    Male \n    D \n    138 \n    279 \n  \n  \n    Female \n    D \n    131 \n    244 \n  \n  \n    Male \n    E \n    53 \n    138 \n  \n  \n    Female \n    E \n    94 \n    299 \n  \n  \n    Male \n    F \n    22 \n    351 \n  \n  \n    Female \n    F \n    24 \n    317 \n  \n\n\n\n\n\nCon algo de manipulación podemos ver tasas de admisión para Male y Female, y los totales de cada grupo que solicitaron en cada Departamento.\n\n\nCódigo\nadm_tbl <- adm_original |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected), 2), total = Admitted + Rejected) |> \n   select(Gender, Dept, prop_adm, total) |> \n   pivot_wider(names_from = Gender, values_from = prop_adm:total)\nadm_tbl |> knitr::kable()\n\n\n\n\n \n  \n    Dept \n    prop_adm_Male \n    prop_adm_Female \n    total_Male \n    total_Female \n  \n \n\n  \n    A \n    0.62 \n    0.82 \n    825 \n    108 \n  \n  \n    B \n    0.63 \n    0.68 \n    560 \n    25 \n  \n  \n    C \n    0.37 \n    0.34 \n    325 \n    593 \n  \n  \n    D \n    0.33 \n    0.35 \n    417 \n    375 \n  \n  \n    E \n    0.28 \n    0.24 \n    191 \n    393 \n  \n  \n    F \n    0.06 \n    0.07 \n    373 \n    341 \n  \n\n\n\n\n\nY complementamos con las tasas de aceptación a total por género, y tasas de aceptación por departamento:\n\n\nCódigo\nadm_original |> group_by(Gender) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n\n \n  \n    Gender \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    Female \n    557 \n    1278 \n    0.30 \n  \n  \n    Male \n    1198 \n    1493 \n    0.45 \n  \n\n\n\n\n\n\n\nCódigo\nadm_original |> group_by(Dept) |> \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |> \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |> \n   kable()\n\n\n\n\n \n  \n    Dept \n    Admitted \n    Rejected \n    prop_adm \n  \n \n\n  \n    A \n    601 \n    332 \n    0.64 \n  \n  \n    B \n    370 \n    215 \n    0.63 \n  \n  \n    C \n    322 \n    596 \n    0.35 \n  \n  \n    D \n    269 \n    523 \n    0.34 \n  \n  \n    E \n    147 \n    437 \n    0.25 \n  \n  \n    F \n    46 \n    668 \n    0.06 \n  \n\n\n\n\n\n\n¿Qué observas acerca de las tasas de admisión en cada departamento, diferenciadas por género? ¿Qué tiene qué ver con el número de personas que solicitan en cada departamento?\nEsta es una tabla descriptiva. Sin embargo, tiene que ser entendida en el contexto de los datos y su generación. ¿Qué hipótesis importantes sugieren estos datos? ¿Por qué hay tanta diferencia de género de solicitudes en algunos departamentos? ¿Por qué es sorprendente o no las variaciones en tasas de aceptación de estudiantes de cada género?"
  },
  {
    "objectID": "est-cd-1.html#diseño-estadístico-e-inferencia",
    "href": "est-cd-1.html#diseño-estadístico-e-inferencia",
    "title": "Estadística y ciencia de datos",
    "section": "Diseño estadístico e inferencia",
    "text": "Diseño estadístico e inferencia\nUna primera contribución importante de la estadística al análisis de datos contesta la siguiente pregunta:\n\nEl análisis correcto depende del proceso generador de datos\nIncluso cuando tenemos conocimiento detallado de dominio, es posible que algunos de nuestros supuestos sean cuestionables.\n\nSin embargo,\n\nSi pudiéramos alterar el proceso generador de datos de alguna manera razonable, ¿sería posible hacer un análisis que dependa de menos supuestos?\nEn lugar de usar los datos que tenemos a la mano, ¿podemos pensar en una manera de producir los datos que nos de más certeza acerca de las conclusiones que extraemos de ellos, y que nos permita extraer la mayor información posible?\n\nEl diseño estadístico (de experimentos, o de muestreo por ejemplo) nos guía a cómo modificar el proceso generador para simplificar el análisis, y en ese caso nos provee de herramientas para contestar preguntas de interés y cuantificar la incertidumbre ne las respuestas. Veremos más adelante por qué, pero por lo pronto señalamos alguna característica central:\n\nEn los ejemplos que vimos arriba, ocurren dificultades porque la aplicación del tratamiento o la selección de individuos depende de una variable relacionada también con la variable respuesta que nos interesa medir. Veremos que podemos usar aleatorización para cortar estas dependencias.\nEl diseño de muestras y experimentos también nos provee herramientas para decidir cuántos datos necesitamos y de qué tipo para dar respuestas con suficiente precisión para nuestros propósitos.\n\n\n\n\n\nJulious, Steven A, y Mark A Mullee. 1994. «Confounding and Simpson’s paradox». BMJ 309 (6967): 1480-81. https://doi.org/10.1136/bmj.309.6967.1480.\n\n\nPearl, J., M. Glymour, y N. P. Jewell. 2016. Causal Inference in Statistics: A Primer. Wiley. https://books.google.com.mx/books?id=L3G-CgAAQBAJ.\n\n\nTukey, John W. 1962. «The Future of Data Analysis». Ann. Math. Statist. 33 (1): 1-67. https://doi.org/10.1214/aoms/1177704711."
  },
  {
    "objectID": "analisis-resumenes-1.html",
    "href": "analisis-resumenes-1.html",
    "title": "1  Resúmenes para datos numéricos",
    "section": "",
    "text": "En esta parte establecemos las herramientas básicas que utilizaremos para describir y explorar conjuntos de datos. En esta parte veremos cómo analizar y comparar distintos bonches de datos numéricos."
  },
  {
    "objectID": "analisis-resumenes-1.html#cuantiles-o-percentiles-de-una-variable",
    "href": "analisis-resumenes-1.html#cuantiles-o-percentiles-de-una-variable",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.1 Cuantiles o percentiles de una variable",
    "text": "1.1 Cuantiles o percentiles de una variable\nEl primer concepto se refiere a entender cómo se distribuyen los datos a los largo de su escala de medición. Comenzamos con un ejemplo: los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos.\n\n\nCódigo\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\nslice_sample(propinas, n = 10) \n\n\n# A tibble: 10 × 6\n   cuenta_total propina fumador dia   momento num_personas\n          <dbl>   <dbl> <chr>   <chr> <chr>          <dbl>\n 1        11.2     1.76 Si      Sab   Cena               2\n 2        17.9     3.08 Si      Sab   Cena               2\n 3        16.5     2    No      Dom   Cena               4\n 4        29.0     5.92 No      Sab   Cena               3\n 5        16.0     3    No      Vie   Comida             3\n 6        18.2     3.5  Si      Dom   Cena               3\n 7        22.2     5    No      Dom   Cena               2\n 8         8.58    1.92 Si      Vie   Comida             1\n 9        14.1     2.5  No      Dom   Cena               2\n10         8.52    1.48 No      Jue   Comida             2\n\n\nAquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta. Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día (Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida.\nEl primer tipo de comparaciones que nos interesa hacer es para una medición numérica es: ¿Varían mucho o poco los datos? ¿Cuáles son valores típicos o centrales? ¿Existen valores muy extremos alejados de valores típicos?\nSupongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales:\n\n\nCódigo\npropinas <- propinas |> \n  mutate(orden_cuenta = rank(cuenta_total, ties.method = \"first\"), \n         f = orden_cuenta / n()) \ncuenta <- propinas |>  select(orden_cuenta, f, cuenta_total) |>  arrange(f)\nbind_rows(head(cuenta), tail(cuenta)) |>  knitr::kable()\n\n\n\n\n \n  \n    orden_cuenta \n    f \n    cuenta_total \n  \n \n\n  \n    1 \n    0.0040984 \n    3.07 \n  \n  \n    2 \n    0.0081967 \n    5.75 \n  \n  \n    3 \n    0.0122951 \n    7.25 \n  \n  \n    4 \n    0.0163934 \n    7.25 \n  \n  \n    5 \n    0.0204918 \n    7.51 \n  \n  \n    6 \n    0.0245902 \n    7.56 \n  \n  \n    239 \n    0.9795082 \n    44.30 \n  \n  \n    240 \n    0.9836066 \n    45.35 \n  \n  \n    241 \n    0.9877049 \n    48.17 \n  \n  \n    242 \n    0.9918033 \n    48.27 \n  \n  \n    243 \n    0.9959016 \n    48.33 \n  \n  \n    244 \n    1.0000000 \n    50.81 \n  \n\n\n\n\n\ny graficamos los datos en orden, interpolando valores consecutivos.\n\n\n\n\n\nA esta función le llamamos la función de cuantiles para la variable cuenta total. Nos sirve para comparar directamente los distintos valores que observamos los datos según el orden que ocupan.\n\n\n\n\n\n\nCuantiles de datos numéricos\n\n\n\nEl cuantil \\(f\\) de un bonche de datos numéricos es el valor \\(q(f)\\), en la escala de medición de nuestros datos, tal que aproximadamente una fracción \\(f\\) de los datos está por abajo de \\(q(f)\\).\n\nAl cuantil \\(f=0.5\\) le llamamos la mediana.\nA los cuantiles \\(f=0.25\\) y \\(f=0.75\\) les llamamos cuantiles inferior y superior.\n\n\n\nNota: si los datos originales son \\(y_1, y_2, \\ldots, y_n\\), y los mismos datos ordenados son \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), entonces si \\(f= j/n\\), \\(q(f) = y_{(j)}\\). Si \\(f\\) toma un valor intermedio entre \\((j-1)/n\\) y \\(j/n\\), entonces interpolamos \\(y_{(j-1)}\\) y \\(y_{(j)}\\) para encontrar \\(q(f)\\).\nHay otras maneras de definir los cuantiles que pueden ser más convenientes. Los que estamos usando ahora son los cuantiles tipo 4:\n\nquantile(cuenta$cuenta_total, probs = c(6/244, 239/244), type = 4)\n\n2.459016% 97.95082% \n     7.56     44.30 \n\n\n¿Qué podemos leer en la gráfica de cuantiles?\nDispersión y valores centrales\n\nEl rango de datos va de unos 3 dólares hasta 50 dólares\nLos valores centrales (del cuantil 0.25 al 0.75, por ejemplo), están entre unos 13 y 25 dólares\nPodemos usar el cuantil 0.5 (mediana) para dar un valor central de esta distribución, que está alrededor de 18 dólares.\n\nY podemos dar resúmenes más refinados si es necesario\n\nEl cuantil 0.95 es de unos 35 dólares - sólo 5% de las cuentas son de más de 35 dólares\nEl cuantil 0.05 es de unos 8 dólares - sólo 5% de las cuentas son de 8 dólares o menos.\n\nFinalmente, la forma de la gráfica se interpreta usando su pendientes, haciendo comparaciones de diferentes partes de la gráfica:\n\nEntre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente es baja, lo que significa que al avanzar en los cuantiles, los valores observados no cambian mucho.\nCuando la pendiente es alta, quiere decir que los datos tienen más dispersión local o están más separados.\n\nY podemos considerar qué sucede en las colas de la distribucion:\n\nLa distribución de valores tiene asimetría: el 10% de las cuentas más altas tiene considerablemente más dispersión que el 10% de las cuentas más bajas. A veces decimos que la cola de la derecha es más larga que la cola de la izquierda\n\nEn algunos casos, es más natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos cuántos datos caen en cada cubeta:\n\n\n\n\n\nEs una gráfica más popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos.\nFinalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión menos común de Spear/Tufte (ST):\n\n\nCódigo\nlibrary(ggthemes)\ncuartiles <- quantile(cuenta$cuenta_total)\ncuartiles\n\n\n     0%     25%     50%     75%    100% \n 3.0700 13.3475 17.7950 24.1275 50.8100 \n\n\nCódigo\ng_1 <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + \n  labs(subtitle = \"Gráfica de cuantiles: Cuenta total\") +\n  geom_hline(yintercept = cuartiles[2], colour = \"gray\") + \n  geom_hline(yintercept = cuartiles[3], colour = \"gray\") +\n  geom_hline(yintercept = cuartiles[4], colour = \"gray\") +\n  geom_point(alpha = 0.5) + geom_line() \ng_2 <- ggplot(cuenta, aes(x = factor(\"ST\", levels =c(\"ST\")), y = cuenta_total)) + \n  geom_tufteboxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_3 <- ggplot(cuenta, aes(x = factor(\"T\"), y = cuenta_total)) + geom_boxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_4 <- ggplot(cuenta, aes(x = factor(\"P\"), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_1 + g_2 + g_3 + g_4 +\n  plot_layout(widths = c(8, 2, 2, 2))"
  },
  {
    "objectID": "analisis-resumenes-1.html#distribución-acumulada-empírica",
    "href": "analisis-resumenes-1.html#distribución-acumulada-empírica",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.2 Distribución acumulada empírica",
    "text": "1.2 Distribución acumulada empírica\nOtra forma de graficar la dispersión de los datos sin perder información es mediante la función de distribución acumulada empírica, o fda empírica. En un sentido, es la inversa de la función de cuantiles:\n\n\nCódigo\nggplot(cuenta, aes(x = cuenta_total)) +\n  stat_ecdf()\n\n\n\n\n\nEn esta gráfica, vemos que proporción de los datos que son iguales o están por debajo de cada valor en el eje horizontal.\n\nEn análisis de datos, es más frecuente utilizar la función de cuantiles pues existen versiones más generales que son útiles, por ejemplo, para evaluar ajuste de modelos probabilísticos\nEn la teoría, generalmente es más común utilizar la fda empírica, que tiene una única definición que veremos coincide con definiciones teóricas."
  },
  {
    "objectID": "analisis-resumenes-1.html#media-y-desviación-estándar",
    "href": "analisis-resumenes-1.html#media-y-desviación-estándar",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.3 Media y desviación estándar",
    "text": "1.3 Media y desviación estándar\nOtras medidas más comunes de localización y dispersión para conjuntos de datos son media y desviación estándar muestral.\nLa media de un conjunto de datos \\(x_1,\\ldots, x_n\\) es\n\\[\\bar{x} = \\frac{1}{n}\\sum x_i\\]\ny la desviación estándar es\n\\[\\hat{\\sigma} =\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\nEn general, no son muy apropiadas para iniciar el análisis exploratorio, y se requieren cuidados adicionales al utilizarlas, pues:\n\nSon medidas más difíciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Como ejercicio, intenta explicar intuitivamente qué es la media. Después prueba con la desviación estándar. Sin embargo, la mediana o el rango intercuartílico son fáciles de explicar.\nNo son resistentes a valores atípicos o erróneos. Su falta de resistencia los vuelve poco útiles en las primeras etapas de descripción, y muchas veces requieren transformaciones o cuidados adicionales/supuestos para evitar mal comportamiento por esa falta de resistencia.\n\nSin embargo,\n\nLa media y desviación estándar son computacionalmente convenientes, y para el trabajo de modelado, por ejemplo, tienen ventajas claras (cuando se cumplen supuestos). Por lo tanto regresaremos a estas medidas una vez que estudiemos modelos de probabilidad básicos.\nMuchas veces, ya sea por tradición, porque así se ha hecho el análisis antes, conviene usar estas medidas conocidas."
  },
  {
    "objectID": "analisis-resumenes-1.html#distribuciones-sesgadas-y-atípicos",
    "href": "analisis-resumenes-1.html#distribuciones-sesgadas-y-atípicos",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.4 Distribuciones sesgadas y atípicos",
    "text": "1.4 Distribuciones sesgadas y atípicos\nEn algunos casos tenemos que trabajar con mediciones que tienen una cola (usualmente la derecha) mucho más larga que la otra. Veamos cuáles son consecuencias típicas.\nConsideremos por ejemplos una muestra de los datos de ENIGH 2018\n\n\nCódigo\nenigh <- read_csv(\"./datos/enigh-ejemplo.csv\")\n\n\nY los deciles de ingreso son\n\n\nCódigo\nenigh <- mutate(enigh, ingreso_mensual_miles = INGTOT / 3000)\n\nenigh |> \n  summarise(\n    f = seq(0, 1, 0.1),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = seq(0, 1, 0.1), type = 4)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    0.0 \n    0.81 \n  \n  \n    0.1 \n    2.57 \n  \n  \n    0.2 \n    3.86 \n  \n  \n    0.3 \n    5.52 \n  \n  \n    0.4 \n    6.73 \n  \n  \n    0.5 \n    8.25 \n  \n  \n    0.6 \n    9.98 \n  \n  \n    0.7 \n    12.94 \n  \n  \n    0.8 \n    16.18 \n  \n  \n    0.9 \n    22.16 \n  \n  \n    1.0 \n    317.53 \n  \n\n\n\n\n\ndonde podemos ver cómo cuando nos movemos a deciles más altos, la dispersión aumenta. Existen algunos valores muy grandes. Un histograma no funciona muy bien con estos datos.\n\n\nCódigo\nggplot(enigh, aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSi filtramos los valores muy grandes, de todas formas encontramos una forma similar con una cola larga a la derecha:\n\n\nCódigo\nggplot(enigh |> filter(ingreso_mensual_miles < 90), \n       aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNótese que la media de estos datos no es un resúmen muy útil, porque es difícil de interpretar. Por los valores grandes, la media es considerablemente más alta que la mediana:\n\n\nCódigo\nenigh |> \n  summarise(\n    media = mean(ingreso_mensual_miles),\n    mediana =  quantile(ingreso_mensual_miles, probs = 0.5)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    media \n    mediana \n  \n \n\n  \n    12.04 \n    8.27 \n  \n\n\n\n\n\nEsta es otra razón para incluir información de cuantiles en la etapa descriptiva. Por ejemplo, podríamos resumir:\n\n\nCódigo\nenigh |> \n  summarise(\n    f = c(\"min\", 0.5, \"0.50\",  0.95, \"max\"),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = c(0, 0.05, 0.5, 0.95, 1))) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    min \n    0.81 \n  \n  \n    0.5 \n    1.92 \n  \n  \n    0.50 \n    8.27 \n  \n  \n    0.95 \n    32.24 \n  \n  \n    max \n    317.53 \n  \n\n\n\n\n\nOtra opción es utilizar una escala logarítmica. El logaritmo de los ingresos es más fácil de describir y veremos también más fácil de trabajar.\n\n\nCódigo\nggplot(enigh, \n       aes(x = ingreso_mensual_miles)) + \n  geom_histogram(binwidth = 0.12) +\n  scale_x_log10(breaks = c(1, 2, 4, 8, 16, 32, 64, 128, 256))\n\n\n\n\n\nPor las propiedades de los cuantiles, cualquier cantidad basada en cuantiles que se calcula en escala logarítmica puede pasarase a la escala original transformando\n\n\nCódigo\nquantile(log(enigh$ingreso_mensual_miles)) |> exp()\n\n\n         0%         25%         50%         75%        100% \n  0.8132833   4.7986689   8.2740789  14.1517930 317.5284167 \n\n\nCódigo\nquantile(enigh$ingreso_mensual_miles) \n\n\n         0%         25%         50%         75%        100% \n  0.8132833   4.7986692   8.2741033  14.1517942 317.5284167 \n\n\nNota: esto no sucede con medidas más complicadas como la media. El exponencial de la media de los logaritmos no es la media en la escala original."
  },
  {
    "objectID": "analisis-resumenes-1.html#comparando-grupos-con-variables-numéricas",
    "href": "analisis-resumenes-1.html#comparando-grupos-con-variables-numéricas",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.5 Comparando grupos con variables numéricas",
    "text": "1.5 Comparando grupos con variables numéricas\n\nEjemplo: precios de casas\nConsideramos datos de precios de ventas de la ciudad de Ames, Iowa. Nos interesa entender la variación del precio de las casas.\n\n\n\nCalculamos primeros unos cuantiles de los precios de las casas:\n\n\nCódigo\nquantile(casas |>  pull(precio_miles)) \n\n\n   0%   25%   50%   75%  100% \n 37.9 132.0 165.0 215.0 755.0 \n\n\nUna primera comparación que podemos hacer es considerar las distintas zonas de la ciudad. Podemos usar diagramas de caja y brazos para comparar precios en distintas zonas de la ciudad:\n\n\nCódigo\nggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip()\n\n\n\n\n\nNótese que de cada zona, los datos tienen una cola derecha más larga que la izquierda, e incluso hay valores extremos en la cola derecha que exceden el rango de variación usual. Una razón por la que puede suceder esto es que haya características particulares que agregan valor considerable a una casa, por ejemplo, el tamaño, una alberca, etc.\nEn primer lugar, podemos considerar el área de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo:\n\n\n\n\n\nCódigo\nggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip()\n\n\n\n\n\nNótese ahora que la variación alrededor de la media es mucho más simétrica, y ya no vemos tantos datos extremos. Aún más, la variación dentro de cada zona parece ser similar, y podríamos describir restos datos de la siguiente forma:\nCuantificamos la variación que observamos de zona a zona y la variación que hay dentro de zonas. La variación que vemos entre las medianas de la zona es:\n\n\nCódigo\ncasas |> group_by(nombre_zona) |> \n  summarise(mediana_zona = median(precio_m2)) |> \n  pull(mediana_zona) |> quantile() |> round()\n\n\n  0%  25%  50%  75% 100% \n 963 1219 1298 1420 1725 \n\n\nY las variaciones con respecto a las medianas dentro de cada zona, agrupadas, se resume como:\n\n\nCódigo\nquantile(casas |> group_by(nombre_zona) |> \n  mutate(residual = precio_m2 - median(precio_m2)) |> \n  pull(residual)) |> round()\n\n\n  0%  25%  50%  75% 100% \n-765 -166    0  172 1314 \n\n\nNótese que este último paso tiene sentido pues la variación dentro de las zonas, en términos de precio por metro cuadrado, es similar. Esto no lo podríamos hacer de manera efectiva si hubiéramos usado el precio de las casas sin ajustar por su tamaño.\nY vemos que la mayor parte de la variación del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tamaño de las casas. La variación dentro de cada zona es aproximadamente simétrica, aunque la cola derecha es ligeramente más larga con algunos valores extremos."
  },
  {
    "objectID": "analisis-resumenes-1.html#factor-y-respuesta-numéricos-opcional",
    "href": "analisis-resumenes-1.html#factor-y-respuesta-numéricos-opcional",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.6 Factor y respuesta numéricos (opcional)",
    "text": "1.6 Factor y respuesta numéricos (opcional)\nEn las secciones anteriores vimos cómo describir “bonches” de datos numéricos y categóricos. Adicionalmente, vimos cómo usar esas técnicas para comparar las descripciones a lo largo de varios subconjuntos de los datos.\nEn estos casos, muchas veces llamamos factor a la variables que forma los grupos, y respuesta a la variable que estamos comparando. Por ejemplo, en el caso de tomadores de té comparamos uso de complementos (respuesta) a lo largo de consumidores de distintos tipos de té (factor) En el caso de los precios de las casas comparamos el precio de las casas (respuesta) dependiendo del vecindario (factor) dónde se encuentran.\nCuando tenemos una factor numérico y una respuesta numérica podemos comenzar haciendo diagramas de dispersión. Por ejemplo,"
  },
  {
    "objectID": "analisis-resumenes-1.html#ejemplo-cuenta-total-y-propina",
    "href": "analisis-resumenes-1.html#ejemplo-cuenta-total-y-propina",
    "title": "1  Resúmenes para datos numéricos",
    "section": "Ejemplo: cuenta total y propina",
    "text": "Ejemplo: cuenta total y propina\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n\n\nPodríamos comenzar haciendo:\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_point() + geom_rug(colour = \"salmon\", alpha = 0.5)\n\n\n\n\n\nAhora queremos comparar la distribución de propina (respuesta) para distintos niveles del factor (cuenta_total). Por ejemplo, ¿cómo se compara propina cuando la cuenta es de 15 dólares vs 30 dólares?\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_vline(xintercept = c(15, 30), colour = \"red\") +\n   geom_point() \n\n\n\n\n\nVemos que los datos de propinas alrededor de 30 dólares están centrados en valores más grandes que en el nivel de 15 dólares, y también que hay más dispersión en el nivel de 30 dólares. Sin embargo, vemos que tenemos un problema: existen realmente muy pocos datos que tengan exactamente 15 o 30 dólares de cuenta. La estrategia es entonces considerar qué sucede cuando la cuenta está alrededor de 15 o alrededor de 30 dólares, donde alrededor depende del problema particular y de cuántos datos tenemos:\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.5) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.5) +\n   geom_point() \n\n\n\n\n\nConsiderando estos grupos de datos, podemos describir de las siguiente forma, por ejemplo:\n\n\nCódigo\npropinas |> \n   mutate(grupo = cut(cuenta_total,  breaks = c(0, 13, 17, 28, 32))) |> \n   filter(grupo %in% c(\"(13,17]\", \"(28,32]\")) |> \n   group_by(grupo) |> \n   summarise(\n      n = n(),\n      q10 = quantile(propina, 0.10),\n      mediana = quantile(propina, 0.5),\n      q90 = quantile(propina, 0.90),\n      rango_cuartiles = quantile(propina, 0.75) - quantile(propina, 0.25)) |> \n   kable(digits = 2)\n\n\n\n\n \n  \n    grupo \n    n \n    q10 \n    mediana \n    q90 \n    rango_cuartiles \n  \n \n\n  \n    (13,17] \n    57 \n    1.85 \n    2.47 \n    3.49 \n    1.0 \n  \n  \n    (28,32] \n    16 \n    2.02 \n    3.69 \n    5.76 \n    2.2 \n  \n\n\n\n\n\nConde confirmamos que el nivel general de propinas es más alto alrededor de cuentas de total 30 que de total 15, y la dispersión también es mayor. Podríamos hacer un diagrama de caja y brazos también."
  },
  {
    "objectID": "analisis-resumenes-1.html#suavizadores-locales",
    "href": "analisis-resumenes-1.html#suavizadores-locales",
    "title": "1  Resúmenes para datos numéricos",
    "section": "1.7 Suavizadores locales",
    "text": "1.7 Suavizadores locales\nEl enfoque del ejemplo anterior puede ayudar en algunos casos nuestra tarea descriptiva, pero quisiéramos tener un método más general y completo para entender cómo es una respuesta numérica cuando el factor es también numérico.\nEn este caso, podemos hacer por ejemplo medias o medianas locales. La idea general es, en términos de nuestro ejemplo de propinas:\n\nQueremos producir un resumen en un valor de cuenta total \\(x\\).\nConsideramos valores de propina asociados a cuentas totales en un intervalo \\([x-e, x+e]\\).\nCalculamos estadísticas resumen en este rango para la respuesta\nUsualmente también ponderamos más alto valores que están cerca de \\(x\\) y ponderamos menos valores más lejanos a \\(x\\)\n\nEste tipo de suavizadores se llaman a veces suavizadores loess (ver (Cleveland 1993)).\nPor ejemplo,\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_smooth(method = \"loess\", span = 0.5, degree= 0, \n               method.args = list(family = \"symmetric\"), se = FALSE) \n\n\nWarning: Ignoring unknown parameters: degree\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCódigo\n# symmetric es un método robusto iterativo, que reduce el peso de atípicos\n\n\nEl parametro span controla el tamaño de la ventana de datos que se toma en cada punto. Nótese como alrededor de 15 y 30 los valores por donde pasa el suavizador son similares a las medianas que escribimos arriba.\nPodemos ajustar en cada ventana tambien rectas de minimos cuadrados, y obtener un suavizador de tipo lineal. En la siguiente gráfica mostramos cómo funciona este suavizador para distintos tamaños de ventanas (span)\n\n\n\nSuavizador loess\n\n\n\n\n\nLos suavizadores loess tienen como fin mostrar alrededor de qué valor de distribuye la respuesta (eje vertical) para distintos valores del factor (eje horizontal). Se escoge span suficientemente baja de forma que mostremos patrones claros en los datos y casi no capturemos variación debida a los tamaños de muestra chicos.\n\n\nEn la animación anterior, un valor de span de 0.15 funciona aprpiadamente, y uno de 0.05 es demasiado bajo y uno de 1.0 es demasiado alto. Es importante explorar con el valor de span pues depende de cuántos datos tenemos y cómo es su dispersión.\nPodemos también mostrar estimaciones de medianas y cuantiles de la siguiente forma (nota: es necesario escoger lambda con cuidado, cuanto más alto sea lambda más suave es la curva obtenida):\n\n\nCódigo\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75)) +\n   scale_y_continuous(breaks = seq(0, 10, 1))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 15)\n\n\n\n\n\nFinalmente, entendimiento de los datos no permite también hacer gráficas más útiles. En este ejemplo particular podría por ejemplo calcular el porcentaje de la propina sobre la cuenta total:\n\n\nCódigo\npropinas <- mutate(propinas, pct_propina = propina / cuenta_total)\nggplot(propinas, aes(x = cuenta_total, y = pct_propina)) +\n   geom_point() +\n   scale_y_continuous(breaks = seq(0,1, 0.05)) +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.25, 0.5, 0.75))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 15)\n\n\n\n\n\nObserva que la descripción es más simple que si usamos propina cruda y cuenta\n\nPara cuentas chicas, el porcentaje de propina puede ser muy alto (aún cuando la propina en sí no es tan grande):\n\n\n\nCódigo\nfilter(propinas, pct_propina > 0.30) |> \n  arrange(desc(pct_propina)) |> \n  kable(digits = 2)\n\n\n\n\n \n  \n    cuenta_total \n    propina \n    fumador \n    dia \n    momento \n    num_personas \n    pct_propina \n  \n \n\n  \n    7.25 \n    5.15 \n    Si \n    Dom \n    Cena \n    2 \n    0.71 \n  \n  \n    9.60 \n    4.00 \n    Si \n    Dom \n    Cena \n    2 \n    0.42 \n  \n  \n    3.07 \n    1.00 \n    Si \n    Sab \n    Cena \n    1 \n    0.33 \n  \n\n\n\n\n\n\nPara cuentas relativamente chicas (10 dólares, el porcentaje de propina está por encima de 15%). Este porcentaje tiende a reducirse a valores 10% y 15% para cuentas más grandes\nExiste variación considerable alrededor de estos valores centrales. El rango intercuartiles es aproximadamente de 5 puntos porcentuales.\n\nO de manera más resumida:\n\nLa mediana de propinas está ligeramente por arriba de 15% para cuantas relativamente chicas. Esta mediana baja hasta alrededor de 10%-15% para cuentas más grandes (más de 40 dólares)\nLa mitad de las propinas no varía más de unos 3 puntos porcentuales alrededor de estas medianas.\nExisten propinas atípicas: algunas muy bajas de 1 dólar, muy por debajo del 15%, y ocasionalmente algunas muy altas en porcentaje. Estas últimas ocurren ocasinalmente especialmente en cuentas chicas (por ejemplo, una propina de 1 dólar en una cuenta de 3 dólares).\n\n\n\n\n\nCleveland, William S. 1993. Visualizing Data. Hobart Press."
  },
  {
    "objectID": "analisis-resumenes-2.html",
    "href": "analisis-resumenes-2.html",
    "title": "2  Datos univariados categóricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones individuales. Consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "analisis-resumenes-2.html#datos-categóricos-y-tablas",
    "href": "analisis-resumenes-2.html#datos-categóricos-y-tablas",
    "title": "2  Datos univariados categóricos",
    "section": "2.1 Datos categóricos y tablas",
    "text": "2.1 Datos categóricos y tablas\nUna medición categórica es una que toma sus valores posibles en un conjunto que no es numérico. Consideremos los siguiente datos de 300 tomadores de té (Lê, Josse, y Husson (2008)):\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\n\n\n\n\nCódigo\n# cargamos y traducimos los datos\nte_tbl <- read_csv(\"./datos/tea.csv\") |> \n   mutate(id = row_number()) |> \n   select(id, Tea, How, sugar, how, price, age) |> \n   rename(tipo = Tea, complementos = How, azucar = sugar, \n          presentacion = how, precio = price, edad = age) |> \n   mutate(tipo = recode(tipo, black = \"negro\", green = \"verde\", `Earl Grey` = \"earl_grey\"),\n          complementos = recode(complementos, alone = \"solo\", milk = \"leche\", \n                                lemon = \"limón\", .default = \"otros\"),\n          azucar = recode(azucar, sugar = \"con_azúcar\", No.sugar = \"sin_azúcar\"),\n          presentacion = recode(presentacion, `tea bag`=\"bolsa\", \n                                unpackaged = \"suelto\", .default = \"mixto\"),\n          precio = recode(precio, p_upscale = \"fino\", p_branded = \"de_marca\",\n                          p_private_label = \"marca_propia\", p_variable = \"variable\",\n                          .default = \"no_sabe\"))\nsample_n(te_tbl, 10) |> kable()\n\n\n\n\n \n  \n    id \n    tipo \n    complementos \n    azucar \n    presentacion \n    precio \n    edad \n  \n \n\n  \n    250 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    de_marca \n    31 \n  \n  \n    227 \n    verde \n    solo \n    sin_azúcar \n    mixto \n    fino \n    25 \n  \n  \n    272 \n    negro \n    solo \n    sin_azúcar \n    suelto \n    variable \n    27 \n  \n  \n    73 \n    earl_grey \n    leche \n    con_azúcar \n    bolsa \n    variable \n    22 \n  \n  \n    76 \n    earl_grey \n    leche \n    con_azúcar \n    bolsa \n    de_marca \n    34 \n  \n  \n    154 \n    verde \n    solo \n    sin_azúcar \n    bolsa \n    de_marca \n    49 \n  \n  \n    34 \n    negro \n    solo \n    sin_azúcar \n    bolsa \n    no_sabe \n    50 \n  \n  \n    101 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    variable \n    33 \n  \n  \n    269 \n    negro \n    solo \n    con_azúcar \n    mixto \n    fino \n    25 \n  \n  \n    115 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    de_marca \n    23 \n  \n\n\n\n\n\nMediciones como tipo, presentación o azucar son variables categóricas. Desde el punto de vista univariado, generalmente no es necesario resumir, sino simplemente agrupar y contar cuántas veces ocurre cada categoría. Por ejemplo\n\n\nCódigo\ntabla_1 <- te_tbl |> count(tipo) |> \n   arrange(desc(n))\ntabla_1 |> kable()\n\n\n\n\n \n  \n    tipo \n    n \n  \n \n\n  \n    earl_grey \n    193 \n  \n  \n    negro \n    74 \n  \n  \n    verde \n    33 \n  \n\n\n\n\n\nUsualmente es más útil reportar la porporción o porcentaje de casos por categoría\n\n\nCódigo\ntabla_2 <- te_tbl |> \n   count(tipo) |> \n   mutate(n_total = sum(n), prop = n / n_total) |> \n   select(tipo, n_total, prop) |> \n   mutate(across(where(is.numeric), round, 2)) |> \n   arrange(desc(prop))\ntabla_2 |> kable()\n\n\n\n\n \n  \n    tipo \n    n_total \n    prop \n  \n \n\n  \n    earl_grey \n    300 \n    0.64 \n  \n  \n    negro \n    300 \n    0.25 \n  \n  \n    verde \n    300 \n    0.11 \n  \n\n\n\n\n\nPodemos hacer varias variables juntas de la siguiente manera:\n\n\nCódigo\nperfiles_col_tbl <- te_tbl |> select(id, tipo, complementos, presentacion, azucar) |> \n   pivot_longer(cols = tipo:azucar, names_to = \"variable\", values_to = \"valor\") |> \n   count(variable, valor) |> \n   group_by(variable) |> \n   mutate(n_total = sum(n), prop = n / n_total) |>\n   mutate(prop = round(prop, 2)) |> \n   arrange(desc(prop), .by_group = TRUE)\nperfiles_col_tbl |> kable()\n\n\n\n\n \n  \n    variable \n    valor \n    n \n    n_total \n    prop \n  \n \n\n  \n    azucar \n    sin_azúcar \n    155 \n    300 \n    0.52 \n  \n  \n    azucar \n    con_azúcar \n    145 \n    300 \n    0.48 \n  \n  \n    complementos \n    solo \n    195 \n    300 \n    0.65 \n  \n  \n    complementos \n    leche \n    63 \n    300 \n    0.21 \n  \n  \n    complementos \n    limón \n    33 \n    300 \n    0.11 \n  \n  \n    complementos \n    otros \n    9 \n    300 \n    0.03 \n  \n  \n    presentacion \n    bolsa \n    170 \n    300 \n    0.57 \n  \n  \n    presentacion \n    mixto \n    94 \n    300 \n    0.31 \n  \n  \n    presentacion \n    suelto \n    36 \n    300 \n    0.12 \n  \n  \n    tipo \n    earl_grey \n    193 \n    300 \n    0.64 \n  \n  \n    tipo \n    negro \n    74 \n    300 \n    0.25 \n  \n  \n    tipo \n    verde \n    33 \n    300 \n    0.11 \n  \n\n\n\n\n\nPara leer más fácil, imprimimos individualmente estas tablas, o hacemos algo como lo que sigue para mostrarlas todas juntas:\n\n\nCódigo\nperfiles_col_tbl |> \n   ungroup() |> \n   select(-variable, -n_total) |> \n   kable() |>  \n   pack_rows(index = table(perfiles_col_tbl$variable))\n\n\n\n\n \n  \n    valor \n    n \n    prop \n  \n \n\n  azucar\n\n    sin_azúcar \n    155 \n    0.52 \n  \n  \n    con_azúcar \n    145 \n    0.48 \n  \n  complementos\n\n    solo \n    195 \n    0.65 \n  \n  \n    leche \n    63 \n    0.21 \n  \n  \n    limón \n    33 \n    0.11 \n  \n  \n    otros \n    9 \n    0.03 \n  \n  presentacion\n\n    bolsa \n    170 \n    0.57 \n  \n  \n    mixto \n    94 \n    0.31 \n  \n  \n    suelto \n    36 \n    0.12 \n  \n  tipo\n\n    earl_grey \n    193 \n    0.64 \n  \n  \n    negro \n    74 \n    0.25 \n  \n  \n    verde \n    33 \n    0.11"
  },
  {
    "objectID": "analisis-resumenes-2.html#comparando-grupos-con-variables-categóricas",
    "href": "analisis-resumenes-2.html#comparando-grupos-con-variables-categóricas",
    "title": "2  Datos univariados categóricos",
    "section": "2.2 Comparando grupos con variables categóricas",
    "text": "2.2 Comparando grupos con variables categóricas\nEste análisis generalmente es más interesante cuando comparamos grupos. Supongamos que nos interesa ver si existe una relación entre usar el tipo de té que toman estas personas y el uso de complementos como leche o limón. Podríamos entonces dividir los datos según el uso de azúcar y repetir para cada grupo las tablas mostradas arriba:\n\n\nCódigo\nperfiles_col_tbl <- te_tbl |> count(complementos, tipo) |> \n   group_by(tipo) |> \n   mutate(prop = n / sum(n)) |>\n   group_by(complementos) |> \n   select(-n) |> \n   pivot_wider(names_from = tipo, values_from = prop, values_fill = 0)\nperfiles_col_tbl |>  kable(digits = 2, caption = \"Perfiles por columna\")\n\n\n\n\nPerfiles por columna\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n  \n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n  \n\n\n\n\n\nComparando los perfiles de las columnas observamos variaciones interesantes: por ejemplo, los tomadores de Earl Grey tienden a usar más limón como complemento que otros grupos. Son resúmenes univariados que ahora comparamos a lo largo de grupos. Podemos hacer las comparaciones más simples si hacemos todas contra una columna marginal del uso general en la muestra de los distintos complementos_\n\n\nCódigo\ncomp_tbl <- te_tbl |> count(complementos) |> mutate(total = n / sum(n))\nperfiles_col_tbl <- left_join(perfiles_col_tbl, comp_tbl) |> \n      arrange(desc(total)) |> \n      select(-n)\n\n\nJoining, by = \"complementos\"\n\n\nCódigo\nperfiles_col_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n    0.65 \n  \n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n    0.21 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n    0.11 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n    0.03 \n  \n\n\n\n\n\nEn este punto, vemos que hay coincidencias y diferencias entre los grupos de tomadores de té. Podemos expresar esto de manera simple calculando índices contra la columna de total:\n\n\nCódigo\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric), ~ .x / total)) |> \n   select(-total)\nres_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    solo \n    1.02 \n    0.87 \n    1.17 \n  \n  \n    leche \n    0.94 \n    1.22 \n    0.87 \n  \n  \n    limón \n    1.13 \n    0.86 \n    0.55 \n  \n  \n    otros \n    0.52 \n    2.70 \n    0.00 \n  \n\n\n\n\n\nValores por encima de 1 indican columnas por arriba de la población general, y análogamente para valores por debajo de uno. Estas cantidades pueden escribirse en términos porcentuales, o se les puede restar 1 para terminar como una variación porcentual del promedio. A estas cantidades se les llama residuales crudos:\n\n\nCódigo\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) \nres_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.70 \n    -1.00 \n    0.03 \n  \n\n\n\n\n\nPodemos finalmente marcar la tabla:\n\n\nCódigo\nres_tbl |>  mutate(across(where(is.numeric), round, 2)) |> \n   mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.1, \"black\", \n                                         ifelse(.x < -0.1, \"red\", \"gray\"))))) |>\n   arrange(desc(total)) |> \n   kable(escape = FALSE) \n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03 \n  \n\n\n\n\n\n\n\n\n\n\n\nPerfiles\n\n\n\nA este tipo de análisis de tablas cruzadas a veces se le llama análisis de perfiles columna. Nos permite entender cómo varía la distribución de la variable de los renglones según el grupo indicado por la columna. - Desviaciones grandes en los residuales indican asociaciones fuertes entre la variable de los reglones y de las columnas - Recordemos que este análisis aplica a la muestra de datos que tenemos. Columnas con pocos individuos tienden a mostrar más variación y debemos ser cuidadosos al generalizar.\n\n\nPodemos incluir también totales para ayudarnos a juzgar las variaciones:\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n     \n    193 \n    74 \n    33 \n    1.00 \n  \n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03"
  },
  {
    "objectID": "analisis-resumenes-2.html#observación-perfiles-renglón-y-columna",
    "href": "analisis-resumenes-2.html#observación-perfiles-renglón-y-columna",
    "title": "2  Datos univariados categóricos",
    "section": "2.3 Observación: perfiles renglón y columna",
    "text": "2.3 Observación: perfiles renglón y columna\nEl análisis también lo podemos hacer con los perfiles de los renglones. Los residuales crudos que usamos para interpretar son los mismos. La razón es la siguiente:\nPara los perfiles columna, si escribimos \\(n_{+j}\\) como los totales por columna, y \\(n_{i+}\\) los totales por renglón, tenemos que los perfiles columna son:\n\\[c_{i,j} = \\frac{n_{i,j}}{n_{+j}}\\]\nEscribimos también \\(c_i = \\frac{n_{i+}}{n}\\) y \\(r_j = \\frac{n_{+j}}{n}\\) como los porcentajes marginales por columna y por renglón respectivamente.\nLos residuales son entonces\n\\[r_{i,j} = \\frac{\\frac{n_{i,j}}{n_{+,j}}} { \\frac{n_{i,+}}{n}} - 1 = \\frac{p_{i,j} - r_ic_j}{r_ic_j}\\]\nNótese que no importa entonces cómo comencemos el cálculo, por renglones o por columnas, el resultado es el mismo."
  },
  {
    "objectID": "analisis-resumenes-2.html#visualización-de-tablas-cruzadas-opcional",
    "href": "analisis-resumenes-2.html#visualización-de-tablas-cruzadas-opcional",
    "title": "2  Datos univariados categóricos",
    "section": "2.4 Visualización de tablas cruzadas (opcional)",
    "text": "2.4 Visualización de tablas cruzadas (opcional)\nPara tablas más grandes, muchas veces las técnicas que mostramos arriba no son suficientes para entender y presentar patrones importantes en los datos. En estos casos, buscamos reducir la dimensionalidad de los datos para poder presentarlos en una gráfica de dos dimensiones.\nPodemos utilizar análisis de correspondencias. A grandes rasgos (ver (Izenman 2009) para los detalles) buscamos una representación tal que:\n\nCada categoría de las columnas está representada por una flecha que sale del origen de nuestra gráfica\nCada categoría de los renglones está representada por un punto en nuestra gráfica\nSi proyectamos los puntos (renglones) sobre las direcciones de las columnas, entonces el tamaño de la proyección es lo más cercano posible a el residual correspondiente de las tablas del análisis mostrado arriba.\n\nPara construir esta gráfica, entonces, existe un proceso de optimización que busca representar lo más fielmente los residuales del análisis mostrado arriba en dos dimensiones, y de esta forma buscamos recuperar una buena parte de la información de los residuales de una manera más compacta."
  },
  {
    "objectID": "analisis-resumenes-2.html#ejemplo-tés-y-complementos",
    "href": "analisis-resumenes-2.html#ejemplo-tés-y-complementos",
    "title": "2  Datos univariados categóricos",
    "section": "2.5 Ejemplo: tés y complementos",
    "text": "2.5 Ejemplo: tés y complementos\n\n\nCódigo\nlibrary(ca)\ncorr_te <- ca(table(te_tbl$complementos, te_tbl$tipo))\nplot(corr_te, map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\nLa contribución de cada dimensión a la aproximación se indica en los ejes. Como vemos en la gráfica, y la suma de las contribuciones nos da la calidad de la representación, que en este caso es perfecta.\n::: {.cell type=‘comentario’}\n\n\nEl análisis de correspondencias es un tema relativamente avanzado de estadística multivariada, y su definición precisa requiere de matemáticas más avanzadas (por ejemplo la descomposición en valores singulares).\nCualquier hallazgo obtenido en este tipo de análisis debe ser verificado en las tablas correspondientes de perfiles\nHay distintos tipos de gráficas (biplots) asociadas al análisis de correspondencias, que privilegian representar mejor a distintos tipos de características de los datos\n\n:::"
  },
  {
    "objectID": "analisis-resumenes-2.html#ejemplo-robo-en-tiendas",
    "href": "analisis-resumenes-2.html#ejemplo-robo-en-tiendas",
    "title": "2  Datos univariados categóricos",
    "section": "2.6 Ejemplo: robo en tiendas",
    "text": "2.6 Ejemplo: robo en tiendas\nConsideramos los siguientes datos de robos en tiendas en Holanda por personas de distintas edades y genéros (Izenman (2009)). En este caso, las variables ya están cruzadas:\n\n\nCódigo\nhurto_tbl <- read_csv(\"./datos/hurto.csv\") |> \n   mutate(grupo = ifelse(grupo == \"-12 h\", \"01-12 h\", grupo),\n          grupo = ifelse(grupo == \"-12 m\", \"01-12 m\", grupo))\n\n\nRows: 18 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): grupo\ndbl (13): ropa, accesorios, tabaco, escritura, libros, discos, bienes, dulce...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCódigo\nhurto_tbl |> kable()\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    discos \n    bienes \n    dulces \n    juguetes \n    joyería \n    perfumes \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    81 \n    66 \n    150 \n    667 \n    67 \n    24 \n    47 \n    430 \n    743 \n    132 \n    32 \n    197 \n    209 \n  \n  \n    12-14 h \n    138 \n    204 \n    340 \n    1409 \n    259 \n    272 \n    117 \n    637 \n    684 \n    408 \n    57 \n    547 \n    550 \n  \n  \n    15-17 h \n    304 \n    193 \n    229 \n    527 \n    258 \n    368 \n    98 \n    246 \n    116 \n    298 \n    61 \n    402 \n    454 \n  \n  \n    18-20 h \n    384 \n    149 \n    151 \n    84 \n    146 \n    141 \n    61 \n    40 \n    13 \n    71 \n    52 \n    138 \n    252 \n  \n  \n    21-29 h \n    942 \n    297 \n    313 \n    92 \n    251 \n    167 \n    193 \n    30 \n    16 \n    130 \n    111 \n    280 \n    624 \n  \n  \n    30-39 h \n    359 \n    109 \n    136 \n    36 \n    96 \n    67 \n    75 \n    11 \n    16 \n    31 \n    54 \n    200 \n    195 \n  \n  \n    40-49 h \n    178 \n    53 \n    121 \n    36 \n    48 \n    29 \n    50 \n    5 \n    6 \n    14 \n    41 \n    152 \n    88 \n  \n  \n    50-64 h \n    137 \n    68 \n    171 \n    37 \n    56 \n    27 \n    55 \n    17 \n    3 \n    11 \n    50 \n    211 \n    90 \n  \n  \n    64+ h \n    45 \n    28 \n    145 \n    17 \n    41 \n    7 \n    29 \n    28 \n    8 \n    10 \n    28 \n    111 \n    34 \n  \n  \n    01-12 m \n    71 \n    19 \n    59 \n    224 \n    19 \n    7 \n    22 \n    137 \n    113 \n    162 \n    70 \n    15 \n    24 \n  \n  \n    12-14 m \n    241 \n    98 \n    111 \n    346 \n    60 \n    32 \n    29 \n    240 \n    98 \n    548 \n    178 \n    29 \n    58 \n  \n  \n    15-17 m \n    477 \n    114 \n    58 \n    91 \n    50 \n    27 \n    41 \n    80 \n    14 \n    303 \n    141 \n    9 \n    72 \n  \n  \n    18-20 m \n    436 \n    108 \n    76 \n    18 \n    32 \n    12 \n    32 \n    12 \n    10 \n    74 \n    70 \n    14 \n    67 \n  \n  \n    21-29 m \n    1180 \n    207 \n    132 \n    30 \n    61 \n    21 \n    65 \n    16 \n    12 \n    100 \n    104 \n    30 \n    157 \n  \n  \n    30-39 m \n    1009 \n    165 \n    121 \n    27 \n    43 \n    9 \n    74 \n    14 \n    31 \n    48 \n    81 \n    36 \n    107 \n  \n  \n    40-49 m \n    517 \n    102 \n    93 \n    23 \n    31 \n    7 \n    51 \n    10 \n    8 \n    22 \n    46 \n    24 \n    66 \n  \n  \n    50-64 m \n    488 \n    127 \n    214 \n    27 \n    57 \n    13 \n    79 \n    23 \n    17 \n    26 \n    69 \n    35 \n    64 \n  \n  \n    64+ m \n    173 \n    64 \n    215 \n    13 \n    44 \n    0 \n    39 \n    42 \n    6 \n    12 \n    41 \n    11 \n    55 \n  \n\n\n\n\n\nEsta tabla es más grande y difícil de entender tal cual está. Comenzamos por examinar las marginales:\n\n\nCódigo\nhurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop)) |> \n   kable(digits = 2)\n\n\n\n\n \n  \n    producto \n    n \n    prop \n  \n \n\n  \n    ropa \n    7160 \n    0.22 \n  \n  \n    escritura \n    3704 \n    0.11 \n  \n  \n    otros \n    3166 \n    0.10 \n  \n  \n    tabaco \n    2835 \n    0.09 \n  \n  \n    herramientas \n    2441 \n    0.07 \n  \n  \n    joyería \n    2400 \n    0.07 \n  \n  \n    accesorios \n    2171 \n    0.07 \n  \n  \n    dulces \n    2018 \n    0.06 \n  \n  \n    juguetes \n    1914 \n    0.06 \n  \n  \n    libros \n    1619 \n    0.05 \n  \n  \n    perfumes \n    1286 \n    0.04 \n  \n  \n    discos \n    1230 \n    0.04 \n  \n  \n    bienes \n    1157 \n    0.03 \n  \n\n\n\n\n\n\n\nCódigo\ngrupos_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(grupo) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop))\ngrupos_tbl |> kable(digits = 2)\n\n\n\n\n \n  \n    grupo \n    n \n    prop \n  \n \n\n  \n    12-14 h \n    5622 \n    0.17 \n  \n  \n    15-17 h \n    3554 \n    0.11 \n  \n  \n    21-29 h \n    3446 \n    0.10 \n  \n  \n    01-12 h \n    2845 \n    0.09 \n  \n  \n    21-29 m \n    2115 \n    0.06 \n  \n  \n    12-14 m \n    2068 \n    0.06 \n  \n  \n    30-39 m \n    1765 \n    0.05 \n  \n  \n    18-20 h \n    1682 \n    0.05 \n  \n  \n    15-17 m \n    1477 \n    0.04 \n  \n  \n    30-39 h \n    1385 \n    0.04 \n  \n  \n    50-64 m \n    1239 \n    0.04 \n  \n  \n    40-49 m \n    1000 \n    0.03 \n  \n  \n    18-20 m \n    961 \n    0.03 \n  \n  \n    01-12 m \n    942 \n    0.03 \n  \n  \n    50-64 h \n    933 \n    0.03 \n  \n  \n    40-49 h \n    821 \n    0.02 \n  \n  \n    64+ m \n    715 \n    0.02 \n  \n  \n    64+ h \n    531 \n    0.02 \n  \n\n\n\n\n\nIntentamos análisis de correspondencias para comparar los perfiles columna:\n\n\nCódigo\nhurto_df <- as.data.frame(hurto_tbl)\nrownames(hurto_df) <- hurto_tbl$grupo\nhurto_df$grupo <- NULL\ncorr_hurto <- ca(hurto_df)\ngrafica_datos <- plot(corr_hurto, map = \"rowgreen\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\nSegún esta gráfica, ¿qué categorias de productos están sobrerrepresentadas en cada grupo de edad? ¿Cómo tendrían que verse el análisis de perfiles columna?\n\nComo se aprecia, en la siguiente tabla, es difícil entender los patrones generales en los datos. Quitamos algunas columnas para imprimir más fácilmente\n\n\nCódigo\nperfiles_hurto_tbl <- hurto_tbl |> \n   pivot_longer(cols = ropa:otros, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   mutate(prop = n / sum(n)) |> \n   select(-n) |> \n   pivot_wider(names_from = producto, values_from = prop) \nperfiles_hurto_tbl |> \n   select(-bienes, -discos, -perfumes) |> \n   kable(digits = 2) |> \n   kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    0.01 \n    0.03 \n    0.05 \n    0.18 \n    0.04 \n    0.21 \n    0.39 \n    0.06 \n    0.08 \n    0.07 \n  \n  \n    12-14 h \n    0.02 \n    0.09 \n    0.12 \n    0.38 \n    0.16 \n    0.32 \n    0.36 \n    0.17 \n    0.22 \n    0.17 \n  \n  \n    15-17 h \n    0.04 \n    0.09 \n    0.08 \n    0.14 \n    0.16 \n    0.12 \n    0.06 \n    0.12 \n    0.16 \n    0.14 \n  \n  \n    18-20 h \n    0.05 \n    0.07 \n    0.05 \n    0.02 \n    0.09 \n    0.02 \n    0.01 \n    0.03 \n    0.06 \n    0.08 \n  \n  \n    21-29 h \n    0.13 \n    0.14 \n    0.11 \n    0.02 \n    0.16 \n    0.01 \n    0.01 \n    0.05 \n    0.11 \n    0.20 \n  \n  \n    30-39 h \n    0.05 \n    0.05 \n    0.05 \n    0.01 \n    0.06 \n    0.01 \n    0.01 \n    0.01 \n    0.08 \n    0.06 \n  \n  \n    40-49 h \n    0.02 \n    0.02 \n    0.04 \n    0.01 \n    0.03 \n    0.00 \n    0.00 \n    0.01 \n    0.06 \n    0.03 \n  \n  \n    50-64 h \n    0.02 \n    0.03 \n    0.06 \n    0.01 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.09 \n    0.03 \n  \n  \n    64+ h \n    0.01 \n    0.01 \n    0.05 \n    0.00 \n    0.03 \n    0.01 \n    0.00 \n    0.00 \n    0.05 \n    0.01 \n  \n  \n    01-12 m \n    0.01 \n    0.01 \n    0.02 \n    0.06 \n    0.01 \n    0.07 \n    0.06 \n    0.07 \n    0.01 \n    0.01 \n  \n  \n    12-14 m \n    0.03 \n    0.05 \n    0.04 \n    0.09 \n    0.04 \n    0.12 \n    0.05 \n    0.23 \n    0.01 \n    0.02 \n  \n  \n    15-17 m \n    0.07 \n    0.05 \n    0.02 \n    0.02 \n    0.03 \n    0.04 \n    0.01 \n    0.13 \n    0.00 \n    0.02 \n  \n  \n    18-20 m \n    0.06 \n    0.05 \n    0.03 \n    0.00 \n    0.02 \n    0.01 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n  \n  \n    21-29 m \n    0.16 \n    0.10 \n    0.05 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.04 \n    0.01 \n    0.05 \n  \n  \n    30-39 m \n    0.14 \n    0.08 \n    0.04 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n    0.02 \n    0.01 \n    0.03 \n  \n  \n    40-49 m \n    0.07 \n    0.05 \n    0.03 \n    0.01 \n    0.02 \n    0.00 \n    0.00 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    50-64 m \n    0.07 \n    0.06 \n    0.08 \n    0.01 \n    0.04 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n  \n  \n    64+ m \n    0.02 \n    0.03 \n    0.08 \n    0.00 \n    0.03 \n    0.02 \n    0.00 \n    0.00 \n    0.00 \n    0.02 \n  \n\n\n\n\n\n\n\nCódigo\nres_hurto_tbl <- left_join(perfiles_hurto_tbl, grupos_tbl |> rename(total = prop)) |> \n    select(-n) |> \n    select(-bienes, -discos, -perfumes) |> \n    mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) |> \n    mutate(across(where(is.numeric), round, 2)) \n\n\nJoining, by = \"grupo\"\n\n\nCódigo\nres_hurto_tbl |> \n    mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    select(-total) |> \n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    ropa \n    accesorios \n    tabaco \n    escritura \n    libros \n    dulces \n    juguetes \n    joyería \n    herramientas \n    otros \n  \n \n\n  \n    01-12 h \n    -0.87 \n    -0.65 \n    -0.38 \n    1.1 \n    -0.52 \n    1.48 \n    3.52 \n    -0.36 \n    -0.06 \n    -0.23 \n  \n  \n    12-14 h \n    -0.89 \n    -0.45 \n    -0.29 \n    1.24 \n    -0.06 \n    0.86 \n    1.1 \n    0 \n    0.32 \n    0.02 \n  \n  \n    15-17 h \n    -0.6 \n    -0.17 \n    -0.25 \n    0.33 \n    0.48 \n    0.14 \n    -0.44 \n    0.16 \n    0.53 \n    0.34 \n  \n  \n    18-20 h \n    0.06 \n    0.35 \n    0.05 \n    -0.55 \n    0.77 \n    -0.61 \n    -0.87 \n    -0.42 \n    0.11 \n    0.57 \n  \n  \n    21-29 h \n    0.26 \n    0.31 \n    0.06 \n    -0.76 \n    0.49 \n    -0.86 \n    -0.92 \n    -0.48 \n    0.1 \n    0.89 \n  \n  \n    30-39 h \n    0.2 \n    0.2 \n    0.15 \n    -0.77 \n    0.42 \n    -0.87 \n    -0.8 \n    -0.69 \n    0.96 \n    0.47 \n  \n  \n    40-49 h \n    0 \n    -0.02 \n    0.72 \n    -0.61 \n    0.2 \n    -0.9 \n    -0.87 \n    -0.76 \n    1.51 \n    0.12 \n  \n  \n    50-64 h \n    -0.32 \n    0.11 \n    1.14 \n    -0.65 \n    0.23 \n    -0.7 \n    -0.94 \n    -0.84 \n    2.07 \n    0.01 \n  \n  \n    64+ h \n    -0.61 \n    -0.2 \n    2.19 \n    -0.71 \n    0.58 \n    -0.14 \n    -0.74 \n    -0.74 \n    1.83 \n    -0.33 \n  \n  \n    01-12 m \n    -0.65 \n    -0.69 \n    -0.27 \n    1.13 \n    -0.59 \n    1.39 \n    1.07 \n    1.37 \n    -0.78 \n    -0.73 \n  \n  \n    12-14 m \n    -0.46 \n    -0.28 \n    -0.37 \n    0.5 \n    -0.41 \n    0.9 \n    -0.18 \n    2.65 \n    -0.81 \n    -0.71 \n  \n  \n    15-17 m \n    0.49 \n    0.18 \n    -0.54 \n    -0.45 \n    -0.31 \n    -0.11 \n    -0.84 \n    1.83 \n    -0.92 \n    -0.49 \n  \n  \n    18-20 m \n    1.1 \n    0.71 \n    -0.08 \n    -0.83 \n    -0.32 \n    -0.8 \n    -0.82 \n    0.06 \n    -0.8 \n    -0.27 \n  \n  \n    21-29 m \n    1.58 \n    0.49 \n    -0.27 \n    -0.87 \n    -0.41 \n    -0.88 \n    -0.9 \n    -0.35 \n    -0.81 \n    -0.22 \n  \n  \n    30-39 m \n    1.64 \n    0.43 \n    -0.2 \n    -0.86 \n    -0.5 \n    -0.87 \n    -0.7 \n    -0.62 \n    -0.72 \n    -0.37 \n  \n  \n    40-49 m \n    1.39 \n    0.56 \n    0.09 \n    -0.79 \n    -0.37 \n    -0.84 \n    -0.86 \n    -0.7 \n    -0.67 \n    -0.31 \n  \n  \n    50-64 m \n    0.82 \n    0.56 \n    1.02 \n    -0.81 \n    -0.06 \n    -0.7 \n    -0.76 \n    -0.71 \n    -0.62 \n    -0.46 \n  \n  \n    64+ m \n    0.12 \n    0.36 \n    2.51 \n    -0.84 \n    0.26 \n    -0.04 \n    -0.85 \n    -0.77 \n    -0.79 \n    -0.2 \n  \n\n\n\n\n\n\nCompara tus conclusiones del mapa de correspondencias con esta información de los residuales\n\nNota adicionalmente que el ordenamiento de las categorías en la primera dimensión del mapa de correspondencias ayuda a interpretar:\n\n\nCódigo\nres_hurto_tbl |> select(\"grupo\", \"escritura\", \"juguetes\", \"dulces\", \"joyería\",\n                         \"herramientas\", \"otros\", \"libros\", \"tabaco\", \"accesorios\", \n                         \"ropa\") |> \n    mutate(across(where(is.numeric), \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    kable(escape = FALSE) |>\n    kable_styling(font_size = 10)\n\n\n\n\n \n  \n    grupo \n    escritura \n    juguetes \n    dulces \n    joyería \n    herramientas \n    otros \n    libros \n    tabaco \n    accesorios \n    ropa \n  \n \n\n  \n    01-12 h \n    1.1 \n    3.52 \n    1.48 \n    -0.36 \n    -0.06 \n    -0.23 \n    -0.52 \n    -0.38 \n    -0.65 \n    -0.87 \n  \n  \n    12-14 h \n    1.24 \n    1.1 \n    0.86 \n    0 \n    0.32 \n    0.02 \n    -0.06 \n    -0.29 \n    -0.45 \n    -0.89 \n  \n  \n    15-17 h \n    0.33 \n    -0.44 \n    0.14 \n    0.16 \n    0.53 \n    0.34 \n    0.48 \n    -0.25 \n    -0.17 \n    -0.6 \n  \n  \n    18-20 h \n    -0.55 \n    -0.87 \n    -0.61 \n    -0.42 \n    0.11 \n    0.57 \n    0.77 \n    0.05 \n    0.35 \n    0.06 \n  \n  \n    21-29 h \n    -0.76 \n    -0.92 \n    -0.86 \n    -0.48 \n    0.1 \n    0.89 \n    0.49 \n    0.06 \n    0.31 \n    0.26 \n  \n  \n    30-39 h \n    -0.77 \n    -0.8 \n    -0.87 \n    -0.69 \n    0.96 \n    0.47 \n    0.42 \n    0.15 \n    0.2 \n    0.2 \n  \n  \n    40-49 h \n    -0.61 \n    -0.87 \n    -0.9 \n    -0.76 \n    1.51 \n    0.12 \n    0.2 \n    0.72 \n    -0.02 \n    0 \n  \n  \n    50-64 h \n    -0.65 \n    -0.94 \n    -0.7 \n    -0.84 \n    2.07 \n    0.01 \n    0.23 \n    1.14 \n    0.11 \n    -0.32 \n  \n  \n    64+ h \n    -0.71 \n    -0.74 \n    -0.14 \n    -0.74 \n    1.83 \n    -0.33 \n    0.58 \n    2.19 \n    -0.2 \n    -0.61 \n  \n  \n    01-12 m \n    1.13 \n    1.07 \n    1.39 \n    1.37 \n    -0.78 \n    -0.73 \n    -0.59 \n    -0.27 \n    -0.69 \n    -0.65 \n  \n  \n    12-14 m \n    0.5 \n    -0.18 \n    0.9 \n    2.65 \n    -0.81 \n    -0.71 \n    -0.41 \n    -0.37 \n    -0.28 \n    -0.46 \n  \n  \n    15-17 m \n    -0.45 \n    -0.84 \n    -0.11 \n    1.83 \n    -0.92 \n    -0.49 \n    -0.31 \n    -0.54 \n    0.18 \n    0.49 \n  \n  \n    18-20 m \n    -0.83 \n    -0.82 \n    -0.8 \n    0.06 \n    -0.8 \n    -0.27 \n    -0.32 \n    -0.08 \n    0.71 \n    1.1 \n  \n  \n    21-29 m \n    -0.87 \n    -0.9 \n    -0.88 \n    -0.35 \n    -0.81 \n    -0.22 \n    -0.41 \n    -0.27 \n    0.49 \n    1.58 \n  \n  \n    30-39 m \n    -0.86 \n    -0.7 \n    -0.87 \n    -0.62 \n    -0.72 \n    -0.37 \n    -0.5 \n    -0.2 \n    0.43 \n    1.64 \n  \n  \n    40-49 m \n    -0.79 \n    -0.86 \n    -0.84 \n    -0.7 \n    -0.67 \n    -0.31 \n    -0.37 \n    0.09 \n    0.56 \n    1.39 \n  \n  \n    50-64 m \n    -0.81 \n    -0.76 \n    -0.7 \n    -0.71 \n    -0.62 \n    -0.46 \n    -0.06 \n    1.02 \n    0.56 \n    0.82 \n  \n  \n    64+ m \n    -0.84 \n    -0.85 \n    -0.04 \n    -0.77 \n    -0.79 \n    -0.2 \n    0.26 \n    2.51 \n    0.36 \n    0.12 \n  \n\n\n\n\n\n\nOtras dimensiones\nEn el caso anterior, la calidad de la representación es cercana al 80%. Existen algunas desviaciones que la posiblemente la gŕafica no explica del todo, y algunas proyecciones son aproximadas. Podemos ver cómo se ven otras dimensiones de este análisis para entender desviaciones adicionales:\n\n\nCódigo\nplot(corr_hurto, dim = c(1, 3), map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\n\n\n\nIzenman, A. J. 2009. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts en Statistics. Springer New York. https://books.google.com.mx/books?id=1CuznRORa3EC.\n\n\nLê, Sébastien, Julie Josse, y François Husson. 2008. «FactoMineR: An R Package for Multivariate Analysis». Journal of Statistical Software, Articles 25 (1): 1-18. https://doi.org/10.18637/jss.v025.i01."
  },
  {
    "objectID": "intro-inferencia.html",
    "href": "intro-inferencia.html",
    "title": "Inferencia estadística",
    "section": "",
    "text": "A grandes rasgos, en la inferencia estadística buscamos hacer afirmaciones acerca de una colección de datos de la cual sólo tenemos información parcial.\nNos concentraremos en dos de las situaciones más comunes:\nPor ejemplo, consideremos esta población de 15 personas:\nPara una muestra de ellos tenemos información acerca de su estatura y peso. ¿Qué podríamos decir acerca de la estatura y el peso de la población general?\nEn este caso, la situación se ve como sigue. Imaginemos que tenemos 15 personas con dolor de cabeza, y obtenemos los siguientes datos:\nNuestra pregunta en este caso es del tipo: ¿ayuda la aspirina a reducir el dolor de cabeza en esta población? ¿qué tanto ayuda? Igualmente, tenemos información incompleta, en el sentido de que sólo observamos un resultado potencial de cada persona, dependiendo de si tomó aspirina o no. Si supiéramos los dos resultados potenciales de cada persona entonces podríamos contestar la pregunta sin dificultad."
  },
  {
    "objectID": "intro-inferencia.html#proceso-de-selección-o-asignación",
    "href": "intro-inferencia.html#proceso-de-selección-o-asignación",
    "title": "Inferencia estadística",
    "section": "Proceso de selección o asignación",
    "text": "Proceso de selección o asignación\nLas preguntas que planteamos arriba son difíciles de contestar cuando no conocemos bien el proceso de selección de individuos en la muestra o no conocemos el proceso de asignación de la aspirina.\nPor ejemplo, llegaríamos a conclusiones muy distintas si nos dijeran que:\n\nEscogimos las 5 personas que usan ropa talla chica.\nEscogimos las 5 personas que llegaron primero en una carrera de 100 metros.\nEscogimos las personas cuyo día de nacimiento era más bajo.\n\nO en el ejemplo de la aspirina,\n\nSólo dimos aspirinas a las personas que reportaron un nivel de dolor de cabeza muy alto.\nSolo dimos aspirina a las personas que llegaron primero en una carrera de 100 metros.\nDimos una aspirina exclusivamente a las personas cuyo día de nacimiento es par.\n\n\n\n\n\n\n\nTip\n\n\n\nDiscute qué conclusiones podrías llegar en cada uno de estos escenarios.\n\n\nLos casos 1 y 2 en ambas poblaciones son en general difíciles de resolver adecuadamente, y explicaremos con más ejemplos. Adicionalmente, es también más difícil cuantificar el nivel de incertidumbre de nuestras respuestas, pues dependen de muchos detalles del proceso de selección o asignación.\n\n\n\n\n\n\nProceso generador de datos\n\n\n\nCuando el proceso de selección de observaciones o asignación tiene relaciones complicadas con las cantidades de interés, puede ser muy difícil dar respuesta a preguntas inferenciales de manera adecuada, y es importante entender el proceso que genera los datos, muchas veces a un nivel muy detallado."
  },
  {
    "objectID": "intro-inferencia.html#procesos-generadores-de-datos-e-inferencia-causal",
    "href": "intro-inferencia.html#procesos-generadores-de-datos-e-inferencia-causal",
    "title": "Inferencia estadística",
    "section": "Procesos generadores de datos e inferencia causal",
    "text": "Procesos generadores de datos e inferencia causal\nConsideramos los datos de ENLACE (2011), y en particular los resultados promedio de matemáticas en sexto grado por escuela. ENLACE era una prueba que se aplicaba en todas las escuelas, de forma que tenemos información de la población completa de interés.\nNos interesa saber cómo varían los resultados en función de el tipo de primaria: pública o privada.\nEl rango de la calificación de matemáticas para un alumno es de 0-800, y aproximadamente la mitad de los alumnos califica en el rengo de 450 a 550. Vemos dispersión considerable en las calificaciones de las escuelas, y diferencias considerables entre tipo de escuelas:\n\n\n\n\n\nCódigo\nenlace_tbl <- enlace |> group_by(tipo) |> \n    summarise(n_escuelas = n(),\n              cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) |> \n    unnest(cols = cuantiles) |> mutate(valor = round(valor)) \nenlace_tbl |> spread(cuantil, valor) |>  formatear_tabla()\n\n\n\n\n \n  \n    tipo \n    n_escuelas \n    0.05 \n    0.25 \n    0.5 \n    0.75 \n    0.95 \n  \n \n\n  \n    Indígena/Conafe \n    13599 \n    304 \n    358 \n    412 \n    478 \n    588 \n  \n  \n    General \n    60166 \n    380 \n    454 \n    502 \n    548 \n    631 \n  \n  \n    Particular \n    6816 \n    479 \n    551 \n    593 \n    634 \n    703 \n  \n\n\n\n\n\nPodemos graficar de varias maneras, por ejemplo, mostrando los cuantiles 0.05, 0.25, 0.5, 0.75 y 0.95:\n\n\n\n\n\nY vemos que las escuelas privadas tienen mejor resultados que las públicas por un margen considerable. Ahora preguntamos: ¿la calidad de las escuelas es lo que causa estos resultados? Por ejemplo, ¿si cambiáramos un niño de una escuela pública a una privada su resultado sería mejor?\nEs dificil contestar esta pregunta, porque no entendemos el proceso generador de datos que asigna niños a escuelas. Una posible hipótesis de cómo se asigna el tratamiento y cómo se relaciona con la calificación en Enlace (nota: hya muchas más variables y estructuras causales que se pueden considerar, este es sólo un ejemplo que nos indica cómo podría ser problemática una comparación directa entre tipo de escuelas):\n\n\nCódigo\nlibrary(dagitty)\nlibrary(ggdag)\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nCódigo\ndag_cafe <- dagitty('dag{\"Escuela\" [exposure,pos=\"-3,3\"]\n  \"Calif\" [outcome,pos=\"3,3\"]\n  \"Marginación\" [pos=\"0,1\"]\n  \"Educación padres\" [pos = \"2,2\"]\n  \"Recursos escolares\" [pos = \"0,4\"]\n  \"Marginación\"  -> \"Escuela\"\n    \"Escuela\" -> \"Calif\"\n    \"Marginación\" -> \"Educación padres\" -> \"Calif\"\n    \"Escuela\" -> \"Recursos escolares\" -> \"Calif\"\n  }')\ndag_cafe_tidy <- tidy_dagitty(dag_cafe) #|> \n  #mutate(tipo = ifelse(name == \"Cafe\" & to == \"Cancer\", \"dotted\", \"solid\"))\ndag_cafe_tidy |>\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend )) + \n  geom_dag_edges() +\n  geom_dag_point(colour = \"salmon\", size = 20) +\n  geom_dag_text(colour = \"gray20\") +\n  theme_dag()\n\n\n\n\n\n\nEl grado de marginación del hogar de un estudiante influye en el tipo de escuela al que asiste.\nPero también influye en la calificación que obtiene el estudiante, por ejemplo actuando mediante el nivel de educación de los padres.\nEsto implica que cuando cruzamos tipo de escuela y calificación obtenida, en parte estamos viendo el efecto que la escuela tiene en los estudiantes, pero también un efecto común de la marginación.\nLa comparación entre tipos de escuelas está sesgada si el propósito es estimar el efecto de tipo de escuela en los resultados de los estudiantes.\n\n\n\nCódigo\nenlace_tbl_marg <- enlace %>% \n    group_by(tipo, marginacion) %>% \n    summarise(n_alumnos = sum(num_evaluados_total),\n              cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) %>% \n    unnest(cols = cuantiles) %>% mutate(valor = round(valor)) %>% \n    filter( n_alumnos > 20)\n\n\nPodemos considerar el grado de marginación del municipio donde está cada escuela, y vemos la diferencia tan grande se debe en parte a las escuelas privadas tienden estar en municipios de baja marginación. Por ejemplo, los promedios no son tan diferentes para escuelas públicas en zonas de marginación muy baja comparado con escuelas privadas en zonas de marginación alta:\n\n\n\n\n\nEn esta nueva comparación, existen muchos otros factores que probablemente tenemos que pensar, algunos de ellos no estarán disponibles en los datos, y otros puede que se escapen de nuestra consideración.\n\nEl problema central en este análisis es que el proceso generador de datos, en cuanto a como se selecciona cada escuela para cada alumno, es complejo.\nHacer inferencia causal en este caso es retador y depende de varios supuestos que no son estadísticos."
  },
  {
    "objectID": "intro-inferencia.html#procesos-generadores-e-inferencia-a-poblaciones",
    "href": "intro-inferencia.html#procesos-generadores-e-inferencia-a-poblaciones",
    "title": "Inferencia estadística",
    "section": "Procesos generadores e inferencia a poblaciones",
    "text": "Procesos generadores e inferencia a poblaciones\nSupongamos que nos interesa estimar la prevalencia de anemia en menores de edad escolar en México (6 a 15 años). Si extraemos los registros de hospitalización del IMSS, por ejemplo, nos encontramos con que 47% de los niños registrados tiene anemia. ¿Por qué esta es una estimación dudosa (y demasiado alta) de la prevalencia en México de anemia en niños?\n\nLa razón es que si un menor tienen una enfermedad grave es más probable que sea hospitalizado.\nSi un menor tiene una enfermedad grave, es más probable que tenga anemia.\nSi un menor tiene anemia, es más probable que sea hospitalizado.\nAdicionalmente, la probabilidad de hospitalización para cada tipo de enfermedad cambia con la edad.\n\nConcluimos que:\n\nLa probabilidad de selección en la muestra cambia dependiendo si un menor tiene anemia o no.\n\nEn conclusión, si no sabemos cómo es que otras enfermedades que llevan a la hospitalización influye en tener anemia en menores y cómo la edad está relacionada con casos de anemia, será difícil extraer conclusiones de estos datos.\nEste caso es muy claro por qué es difícil decir algo de la población general de menores con la muestra de menores hospitalizados, y en muchos casos este problema puede ser más sutil."
  },
  {
    "objectID": "pruebas-hipotesis-aleatorizacion.html",
    "href": "pruebas-hipotesis-aleatorizacion.html",
    "title": "3  Aleatorización en inferencia causal",
    "section": "",
    "text": "Empezaremos con ejemplos de inferencia causal y consideramos el ejemplo de (Box et al. (1978)).\nSupongamos que un jardinero aficionado tiene un fertilizante, y quiere ver si tiene un efecto agregarlo a sus plantas. Nuestro jardinero solamente tiene una línea donde caben 20 plantas.\nCuando las plantas crezcan, observaremos variabilidad, independientemente de si se usa fertilizante o no. Esta variabilidad proviene de muchos factores ambientales, variaciones en las condiciones del suelo, insectos, etc. Interpretar los resultados correctamente implica necesariamente cuantificar esa variabilidad.\nEl jardinero escogió algunos lugares dónde poner el fertilizante y dónde no. El resultado que obtuvo es:\nPara decidir qué tan bueno es el nuevo fertilizante, el jardinero decide usar la siguiente estadística \\(D\\) (Kolmogorov-Smirnov):\nUn valor de \\(D\\) grande sugiere que el fertilizante tiene algún efecto. En nuestro experimento, obtuvimos:\nLa diferencia más grande en estas curvas es:\nParece ser que las plantas con fertilizante tuvieron mejores resultados (la distribución de las fertilizadas está recorrida hacia la derecha). El problema aquí es que las plantas tienen variabilidad, y la diferencia que observamos, que no es muy grande, podría deberse a esa variabilidad, y no tener qué ver con el fertilizante.\n¿Cómo juzgamos si este resultado puede atribuirse a variabilidad en el crecimiento de cada planta?\nReescribimos nuestros datos como:\nNótese que escribimos en cada caso el dato observado y el no observado.\nAhora supongamos que el tratamiento no tiene ningún efecto sobre el crecimiento de las plantas. Bajo esta hipótesis, podemos rellenar los valores no observados: en cada caso, el dato faltante lo conocemos, y es igual al valor observado para cada planta.\nBajo esta hipótesis, podemos calcular qué pasaría si hubiéramos escogido distintas plantas para el tratamiento de fertilizante.\nSimplemente consideramos todas las permutaciones de la columna \\(T\\), y vemos el valor que tiene nuestra estadística \\(D\\) en cada caso. La distribución resultante es construida bajo la hipótesis de que el fertilizante no tiene efecto, y muestra la variabilidad de nuestra estadística bajo esta hipótesis.\nUsualmente, en lugar de calcular todas las permutaciones, simulamos un número grande de ellas (de mil a 10 mil, por ejemplo). Abajo mostramos dos ejemplos:\nY aquí vemos todos los posibles resultados bajo distintas asignaciones del fertilizante, bajo la hipótesis del que el fertilizante no tiene ningún efecto. Adicionalmente, marcamos el valor que observamos en el experimento.\nComo vemos, el resultado que obtuvimos está en el lado alto de la destribución. Parece ser que el fertilizante tiene algún efecto.\nSin embargo, hay un hueco en nuestro argumento. Por ejemplo,\nSi esto es cierto, entonces nuestro argumento no es válido. Quizá la diferencia es grande porque el fertilizante se aplicó a plantas con más potencial desde un principio.\nUna solución simple es la siguiente:\nLlamamos a este valor-p de la prueba, y cuanto más chico es, mayor evidencia tenemos contra la hipótesis nula."
  },
  {
    "objectID": "pruebas-hipotesis-aleatorizacion.html#pruebas-de-hipótesis-visuales",
    "href": "pruebas-hipotesis-aleatorizacion.html#pruebas-de-hipótesis-visuales",
    "title": "3  Aleatorización en inferencia causal",
    "section": "3.1 Pruebas de hipótesis visuales",
    "text": "3.1 Pruebas de hipótesis visuales\nOtra idea es hacer una prueba visual. Primero graficamos varias replicaciones de datos nulos, es decir, datos en donde hemos permutado al azar la columna de tratamiento. Entrenamos nuestra percepción a variaciones consistentes con la hipótesis nula:\n\n\nCódigo\nlibrary(nullabor)\n\nset.seed(83814)\n# comenzamos con rorsach, viendo datos nulos\nreps_rorschach <- rorschach(method = null_permute(\"T\"), n = 20, res_obs) |> \n  as_tibble()\nggplot(reps_rorschach, aes(sample = y, colour = T, group = T)) +\n  stat_qq(distribution = stats::qunif) +\n  stat_qq_line(distribution = stats::qunif, fullrange = TRUE) +\n  facet_wrap(~ .sample) + theme(strip.background =element_rect(fill=\"gray85\"))\n\n\n\n\n\nY ahora hacemos nuestra prueba. En la siguiente gráfica 19 cajas tienen datos nulos, y una caja tiene los datos verdaderos.\n¿Puedes identificar los datos verdaderos?\n\n\nCódigo\nreps <- lineup(method = null_permute(\"T\"), n = 20, res_obs) |> \n  as_tibble()\n\n\ndecrypt(\"T50h Hgwg 7u WoM7w7ou KX\")\n\n\nCódigo\nggplot(reps, aes(sample = y, colour = T, group = T)) +\n  stat_qq(distribution = stats::qunif) +\n  stat_qq_line(distribution = stats::qunif, fullrange = TRUE) +\n  facet_wrap(~ .sample) + theme(strip.background =element_rect(fill=\"gray85\"))\n\n\n\n\n\n\nEsta prueba es exacta: la probabilidad de identificar los datos correctamente cuando el fertilizante no tiene efecto es menor o igual a 0.05 (valor \\(p\\)). Esta es la probabilidad de equivocadamente declarar que tenemos evidencia de que la hipótesis nula no se cumple.\n\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York."
  },
  {
    "objectID": "estimacion-aleatorizacion.html",
    "href": "estimacion-aleatorizacion.html",
    "title": "4  Inferencia por intervalos",
    "section": "",
    "text": "En esta parte consideramos hacer inferencia a una población. Quiséramos decir algo acerca de la mediana de una población cuando solo tenemos información parcial.\nEn este ejemplo, queremos aprender acerca de los precios de casas en una región determinada (los precios están en miles de dólares).\nSupongamos que tenemos la siguiente datos, que sólo son parte de la población:\nSupongamos que queremos estimar el valor \\(m\\) que es la mediana poblacional. Este valor tiene la propiedad de que separa a los datos en dos grupos de tamaño igual.\nPara construir estos rangos haremos algunos cálculos básicos.\nEn primer lugar, si observáramos un valor \\(y\\) en la muestra, ¿cuál es la probabilidad de que caiga por arriba de la mediana? Si no sabemos cómo se extrajo la muestra, o se extrajo bajo un mecanismo complicado, esta pregunta es difícil de contestar.\nSin embargo, si este valor se extrajo tomando un valor al azar entre todos los de la población, entonces sabemos que la probabilidad es\n\\[P(y <= m) = 0.5\\]\nCon esta idea básica, podríamos por ejemplo calcular: ¿cuál es la probabilidad, de que la mediana esté entre los valores el mínimo y el máximo de la muestra?\nDenotemos por \\(y_1,y_2,\\ldots, y_n\\) la muestra observada. Denotaremos a la muestra ordenada del valor más chico al valor más grande como\n\\[y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\]\nNuestra primera pregunta entonces es:\nRespuesta: La probabilidad de que la mediana esté por debajo del mínimo \\(y_{(1)}\\) de la muestra es \\((1/2)^n\\), pues todos los valores de la muestra tienen que caer por arriba de la mediana. La probabilidad de que la mediana esté por arriba del máximo \\(y_{(1)}\\) es también \\((1/2)^n\\). Así que la probabilidad de el verdadero valor de la mediana esté en este intervalo es de $ 2(1/2)^n$\nEsta es la probabilidad de tirar solamente soles o solamente águilas en \\(n\\) tiradas de una moneda, Si \\(X\\) denota el número de soles obtenidos en \\(n\\) tiradas de dados, buscamos calcular la probabilidad de que la mediana esté en intervalo como\n\\[1 - P(X > n - 1) - P(X \\leq 0) = 1 - 2(1/2)^n\\]\nPrácticamente, es casi seguro que este intervalo contenga a la media verdadero. Esto no es muy informativo. Probemos ahora con \\([y_{(2)}, y_{(n-1)}]\\). Por un argumento similar, la probabilidad de la mediana verdadera esté en este intervalo es:\nEsto también es altamente probable. Sin embargo, la probabilidad de la mediana poblacional esté entre \\([y_{6}, y_{15}]\\) es\nAsí, este intervalo tiene probabilidad de casi 90% de probabilidad de contener al verdadero valor:\nAhora extraemos el intervalo correspondiente:\nY este es un intervalo de confianza de cerca de 90% para la mediana de la población. Cuando tomamos muestras más grandes, podemos obtener mejores precisiones. Por ejemplo, para una muestra de 151 casas, usaríamos\nTomamos una muestra y extraemos el intervalo:\nEsto intervalos tienen la garantía inferencial, y no es necesario hacer ningún supuesto, excepto en que la muestra se escoge al azar."
  },
  {
    "objectID": "estimacion-aleatorizacion.html#remuestreo",
    "href": "estimacion-aleatorizacion.html#remuestreo",
    "title": "4  Inferencia por intervalos",
    "section": "4.1 Remuestreo",
    "text": "4.1 Remuestreo\nPodemos pensar en este procedimiento de una manera diferente.\nSi la muestra completa se seleccionó al azar, entonces, si escogemos un valor no observado en la muestra, tenemos que \\(y\\) tiene la misma probabilidad de caer en cualquiera de los intervalos\n\\[I_1 = (0, y_1), I_2 = [y_1, y_2), I_3 = [y_2, y_3),\\ldots, I_{n} = [y_{n}, \\infty ]\\]\nLa razón es que si consideramos la muestra original con el nuevo dato agregado tomado al azar, todos los ordenamientos de \\(y_1, y_2, \\ldots, y_n, y_{n+1}\\) son igualmente probables: esto implica que es igualmente probable que \\(y_{n+1}\\) caiga en cualquiera de estos intervalos (probabilidad \\(1/{n+1}\\)).\n\nPodemos imputar nuestra población entera tomando escogiendo para cada valor no observado un intervalo,\nPodemos entonces muestrear de esta población otra vez para obtener una muestra nueva.\nRepetimos los dos anteriores una gran cantidad de veces para ver todos los posibles valores de medianas que podemos obtener.\n\nCada muestra es una sucesión de intervalos. Acumulamos para obtener una distribución sobre estos intervalos. La probabilidad de uqe la mediana salga en alguno de los intervalos extremos es igual a\n\\[(1/n)^{n/2},\\]\nque es un número del orden de \\[10^{-5}\\], que es chico aún cuando \\(n=10\\). Para valores más grandes, esta probabilidad es astronómicamente chica. Como el intervalo final consistirá de datos observados en nuestra muestra, en el resto de los intervalos no importa donde están los verdaderos poblacionales, y nos basta con saber si están o no en cada intervalo.\nEste proceso se llama remuestreo, y lo hacemos a continuación\n\n\nCódigo\nremuestrear <- function(muestra){\n  slice_sample(muestra, prop = 1, replace = TRUE) |> \n  summarise(mediana = median(precio_miles))\n}\nremuestras <- map_df(1:10000, ~ remuestrear(muestra))\n\n\n\n\nCódigo\nggplot(remuestras, aes(x = mediana)) +\n  geom_histogram(breaks = muestra |> pull(precio_miles)) +\n  geom_rug()\n\n\n\n\n\n\n\nCódigo\nquantile(remuestras$mediana, c(0.051, 0.948), type = 8)\n\n\n 5.1% 94.8% \n  146   235 \n\n\ny obtenemos un resultado idéntico al argumento de arriba. La cobertura es prácticamente exacta, y no afecta al resultado lo que sucede en las colas de la distribución, gracias a que estamos usando la mediana."
  },
  {
    "objectID": "estimacion-aleatorizacion.html#remuestreo-para-otras-estadísticas",
    "href": "estimacion-aleatorizacion.html#remuestreo-para-otras-estadísticas",
    "title": "4  Inferencia por intervalos",
    "section": "4.2 Remuestreo para otras estadísticas",
    "text": "4.2 Remuestreo para otras estadísticas\nPodemos repetir este método para otras estadísticas como la media. Sin embargo, en este caso no podemos dar garantías independientes de la distribución poblacional. La razón es que las colas pueden ser muy extremas.\n\nremuestrear_media <- function(muestra){\n  slice_sample(muestra, prop = 1, replace = TRUE) |> \n  summarise(media = mean(precio_miles))\n}\nremuestras_media <- map_df(1:10000, ~ remuestrear_media(muestra))\n\n\n\nCódigo\nggplot(remuestras_media) +\n  geom_histogram(aes(x = media), binwidth = 1.5) +\n  geom_rug(data = muestra, aes(x = precio_miles))\n\n\n\n\n\nY nuestro problema en este caso es que la media sí cambia dependiendo de cómo es la distribución dentro de cada intervalo, incluyendo los intervalos izquierdo y derecho."
  },
  {
    "objectID": "probabilidad.html",
    "href": "probabilidad.html",
    "title": "Introducción a teoría de probabilidades",
    "section": "",
    "text": "Hasta este punto del curso asumimos que el lector tiene un buen grado de familiaridad con la teoría básica de la probabilidad, pero dado que los usos de la probabilidad dentro de un marco bayesiano son mucho más amplios, en este capítulo repasaremos brevemente los fundamentos de la teoría de probabilidad."
  },
  {
    "objectID": "modelo-prob.html",
    "href": "modelo-prob.html",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "",
    "text": "En esta parte comenzaremos a tratar el concepto de probabilidad de ocurrencia de eventos acerca de los que tenemos incertidumbre. Veremos que:"
  },
  {
    "objectID": "modelo-prob.html#equiprobabilidad-y-simetría",
    "href": "modelo-prob.html#equiprobabilidad-y-simetría",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.1 Equiprobabilidad y simetría",
    "text": "5.1 Equiprobabilidad y simetría\nHistóricamente, el concepto de probabilidad nació en el contexto de juegos de azar, y cómo definir apuestas ajustas o equitativas. Por ejemplo:\n\nSupongamos que nos cuesta 1 peso entrar a un juego de dados, y que ganamos si tiramos un 3. ¿Cuánto dinero deberíamos ganar si sale un 3 para que el costo de entrada sea justo?\n\nEl argumento iría como sigue: existen 6 posibles resultados, y como el dado se tira bien y está bien hecho, entonces sería lo mismo apostar a cualquier número. Entonces, si seis personas entran al juego apostando a distintos números, cada uno pagando 1 peso, sería justo que el ganador se llevara 6 pesos.\n\nEn este caso, definimos la probabilidad de obtener un 3 (o cualquier otro número) como 1/6, que es cociente entre la apuesta inicial entre la cantidad recibida si ganamos la apuesta justa.\n\nNótese que el criterio de “justicia” proviene de simetrías del experimento: si el dado no fuera simétrico, por ejemplo, esta sería una manera mala de definir probabilidades.\nEn general, una manera de difinir probabilidad es la siguiente:\n\n\n\nEspacios equiprobables\nSi un evento \\(A\\) puede ocurrir de \\(k\\) maneras de \\(n\\) posibles, y es indiferente apostar a cualquiera de los \\(n\\) posibles resultados, entonces\n\\[P(A) = \\frac{k}{n}\\]\n\n\nCon este enfoque podemos resolver diversos problemas de probabilidad, por ejemplo:"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dados",
    "href": "modelo-prob.html#ejemplo-dados",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dados",
    "text": "Ejemplo: dados\n¿Cuál es la probabilidad de que tiremos una suma de 9 con dos dados de seis lados?\nEn este caso, los resultados son pares \\((x, y)\\) donde \\(x\\) es el resultado del dado 1 y \\(y\\) es el resultado del dado 2. Existen 36 pares, y todos ellos son equivalentes. Para tirar un 9, tenemos que lograr alguna de las siguientes tiradas:\n\\[(3, 6), (4, 5), (5, 4), (6, 3)\\]\nDe modo que la probabilidad de tirar una suma de 9 , que escribimos como \\(S=9\\), es\n\\[P(S = 9) = 4/36 = 1/9\\]"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-cartas",
    "href": "modelo-prob.html#ejemplo-cartas",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "Ejemplo: cartas",
    "text": "Ejemplo: cartas\nSacamos dos cartas sucesivamente de una baraja de 52 cartas, donde 26 son negras y 26 rojas. ¿Cuál es la probabilidad de que la segunda carta que saquemos sea negra? Podemos denotar como \\(N_2\\) el evento que sucede cuando la segunda carta que sacamos es negra. La segunda carta que sacamos puede ser cualquiera de las 52, y somos indiferentes en apostar a cualqueira de las cartas para salir en segundo lugar, así que\n\\[P(N_2) = 26 / 52 = 1/2\\]\nEstos espacios equiprobables nos dan nuestros modelos probabilísticos más simples. Están basados en simetrías del espacio de resultados."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "href": "modelo-prob.html#probabilidad-y-frecuencias-relativas",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.2 Probabilidad y frecuencias relativas",
    "text": "5.2 Probabilidad y frecuencias relativas\nLa conexión entre experimentos reales y modelos de probabilidad, está en que si repetimos muchas veces el experimento, entonces las frecuencias relativas de ocurrencia de los eventos aproxima a las probabilidades teóricas. Por ejemplo, si tiramos muchos volados con una moneda bien balanceada, esperamos obtener alrededor de 1/2 de soles y 1/2 de águilas, y si tiramos un dado muchas veces esperamos obtener alrededor de 1/6 de las tiradas un 5, por ejemplo (bajo los modelos de resultados equiprobables correspondientes).\nEn realidad, esta es otra definición de probabilidad en términos de repeticiones de experimentos.\n\n\n\nProbabilidades y frecuencias\nSupongamos que repetimos una gran cantidad \\(n\\) de veces un experimento, y que registramos \\(k_n\\) = cuántas veces ocurre un evento \\(A\\). La probabilidad de que ocurra \\(A\\) es\n\\[\\lim_{n\\to\\infty} \\frac{k_n}{n} \\to P(A), \\]\nes decir, \\(P(A)\\) el la frecuencia al largo plazo de ocurrencia de \\(A\\).\n\n\nAunque podríamos hacer algunos experimentos físicos más reales, para este curso podemos hacer simulaciones de computadora del experimento que nos interesa."
  },
  {
    "objectID": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "href": "modelo-prob.html#ejemplo-simulación-de-un-dado",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "Ejemplo: simulación de un dado",
    "text": "Ejemplo: simulación de un dado\nPrimero hacemos un dado. Podemos simular una tirada de dado como:\n\n\nCódigo\nsimular_dado <- function(caras = 1:6){\n   sample(caras, 1)\n}\nsimular_dado()\n\n\n[1] 1\n\n\nAhora simulamos una gran cantidad de tiradas de dado:\n\n\nCódigo\nset.seed(199213)\nn <- 5000\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nhead(sims_dado) \n\n\n# A tibble: 6 × 2\n  n_sim resultado\n  <int>     <int>\n1     1         3\n2     2         6\n3     3         3\n4     4         3\n5     5         6\n6     6         3\n\n\nEsta es una variable numérica, pero como toma valores enteros del uno al seis, podemos resumir con frecuencias, como si fuera categórica:\n\n\nCódigo\nsims_dado %>% \n   count(resultado) %>% \n   mutate(frec_relativa = n / sum(n))\n\n\n# A tibble: 6 × 3\n  resultado     n frec_relativa\n      <int> <int>         <dbl>\n1         1   806         0.161\n2         2   814         0.163\n3         3   857         0.171\n4         4   898         0.180\n5         5   856         0.171\n6         6   769         0.154\n\n\nY nuestro modelo teórico (resultados equiprobables) coincide razonablemente bien con las frecuencias observadas a largo plazo. Podemos ver cómo convergen las frecuencias relativas por ejemplo del resultado 1:\n\n\nCódigo\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")\n\n\n\n\n\nNótese que cuando hay pocas repeticiones podemos ver fluctuaciones considerablemente grandes de la frecuencia relativa observada de unos. Sin embargo, conforme aumentamos el tamaño de la meustra observada, esas fluctuaciones son más chicas.\nVeamos otra simulación:\n\n\nCódigo\nsims_dado <- map_df(1:n, ~ c(n_sim = .x, resultado = simular_dado()))\nsims_dado %>% \n   mutate(no_unos = cumsum(resultado == 1)) %>% \n   mutate(frec_uno = no_unos / n_sim) %>%\n   filter(n_sim < 5000) %>% \nggplot(aes(x = n_sim, y = frec_uno)) +\n   geom_hline(yintercept = 1/6, colour = \"red\") +\n   geom_line() + ylab(\"Frecuencia relativa de unos\")"
  },
  {
    "objectID": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "href": "modelo-prob.html#datos-y-modelos-de-probabilidad",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.3 Datos y modelos de probabilidad",
    "text": "5.3 Datos y modelos de probabilidad\n¿Cómo podemos usar modelos de probablidad para describir datos observados? La idea (simplificada) es la siguiente:\n\nHacemos una hipótesis acerca de cómo es el modelo de probabilidad asociado a un fenómeno\nObservamos una muestra de datos del fenómeno que nos interesa\nEvaluamos si las fluctuaciones observadas debidas a la información limitada que tenemos (una muestra) son consistentes con el modelo de probabilidad\n\nConsideremos el ejemplo de los dados. Supongamos que lanzamos el dado un número no muy grande de veces, y observamos:\n\n\nCódigo\nfrecs_obs <- tibble(resultado = 1:6,\n                    n = c(5, 7, 5, 10, 8, 5)) %>% \n   mutate(frec = n / sum(n))\nfrecs_obs %>% kable(digits = 2)\n\n\n\n\n \n  \n    resultado \n    n \n    frec \n  \n \n\n  \n    1 \n    5 \n    0.12 \n  \n  \n    2 \n    7 \n    0.17 \n  \n  \n    3 \n    5 \n    0.12 \n  \n  \n    4 \n    10 \n    0.25 \n  \n  \n    5 \n    8 \n    0.20 \n  \n  \n    6 \n    5 \n    0.12 \n  \n\n\n\n\n\nY nos preguntamos si este resultado podría ser observado bajo los supuestos de nuestro modelo de probabilidad, que en este caso, es el de resultados equiprobables. Podemos por ejemplo graficar los datos junto con simulaciones del modelo, en búsqueda de desajustes:\n\n\nCódigo\nset.seed(8834)\n# una vez\nsim_exp <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()))\n# 19 veces\nsims_exp <- map_df(1:19, function(x){\n         sims <- map_df(1:40, ~ c(id = .x, resultado = simular_dado()) )\n         sims$rep <- x\n         sims\n         })\n\n\n\n\nCódigo\nfrec_sims <- sims_exp %>% \n   group_by(rep, resultado) %>% \n   summarise(n = n()) %>% \n   mutate(frec = n / sum(n))\n\n\n`summarise()` has grouped output by 'rep'. You can override using the `.groups`\nargument.\n\n\nCódigo\nobs_sims_tbl <- bind_rows(frec_sims, frecs_obs %>% mutate(rep = 20))\nggplot(obs_sims_tbl, aes(x = resultado, y = frec)) +\n   geom_col() +\n   facet_wrap(~rep)\n\n\n\n\n\nEn este caso, no vemos ninguna característica de los datos observados que no sea consistente las fluctuaciones esperadas para un tamaño de muestra de \\(n=40\\).\nPregunta: ¿qué defecto le ves a esta gráfica que tiene los datos en la posición 20? ¿Cómo podríamos hacer una evaluación más apropiada de la calidad del ajuste?\nObservación: como veremos, muchas veces proponemos modelos que tienen parámetros que deben ser estimados con la muestra. Este caso más común es más complejo que el explicado arriba, pero el proceso es similar."
  },
  {
    "objectID": "modelo-prob.html#espacios-no-equiprobables",
    "href": "modelo-prob.html#espacios-no-equiprobables",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.4 Espacios no equiprobables",
    "text": "5.4 Espacios no equiprobables\nDepende cómo planteemos nuestro experimento aleatorio, puede ser o no apropiado el modelo equiprobable. Veremos ahora algunos ejemplos donde construimos modelos no equiprobables"
  },
  {
    "objectID": "modelo-prob.html#ejemplo-dos-dados",
    "href": "modelo-prob.html#ejemplo-dos-dados",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "Ejemplo: dos dados",
    "text": "Ejemplo: dos dados\nSupongamos que nos interesa la suma de dos tiradas de dados. Comenzamos con un modelo equiprobable sobre los resultados de cada tirada, que denotamos como \\((x,y)\\). Cada resultado tiene probabilidad 1/36.\nSin embargo, sólo nos interesa la suma. Contando posibles resultados podemos dar la probabilidad de cada resultado:\n\n\n\nSuma\nResultados\nProb\n\n\n\n\n2\n(1,1)\n1/36\n\n\n3\n(1,2),(2,1)\n2/36\n\n\n4\n(1,3),(2,2),(3,1)\n3/36\n\n\n5\n(1,4),(2,3),(3,2),(4,1)\n4/36\n\n\n6\n(1,5),(2,4),(3,3),(4,2),(5,1)\n5/36\n\n\n7\n(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\n6/36\n\n\n8\n(2,6),(3,5),(4,4),(5,3),(6,2)\n5/35\n\n\n9\n(3,6),(4,5),(5,4),(6,3)\n4/36\n\n\n10\n(4,6),(5,5),(6,4)\n3/36\n\n\n11\n(5,6), (6,5)\n2/36\n\n\n12\n(6,6)\n1/36\n\n\n\nY entonces terminamos con la siguiente distribución no equiprobable sobre las posibles sumas:\n\n\nCódigo\nprobs_suma <- tibble(suma = 2:12) %>% \n   mutate(prob = (6 - abs(suma - 7)) / 36)\nprobs_suma %>% kable(digits = 3)\n\n\n\n\n \n  \n    suma \n    prob \n  \n \n\n  \n    2 \n    0.028 \n  \n  \n    3 \n    0.056 \n  \n  \n    4 \n    0.083 \n  \n  \n    5 \n    0.111 \n  \n  \n    6 \n    0.139 \n  \n  \n    7 \n    0.167 \n  \n  \n    8 \n    0.139 \n  \n  \n    9 \n    0.111 \n  \n  \n    10 \n    0.083 \n  \n  \n    11 \n    0.056 \n  \n  \n    12 \n    0.028 \n  \n\n\n\n\n\nEsta distribución la podemos graficar como sigue:\n\n\nCódigo\ng_teorica <- ggplot(probs_suma, aes(x = suma, y = prob)) +\n   geom_col() +\n   scale_x_continuous(breaks = 2:12)\ng_teorica\n\n\n\n\n\nPodríamos tirar dos dado un gran número de veces para verificar si este modelo da las probabilidades correctas (o más propiamente dicho, si estas observaciones son consistentes con el modelo de probabilidad mostrado arriba)."
  },
  {
    "objectID": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "href": "modelo-prob.html#reglas-básicas-de-probabilidad",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.5 Reglas básicas de probabilidad",
    "text": "5.5 Reglas básicas de probabilidad\nTanto modelos equiprobables como la interpretación frecuentista de la probabilidad resultan en un conjunto de reglas útiles para operar con probabilidades. Estas son reglas generales que aplican independientemente de la interpretación particular de la probabilidad que utilicemos.\n\nLa probabilidad del evento cierto es 1 Si \\(A\\) es el evento que cubre todos los posibles resultados, entonces \\(P(A) = 1\\).\n\nAsegúrate que puedes demostrar esto para un espacio equiprobable, por ejemplo.\n\nLa probabilidad de que un evento \\(A\\) no ocurra es \\(1- P(A)\\).\n\n¿Por qué es cierto esto en espacios equiprobables? ¿Y según la definición frecuentista? Usa esta regla y el ejemplo anterior para calcular la probabilidad de tirar una suma menor a 12 cuando tiramos dos dados.\nEsto normalmente lo escribimos como \\(P(A^c) = 1 - P(A)\\).\nDecimos que dos eventos \\(A\\) y \\(B\\) son disjuntos cuando no pueden ocurrir simultánemente, o en otras palabras, el conjunto de resultados donde ocurre \\(A\\) tiene intersección vacía con el conjunto de resultados donde ocurre \\(B\\).\n\nLa probabilidad de que ocurra un resultado dentro de un conjunto de eventos disjuntos es igual a la suma de las probabilidades de dichos eventos\n\nEsto lo podemos escribir como sigue: si \\(A_1, A_2, \\dots, A_n\\) son eventos disjuntos, entonces\n\\[P(A_1\\cup A_2\\cup \\cdots \\cup A_n) = P(A_1) + P(A_2) + \\cdots + P(A_n)\\]\nUtiliza esta regla para\n\nCalcula la probabilidad de tirar una suma mayor a 9 en dos tiradas de dados.\nCalcula la probabilidad de obtener un par al sacar dos cartas de una baraja usual de póquer.\nMás difícil: hay una caja con \\(100\\) pelotas, y una de ellas es dorada. Si sacamos al azar \\(20\\) pelotas, una a la vez, ¿cuál es la probabilidad de que nos salga la pelota dorada? ¿ y si sacamos 57 pelotas?\n\nSoluciones:\n\nPara tirar más de 9, podemos tirar 10, 11 o 12, así que \\(P(A) = P(A_{10}\\cup A_{11} \\cup A_{12})\\). Estos útlimos tres eventos son mutuamente excluyentes, así que por la regla de la suma,\n\n\\[P(A) = P(A_{10}) + P(A_{11}) + P(A_{12})\\] y consultando nuestra tabla de arriba,\n\\[P(A) = \\frac{3}{36} + \\frac{2}{36} + \\frac{1}{36} = 1/6\\]\n\nPodemos sacar \\(52(51)\\) pares distintos. Ahora calculamos la probabidad de sacar un par particular, por ejemplo de ases. Hay \\(4(3)=12\\) distintos pares de ases (en el orden que los sacamos). Lo mismo podemos decir de pares de 2, 3, etc. Así que sumando sobre cada una de estas posibilidades obtenemos:\n\n\\[P(A) = 13\\frac{4(3)}{52(51)} \\approx 0.0588\\]\nTambién podríamos calcular de la siguiente forma: sacamos la primera carta y la vemos. La siguiente carta puede ser una de 51 restantes, y solo con tres de ellas completamos un par, de forma que\n\\[P(A) = 3 / 51 \\approx 0.0588\\]\n\nSea \\(D\\) el evento donde en el proceso sacamos la pelota dorada, y sea \\(D_1\\) el evento donde sacamos la pelota dorada en nuestra primera extracción, \\(D_2\\) si la sacamos en la segunda, etc. Tenemos que\n\n\\[D =D_1 \\cup D_2 \\cup\\cdots \\cup D_{20}\\]\n(sacamos la dorada si y sólo si la sacamos en la extracción 1, 2 , etc. hasta 20) Además, solo hay una pelota dorada, de manera que \\(D_i\\cap D_j= \\emptyset\\), es decir, no pueden ocurrir juntos \\(D_1\\) y \\(D_2\\), \\(D_5\\) y \\(D_11\\), etc., así que por la regla de la suma\n\\[P(D) = P(D_1) + P(D_2) + \\dots + P(D_{20})\\]\nFinalmente, \\(P(D_1)= 1/100\\), pues hay 100 pelotas y todas tienen igual probabilidad de aparecen en la primera extracción. Pero también \\(P(D_2) = 1/100\\), pues todas las pelotas tienen la misma probabilidad de aparecer en la segunda extracción. Se sigue entonces que, haciendo la suma de arriba, que: \\(P(D) = 20(\\frac{1}{100}) = 0.20\\)"
  },
  {
    "objectID": "modelo-prob.html#simulación-y-probabilidad",
    "href": "modelo-prob.html#simulación-y-probabilidad",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.6 Simulación y probabilidad",
    "text": "5.6 Simulación y probabilidad\nUsando la interpetación frecuentista, también es posible resolver una variedad de problemas de probabilidad usando simulación. La idea es, si queremos aproximar la probabilidad \\(P(E)\\) de un evento es:\n\nDefinimos el espacio de resultados del experimento aleatorio.\nSimulamos el experimento aleatorio un número grande de veces.\nCalculamos para cuáles de esas simulaciones se cumple el evento \\(E\\)\nEstimamos la frecuencia relativa de ocurrencia de \\(E\\) a lo largo de todas las simulaciones.\n\n\nEjemplo: simulando dos dados\nPara dos dados, el espacio de resultados podemos escribirlo como los resultados conjunto de dos tiradas: \\((x,y)\\). Cada dado se tira de forma separada, y los resultados son equiprobables. Definimos entonces:\n\n\nCódigo\nsim_dados <- function(num_dados = 2, num_caras = 6){\n   resultado <- sample(1:num_caras, num_dados, replace = TRUE)\n   names(resultado) <- paste0(\"dado_\", 1:num_dados)\n   resultado\n}\nsim_dados()\n\n\ndado_1 dado_2 \n     5      3 \n\n\nAhora simulamos un número grande de veces el experimento:\n\n\nCódigo\nset.seed(2323)\nsims <- map_df(1:10000, ~ sim_dados())\nsims\n\n\n# A tibble: 10,000 × 2\n   dado_1 dado_2\n    <int>  <int>\n 1      5      6\n 2      3      3\n 3      2      5\n 4      3      5\n 5      6      4\n 6      2      2\n 7      3      6\n 8      6      5\n 9      3      4\n10      1      1\n# … with 9,990 more rows\n\n\nAhora que tenemos estas simulaciones, podemos estimar por ejemplo la probabilidad de tirar más de nueve con dos dados. Primero calculamos en qué simulaciones ocurre \\(E\\):\n\n\nCódigo\nsims_e <- \n   sims %>% \n   mutate(suma = dado_1 + dado_2) %>% \n   mutate(evento_E = (suma > 9)) \nsims_e\n\n\n# A tibble: 10,000 × 4\n   dado_1 dado_2  suma evento_E\n    <int>  <int> <int> <lgl>   \n 1      5      6    11 TRUE    \n 2      3      3     6 FALSE   \n 3      2      5     7 FALSE   \n 4      3      5     8 FALSE   \n 5      6      4    10 TRUE    \n 6      2      2     4 FALSE   \n 7      3      6     9 FALSE   \n 8      6      5    11 TRUE    \n 9      3      4     7 FALSE   \n10      1      1     2 FALSE   \n# … with 9,990 more rows\n\n\nY ahora calculamos la frecuencia relativa de ocurrencia de \\(E\\)\n\n\nCódigo\nsims_e %>% \n   summarise(frec_e = mean(evento_E))\n\n\n# A tibble: 1 × 1\n  frec_e\n   <dbl>\n1  0.169\n\n\nQue confirma a dos decimales el resultado que obtuvimos arriba usando la regla de la suma (o contando resultados).\n\n\nEjemplo: problema de pelotas\nExtraemos al azar\n\n\nCódigo\nset.seed(232)\npelotas <- c(\"dorada\", rep(\"otra\", 99))\nsim_extraccion <- function(){\n    sample(pelotas, 20, replace = FALSE)\n}\nsim_extraccion()\n\n\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nSimulamos el experimento aleatorio:\n\n\nCódigo\nsims_pelotas <- map(1:10000, ~ sim_extraccion())\nsims_pelotas[1:2]\n\n\n[[1]]\n [1] \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n [9] \"dorada\" \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n[17] \"otra\"   \"otra\"   \"otra\"   \"otra\"  \n\n[[2]]\n [1] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n[11] \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\" \"otra\"\n\n\nChecamos si ocurre o no el evento se extrajo la pelota dorada:\n\n\nCódigo\nevento_dorada <- map_lgl(sims_pelotas, ~ any(str_detect(.x, \"dorada\")))\nevento_dorada[1:10]                   \n\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nY calculamos la frecuencia relativa de ocurrencia del evento:\n\n\nCódigo\nmean(evento_dorada)\n\n\n[1] 0.2007\n\n\nQue es consistente con el resultado que obtuvimos arriba haciendo cálculos.\n\n\n\nPara estimar probabilidades de ocurrencia de un evento usualmente es posible hacer simulaciones por computadora. Esto es especialmente importante cuando el experimento y evento que consideramos hace difícil (a veces imposible) hacer cálculos analíticos.\nEste tipo de métodos basados en simulación se llaman en general métodos de Monte Carlo.\n\n\n\n\n5.6.1 Ejemplo:\nSupongamos que ponemos 15 puntos igualmente espaciados en una circunferencia. Si escogemos tres puntos al azar, ¿cuál es la probabilidad de que formen un triángulo equilátero? Este problema se puede resolver contando los posibles resultados donde se forma un triángulo equilátero, pero no es trivial. Veamos cómo la haríamos simulando:\n\n\nCódigo\nsimular_3_puntos <- function(){\n   puntos_angulo <- 0:14 * (2  *pi / 15)\n   sample(puntos_angulo, 3)\n}\nsimular_3_puntos()\n\n\n[1] 3.769911 3.351032 1.675516\n\n\nCalcular dado el resultado requiere algo de consideración, pero finalmente terminamos con (¿qué otra manera se te ocurre?):\n\n\nCódigo\nes_equilatero <- function(tres_puntos){\n   tol <- 10^(-6)\n   puntos_ord <- sort(tres_puntos)\n   dif_1 <- puntos_ord[3] - puntos_ord[2]\n   dif_2 <- puntos_ord[2] - puntos_ord[1]\n   dif_3 <- 2*pi + puntos_ord[1] - puntos_ord[3]\n   if(abs(dif_1 - dif_2) < tol & abs(dif_2 - dif_3) < tol){\n      equilatero <- TRUE\n   } else {\n      equilatero <- FALSE\n   }\n   equilatero\n}\n\n\nSimulamos:\n\n\nCódigo\nsims_puntos <- map(1:50000, ~ simular_3_puntos())\n\n\nCalculamos la ocurrencia del evento de interés:\n\n\nCódigo\nevento <- map_lgl(sims_puntos, ~ es_equilatero(.x))\n\n\nY al frecuencia relativa es:\n\n\nCódigo\nmean(evento)\n\n\n[1] 0.01168\n\n\n¿Puedes resolver este problema contando los posibles triángulos, y cuáles de ellos son equiláteros?"
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva",
    "href": "modelo-prob.html#probabilidad-subjetiva",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.7 Probabilidad subjetiva",
    "text": "5.7 Probabilidad subjetiva\nExiste otra manera más flexible de interpretar la probabilidad que tiene que ver con un “grado de creencia” en que un evento va a ocurrir. Por ejemplo, el evento \\(A\\) podría ser “va a llover mañana”, y según mi conocimiento establezco que \\[P(A) = 0.25\\]\nEsta es una probablidad subjetiva pero no arbitraria. Para entender por qué es eso, pensemos en un juego de azar que consiste jugar una lotería de 100 boletos, donde podemos jugar con un número de boletos dado. Comparamos entonces dos alternativas:\n\nSi \\(A\\) ocurre, es decir, llueve mañana, entonces gano 1000 pesos.\nLa lotería de 100 boletos tiene un único premio de 1000 pesos.\n\nEntonces me puedo preguntar:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 5 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.05\\). Ahora puedo preguntarme:\n\n¿Qué prefiero, jugar 1 o jugar 2 con 20 boletos?\n\nSi prefiero 1, entonces creo que \\(P(A)>0.20\\) Sin embargo, probablemente si me pregunto si prefiero jugar 1 o jugar 2 con 90 boletos, prefería jugar la lotería, de modo yo creo que \\(P(A)<0.90.\\)\nCon estos ejercicios de preferencias sobre apuestas comparando con juegos de azar simples, puedo ver cuál es la probabilidad que yo le asigno a cualquier evento. Es posible demostrar que una probabilidad definida de esta manera cumple todas las reglas de probabilidad que mostramos arriba (si mis decisiones son racionales). Es posible definir modelos de probabilidad con este enfoque subjetivo, y es posible operar con ellos de manera usual con la maquinaria matemática.\nLo poderoso de este enfoque es que nos permite atacar problemas que 1) no está claro cómo podrían derivarse a partir de modelos equiprobables, y 2) no está claro que sea posible reproducir el experimento muchas veces para ver cual es la frecuencia relativa de ocurrencia del evento. Por ejemplo:\n\n¿Cuál es la probabilidad de que esta persona X contraiga una enfermedad particular?\n¿Cuál es la probabilidad de que un candidato Y gane una elección?\n¿Qué tan probable es que el próximo año el crecimiento del PIB sea mayor a 1%?\n\nEn todos estos casos importantes donde tenemos que tomar decisiones, la única alternativa real es pensar en términos de probabilidad subjetiva.\nEn la práctica, este tipo de enfoque se utiliza para poner probabilidades sobre cantidades o aspectos difíciles de medir directamente pero que son relativamente simples, y luego se utilizan modelos y reglas de probabilidad para producir otras probabilidades de interés más complicadas."
  },
  {
    "objectID": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "href": "modelo-prob.html#probabilidad-subjetiva-y-calibración-frecuentista",
    "title": "5  Básicos de probabilidad y simulación",
    "section": "5.8 Probabilidad subjetiva y calibración frecuentista",
    "text": "5.8 Probabilidad subjetiva y calibración frecuentista\nComo podemos imaginarnos, para muchas decisiones importantes, el enfoque subjetivo por sí solo puede no ser suficiente. Por ejemplo, imaginemos que trabajamos en INEGI queremos estimar el ingreso total de los hogares en México. Sería difícil proponer para esta tarea un enfoque puramente subjetivo.\nEn estos caso, podemos usar un enfoque donde utilizamos las probabilidades de manera subjetiva (por ejemplo elicitadas de un pánel de expertos), pero demostramos también que nuestro método tiene propiedades frecuentistas apropiadas: por ejemplo, que nuestros intervalos de 95% de estimación son realmente de 95%. Esto normalmente se cumple 1) Checando supuestos de nuestro modelo 2) Sometiendo nuestro método a distintos escenarios de estimación, y checando que se cumplen estimaciones frecuentistas apropiadas. En realidad, usualmente estos dos últimos pasos, 1 y 2, deben ser llevados a cabo no importa el enfoque de interpretación que utilicemos, pues cualquier método, frecuentista o subjetivo, tiene condiciones bajo las que puede fallar."
  },
  {
    "objectID": "prob-condicional.html",
    "href": "prob-condicional.html",
    "title": "6  Probabilidad condicional e independencia",
    "section": "",
    "text": "Uno los conceptos centrales de la probabilidad es el de probabilidad condicional:\nMuchos de los problemas de esta sección son de (Ross (1998))."
  },
  {
    "objectID": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "href": "prob-condicional.html#probabilidad-condicional-en-espacios-equiprobables.",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.1 Probabilidad condicional en espacios equiprobables.",
    "text": "6.1 Probabilidad condicional en espacios equiprobables.\nSupongamos que en un experimento simétro de \\(n\\) posibles resultados, sabemos que ocurrió el evento \\(F\\), es decir, un conjunto de resultados fijo. ¿Cómo podemos calcular la probabilidad de que ocurra un evento \\(E\\) dado que sabemos que \\(F\\) ocurrió? Esta probabilidad se escribe\n\\[P(E|F)\\]\n\n6.1.1 Ejemplo: dos dados\nSupongamos que tiramos dos dados, y nos dicen que la suma de los dos datos es igual a 6. ¿Cuál es la probablidad condicional de haber tirado al menos un cinco dado que la suma es 6?\nSolución: los resultados equiprobables que resultan en un tiro de suma 6 son \\(F= \\{ (5,1),(4,2),(3,3),(2,4),(1,5)\\}\\), que son 5 posibles resultados. En solamente 2 de ellos tiramos un cinco. Como estos resultados son equiprobables, si \\(E\\) es el evento “tirar al menos un 5”,\n\\[P(E|F) = 2/5\\]\nPodemos formalizar de la siguiente manera: para calcular \\(P(E|F)\\) contamos todos los resultados de \\(F\\) donde también ocurre \\(E\\) y dividimos entre las maneras en que puede ocurrir \\(F\\):\n\nEn nuestro caso hay muchos resultados posibles donde tiramos al menos un 5, por ejemplo \\((5,1), (5,2), (5,6)\\) y así sucesivamente. Sin embargo, solo en 2 de ellos la suma es 5.\n\n\n\n\nEn un espacio de resultados equiprobables, si \\(E\\) y \\(F\\) son eventos, entonces\n\\[P(E|F) = \\frac{n(E\\cap F)}{n(F)},\\]\nes decir, dividimos el número de maneras en que pueden ocurrir \\(E\\) y \\(F\\) simultáneamente entre el número de maneras en que puede ocurrir \\(F\\).\n\n\nNótese que otra manera de ver esta definición es como sigue: una vez que sabemos que ocurrió \\(F\\), restringimos todo nuestro análisis a resultados dentro de \\(F\\), y proseguimos como si se tratara de una probabilidad usual.\n\n\n6.1.2 Ejemplo: dos volados\nSupongamos que tiramos dos volados. Cuál es la probabilidad condicional de que los dos volados sean sol (evento \\(E\\)) dado que 1) El primer volado es sol? 2) Alguno de los dos volados es sol, 3) Los dos volados son águilas?\nHay 2 resultados donde el primer volado es sol (enuméralos), así que la primer probabilidad es \\(P(E|F_1)=1/2\\). Explica por qué la segunda probabilidad condicional es igual a \\(P(E|F_2)=1/3\\). ¿Cuánto vale \\(P(E|F_3)\\)?\n\n\n6.1.3 Ejemplo: tres cartas\nSupongamos que extraemos tres cartas al azar de una baraja de 52 cartas (13 son corazones). Nos muestran que la segunda y la tercera carta son corazones. ¿Cuál es la probabilidad condicional de que la primera sea un corazón?\nSolución: como resultados para las primeras tres cartas seleccionadas tenemos \\((x_1,x_2,x_3)\\). Nos interesan solamente los resultados \\((x_1, corazon_1, corazon_2)\\). Existen \\(13*12*11 + 39*13*12 = 7800\\) resultados posibles (primero contamos los que tienen un corazón al principio, y luego los que no tienen un corazón al principio, de modo que la probabilidad que buscamos es\n\\[\\frac{13(12)(11)}{13(12)(11) + 39(13)(12)} = \\frac{11}{11 + 39} = 0.22\\]\nInterpreta la simplificación de arriba para describir una manera más simple de calcular esta probabilidad condicional."
  },
  {
    "objectID": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "href": "prob-condicional.html#simulación-y-probabilidad-condicional",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.2 Simulación y probabilidad condicional",
    "text": "6.2 Simulación y probabilidad condicional\nUna manera de aproximar probabilidades condicionales es simulando el experimento que nos interesa, y calculando frecuencias relativas solamente sobre la información que sabemos que ocurrió: es decir, filtramos las simulaciones escogiendo sólo las que son consistentes con la información dada.\n\nEjemplo: simulación tres cartas\n\n\nCódigo\nbaraja <- tibble(numero = 1:13) %>% \n   crossing(tibble(figura = c(\"C\", \"D\", \"T\", \"P\"))) %>% \n   mutate(carta = paste(numero, figura))\nnrow(baraja)\n\n\n[1] 52\n\n\nCódigo\nbaraja\n\n\n# A tibble: 52 × 3\n   numero figura carta\n    <int> <chr>  <chr>\n 1      1 C      1 C  \n 2      1 D      1 D  \n 3      1 P      1 P  \n 4      1 T      1 T  \n 5      2 C      2 C  \n 6      2 D      2 D  \n 7      2 P      2 P  \n 8      2 T      2 T  \n 9      3 C      3 C  \n10      3 D      3 D  \n# … with 42 more rows\n\n\n\n\nCódigo\ncartas <- baraja$carta\nexp_3_cartas <- function(cartas){\n   sample(cartas, 3) \n}\nset.seed(132185)\nexp_3_cartas(cartas)\n\n\n[1] \"7 C\"  \"8 P\"  \"13 T\"\n\n\nSimulamos el experimento\n\n\nCódigo\nsims <- map(1:20000, ~ exp_3_cartas(cartas))\nsims[1:5]\n\n\n[[1]]\n[1] \"4 C\"  \"8 T\"  \"12 D\"\n\n[[2]]\n[1] \"11 D\" \"6 P\"  \"7 C\" \n\n[[3]]\n[1] \"12 C\" \"1 D\"  \"5 C\" \n\n[[4]]\n[1] \"12 D\" \"9 D\"  \"13 T\"\n\n[[5]]\n[1] \"7 T\"  \"13 D\" \"13 T\"\n\n\nSin más información, la probabilidad de corazón en la primera extracción es:\n\n\nCódigo\nsims %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n\n[1] 0.2474\n\n\nQue debe estar alrededor de 1/4. Ahora condicionamos a que las cartas 2 y 3 son corazones:\n\n\nCódigo\nsims_F <- sims %>%\n   keep(~ str_detect(.x[2], \"C\") & str_detect(.x[3], \"C\"))\nlength(sims_F)\n\n\n[1] 1203\n\n\nCódigo\nsims_F[1:5]\n\n\n[[1]]\n[1] \"9 T\"  \"3 C\"  \"12 C\"\n\n[[2]]\n[1] \"5 D\"  \"2 C\"  \"13 C\"\n\n[[3]]\n[1] \"5 C\"  \"10 C\" \"1 C\" \n\n[[4]]\n[1] \"4 P\" \"6 C\" \"1 C\"\n\n[[5]]\n[1] \"8 C\"  \"13 C\" \"11 C\"\n\n\nY sobre estas simulaciones hacemos el mismo cálculo de arriba:\n\n\nCódigo\nsims_F  %>% \n   map_lgl(~ str_detect(.x[1], \"C\")) %>% \n   mean()\n\n\n[1] 0.2236076\n\n\nEsto nos da una aproximación de \\(P(E|F)\\). Nótese que si \\(F\\) es un evento con probabilidad baja, entonces será necesario correr más veces el experimento, pues el número de veces que ocurre \\(F\\) es relativamente bajo.\nPodemos definir también la probabilidad condicional en general, para cualquier probabilidad \\(P\\) no necesariamente resultante de un modelo equiprobable:\n\n\n\nLa **probabilidad condicional* del evento \\(E\\) dado que ocurrió el evento \\(F\\) se define como\n\\[P(E|F) = \\frac{P(E y F)}{P(F)}\\]"
  },
  {
    "objectID": "prob-condicional.html#regla-de-la-multiplicación",
    "href": "prob-condicional.html#regla-de-la-multiplicación",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.3 Regla de la multiplicación",
    "text": "6.3 Regla de la multiplicación\nA veces nos interesa calcular la probabilidad de que dos eventos ocurran, y conocemos \\(P(F)\\) y \\(P(E|F)\\). En ese caso podemos usar la definición de probabilidad condicional para escribir al regla del producto:\n\\[P(EF) = P(F)P(E|F)\\]\nPor ejemplo, si queremos calcular la probablidad de extraer dos corazones de una baraja usual, tenemos que \\(P(C_1) = 13/52 = 1/4\\). \\(P(C_2|C_1)\\) es fácil de calcular, pues si la primera carta que sacamos es un corazón, entonces para la segunda extracción hay 51 cartas, de las cuales 12 son corazones, de forma que \\(P(C_2|C_1) = 12/51\\). Usando la regla del producto, quedamos con\n\\[P(C_1C_2) = P(C_1)P(C_2|C_1) = \\frac{13}{52}\\frac{12}{51} \\approx 0.0588\\]\nPodemos generalizar esto a\n\n\n\nRegla del producto\n\\[P(E_1E_2E_3\\cdots E_n) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\cdots P(E_n|E_1\\cdots E_{n-1}\\]\n\n\n\nEjercicio\nSe divide al azar una baraja de 52 de cartas en 4 pilas iguales. Calcula la probabilidad de cada pila tenga exactamente un as.\nPodemos hacer el evento \\(E_1\\) que el as de corazones y el de diamantes están en diferentes pilas, \\(E_2\\) el evento de que el as de corazones, el de diamantes, y el de tréboles están en diferentes pilas, y finalmente \\(E_3\\) el evento de que todos los ases están en diferentes pilas. Nótese que buscamos la probabilidad \\(P(E_3)\\), pero será más fácil si escribimos:\n\\[P(E_3) = P(E_3E_2E_1) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\\]\nPrimero, el as de corazones está en alguna de las pilas. La probabilidad de el as de diamantes no esté en esa pila es \\(P(E_1) = 1 - 12/51 = 39/51\\) (¿por qué?), pues la pila que tiene el as de corazones tiene otras 12 cartas de las 51 disponibles. La probabilidad de que el as de diamantes sea una de esas 12 cartas es entonces 12/51.\nSi se cumple \\(E_1\\), entonces el as de corazones y de diamantes están en pilas distintas. Entonces la probabilidad de que el as de tréboles caiga en una de esas dos pilas es \\(24/50\\), así que\n\\[P(E_2|E_1) = 1 -24/50 = 25/50\\]\nFinalmente, si el as de corazones, diamantes y tréboles están en distintas pilas, entonces la probabilidad de que el de espadas caiga en la una de esas pilas es \\(36/49\\), de modo que\n\\[P(E_3|E_2E_1) = 1 - 36/49 = 13/49\\].\nUsando la reglal producto,\n\\[P(E_1E_2E_3) = \\frac{39(26)(13)}{51(50)(49)} \\approx 0.105\\]"
  },
  {
    "objectID": "prob-condicional.html#nota-falacias-probabilísticas",
    "href": "prob-condicional.html#nota-falacias-probabilísticas",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.4 Nota: falacias probabilísticas",
    "text": "6.4 Nota: falacias probabilísticas\nUna de las razones por las que es importante usar la teoría de probabilidad para manipular probabilidades es que las personas, naturalmente, tienden a hacer “cálculos mentales probabilísticos” incorrectos, lo cual a fin de cuentas lleva a decisiones mal informadas. Existe amplia evidencia de esto, ver por ejemplo el trabajo de: Kahneman y Tversky\n“Before their work, economists had gotten far in their analyses of decision making under uncertainty by assuming that people correctly estimate probabilities of various outcomes or, at least, do not estimate these probabilities in a biased way […]. But Kahneman and Tversky found that this is not true: the vast majority of people misestimate probabilities in predictable ways.”\nAhora podemos descutir una de ellas, la falacia de conjunción. Consideremos la siguiente pregunta:\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\n\n¿Qué es más probable?\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement.\n\n¿Qué crees que tiende a escoger la mayoria de las personas como la más probable? ¿Qué regla de probabilidad puedes usar para demostrar que esto es una falacia?"
  },
  {
    "objectID": "prob-condicional.html#regla-de-probablilidad-total",
    "href": "prob-condicional.html#regla-de-probablilidad-total",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.5 Regla de probablilidad total",
    "text": "6.5 Regla de probablilidad total\nUna regla que usaremos varias veces es la regla de probabilidad total, que establece que\n\\[P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\\]\ndonde el evento \\(F^c\\) significa que \\(F\\) no ocurrió.\nEsta regla es útil en muchos casos para calcular probabilidades de un evento dependiendo de la ocurrencia o no de otro.\n\nEjemplo\nSacamos dos cartas de una bajara de 52 cartas. Vimos un ejemplo donde queríamos calcular la probabilidad de \\(N_2=\\) la segunda carta que sacamos es negra. Argumentamos usando espacio equiprobables que \\(P(N_2) = 0.5\\).\nSi \\(N_1 =\\) la primera carta es negra, entonces la ley de probabilidad total explica por qué pasa esto tomando en cuenta el resultado de la primera extracción:\n\nTenemos que \\(P(N_2|N_1) = 25/51\\) y \\(P(N_1) = 1/2\\)\nAdemás, \\(P(N_2|N_1^c) = 26/51\\) y \\(P(N_1^c) = 1/2,\\)\n\nde forma que\n\\[P(N_2) = (25/51)(1/2) + (26/51)(1/2) = \\frac{(25 + 26)}{51(2)} = /2\\]\nLa ley de probabilidades totales consiste de las reglas de ponderación usuales que conocemos.\n\n\nEjemplo\nEn un país hay 20% de adultos de 20 años o menos, y 80% de adultos de 21 años o más. Entre los adultos de 20 años o menos, el 90% solo usa plataformas digitales para ver televisión. Entre los adultos de 21 años o más, el 15% solo usa plataformas digitales para ver televisión. Si tomamos un adulto al azar de esta población, ¿cuál es la probabilidad de que solo use digital para ver TV?\nLa respuesta es\n\\[ 0.90(0.20) + 0.15(0.80) = 0.3\\]"
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.6 Ejercicio: dados y monedas",
    "text": "6.6 Ejercicio: dados y monedas\nSupongamos que tiramos un dado. Tiramos tantos volados como el número que salió en la tirada de dado, y registramos el número de soles. ¿Cuál es la probabilidad de que obtengamos cero soles?\nSea \\(X=\\) número que obtuvimos en la tirada de dado, y sea \\(Y=\\) número de soles obtenidos.\nCalcular directamente \\(P(Y=0)\\) puede hacerse de manera simple con la ley de probabilidad total, pues\n\n\\(P(Y=0|X=1) = 1/2\\), prob de ningún sol en 1 volado\n\\(P(Y=0|X = 2) = 1/4\\) prob de ningún sol en dos volados (suponiendo independencia de los volados)\n\\(P(Y=0|X=3) = 1/8\\), prob de ningún sol en tres volados.\n\ny así sucesivamente. Como \\(P(X=i)=1/6\\) para cualquier número del uno al seis, la probabilidad \\(P(Y=0)\\), usando probabilidad total, es\n\\[(1/6)(1/2) + (1/6)(1/2)^2 +(1/6)(1/2)^3 +(1/6)(1/2)^4 +(1/6)(1/2)^5 +(1/6)(1/2)^6\\]\nque es igual a\n\n\nCódigo\nprobs_x <- rep(1/6, 6)\nprobs_y_x = 0.5^(1:6)\nsum(probs_y_x * probs_x)\n\n\n[1] 0.1640625\n\n\nCheca usando simulación."
  },
  {
    "objectID": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "href": "prob-condicional.html#ejercicio-dados-y-monedas-simulación",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.7 Ejercicio: dados y monedas (simulación)",
    "text": "6.7 Ejercicio: dados y monedas (simulación)\nEste es un experimento más interesante para simular:\n\n\nCódigo\nsimular_soles <- function(){\n   dado <- sample(1:6, 1)\n   soles <- sample(c(\"sol\", \"águila\"), dado, replace = TRUE)\n   num_soles = sum(as.numeric(soles == \"sol\"))\n   num_soles\n}\nset.seed(82332)\nsimular_soles()\n\n\n[1] 3\n\n\nSi hacemos 10 mil simulaciones:\n\n\nCódigo\nsims <- map_dbl(1:10000, ~ simular_soles())\nqplot(sims)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLa frecuencia relativa de cero soles es:\n\n\nCódigo\nsum(sims==0) / length(sims)\n\n\n[1] 0.1655\n\n\nCalcula también la probabilidad de obtener 2 soles o más en este experimento (puedes usar la simulación)."
  },
  {
    "objectID": "prob-condicional.html#regla-de-bayes",
    "href": "prob-condicional.html#regla-de-bayes",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.8 Regla de Bayes",
    "text": "6.8 Regla de Bayes\nLa regla de Bayes es una fórmula que se utiliza para invertir probabilidades condicionales:\n\\[P(E|F) = \\frac{P(F|E)P(E)}{P(F)},\\]\nque es una consecuencia fácil de la definición de probabilidad condicional. Es útil conocerla porque facilita resolver varios problemas de probabilidad que en un principio parecen difíciles.\n\nEjemplo\nSupongamos que tenemos una baraja de 8 cartas, 4 negras y 4 blancas. Extraemos sucesivamente dos cartas a azar. Sea \\(N_1=\\) la primera carta es negra y \\(B_2=\\) la segunda carta es blanca. ¿Cuál es la probabilidad condicional de que la primera carta haya sido negra si nos dicen que la segunda fue blanca?\nAunque ya resolvimos problemas como este, parece confuso en un principio. Sin embargo, podemos calcular:\n\\[P(N_1|B_2) = \\frac{P(B_2|N_1)P(N_1)}{P(B_2)}\\]\nSabemos que \\(P(B_2|N_1) = 4/7\\), y que \\(P(N_1)=1/2\\) y \\(P(B_2) = 1/2\\), de modo que\n\\[P(N_1|B_2) = 4/7 > 1/2\\]\nes decir, si sabemos que la segunda carta fue blanca, eso hace más probable que la primera carta haya sido negra.\n\n\nEjemplo (parte 1)\nSupongamos que una aseguradora cree que hay dos tipos de personas: unos con más riesgo de tener accidentes y otros con menos riesgo. Los datos muestran que una persona con riesgo alto tendrá un accidente en algún momento del año con probabilidad 0.04, y esta probabilidad baja a 0.01 para una persona de riesgo bajo. Si 10% de la población tiene riesgo alto, ¿cuál es la probabilidad de que un asegurado nuevo tenga un accidente un año después de comprar su póliza? (Nota: no sabemos si la persona nueva es de riesgo alto o bajo).\nPuedes resolver son simulación, o usar la ley de probabilidad total. Si \\(A\\) es el evento de tener un accidente, \\(R_A\\) es el evento de que la persona es de riesgo alto y \\(R_B\\) es el evento que la persona es de riesgo bajo, entonces\n\\[P(A) = P(A|R_A)P(R_A) + P(A|R_B)P(R_B)\\]\npues \\(R_A\\) y \\(R_B\\) cubren todas las posibilidades. Entonces\n\\[P(A) = 0.04(0.10) + (0.01)(0.90) = 0.004 + 0.009 = 0.013\\]\nEs decir, su probabilidad es de 1.3% de tener una accidente en el primer año.\n\n\nEjemplo (parte 2)\nAhora vemos que un cliente tuvo un accidente en su primer año. ¿Cuál es la probabilidad de que sea un cliente de riesgo alto?\nLa pregunta es de probabilidad condicional, porque ya tenemos información. Queremos calcular\n\\[P(R_A| A)\\]\nSi usamos la regla de Bayes obtenemos\n\\[P(R_A|A)= \\frac{P(A|R_A)P(R_A)}{P(A)}\\]\nSustituimos los datos que tenemos\n\\[P(R_A|A)= \\frac{0.04(0.10)}{P(A)}\\]\ny \\(P(A)\\) que calculamos en el ejercicio anterior:\n\\[P(R_A|A)= \\frac{0.04(0.10)}{0.013} \\approx 0.3077\\]\nDe manera que al principio la probabilidad no condicionada de ser de alto riesgo era de 10%, y cuando tiene un accidente esta probabilidad se triplica.\n\n\nEjemplo\nSupongamos que en un concurso de TV tenemos tres puertas y debemos escoger una. Atrás de una de ellas hay un premio, y no hay nada detrás de las otras dos. Escogemos una de las puertas.\nAhora el conductor (que sabe dónde está el premio), abre una puerta vacía, y nos pregunta si queremos cambiar o no de puerta. ¿Cuál es la mejor estrategia, cambiar o quedarnos con la que escogimos al principio?\nVeamos la estrategia de quedarnos con la puerta que escogimos. Sin perdida de generalidad, suponemos que escogemos la puerta 1.\nAhora observamos que el conductor abre la puerta 2.\nSea \\(E_1=\\) el premio está en la puerta 1, y \\(A_2=\\) el conductor abre la puerta 2, donde no hay premio. Por la regla de bayes:\n\\[P(E_1 | A_2) = \\frac{P(A_2 | E_1) P(E_1)}{P(A_2)}\\]\nSabemos que \\(P(E_1)= 1/3\\), y que \\(P(A_2|E_1)=1/2\\) (pues el conductor pudo abrir la puerta 2 o 3). Adicionalmente\n\\[P(A_2) = P(A_2|E_1)P(E_1) + P(A_2|E_1^c)P(E_1^c) = (1/2)(1/3) + (1/2) (2/3) = 1/2\\]\npues \\(P(A_2|E_1^c) = 1/2\\), es decir, si el premio no está en la puerta 1, la probabilidad de que abrir la puerta 2 es la probabilidad de que el premio esté en la puerta 2 dado que no está en la puerta 1\nSustituyendo,\n\\[P(E_1 | A_2) = \\frac{(1/2)(1/3)}{1/2} = 1/3\\]\nAsi que si cambiamos, la probabilidad de ganar es de 2/3.\nSimula para verificar tus resultados."
  },
  {
    "objectID": "prob-condicional.html#independencia",
    "href": "prob-condicional.html#independencia",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.9 Independencia",
    "text": "6.9 Independencia\nCuando tenemos dos eventos, y tenemos que \\(P(E|F) > P(E)\\) o \\(P(F|E) > P(F)\\) (checa que una implica la otra), decimos que los eventos tienen dependencia positiva: cuando sabemos que uno ocurre, la probabiidad del otro aumenta.\nEn algunos casos \\(P(E|F) = P(E)\\) y \\(P(F|E) = P(F)\\), lo cual sucede cuando \\(P(EF)=P(F)P(E)\\). En este caso decimos que los eventos \\(E\\) y \\(F\\) son independientes. Nótese que esto no quiere decir que no haya ninguna conexión entre \\(E\\) y \\(F\\) (puede ser que la ocurrencia de \\(F\\) cambie las maneras en que puede ocurrir \\(E\\)), sólo que la probabilidad de uno no cambia al condicionar al otro.\n\nEjemplos\n\nMuestra que si sabemos que sacar un corazón de una baraja de 52 cartas es independiente de sacar un as\nMuestra que en nuestro modelo equiprobable de dos volados, el resultado de un volado es independiente del resultado del otro.\nEl evento “la suma de la tirada de dos dados” es independiente del resultado del primer dado."
  },
  {
    "objectID": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "href": "prob-condicional.html#independencia-de-más-de-dos-eventos",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.10 Independencia de más de dos eventos",
    "text": "6.10 Independencia de más de dos eventos\nNótese que cuando los eventos \\(E\\) y \\(F\\) son indpendientes, por definición\n\\[P(E \\,y\\, F) = P(E) P(F)\\]\nDecimos que los eventos \\(E\\), \\(F\\) y \\(G\\) son independientes cuando\n\n\\(P(E \\,y \\,F \\, y \\, G) = P(E)P(F)P(G)\\)\n\\(P(E \\,y \\,F ) = P(E)P(F)\\)\n\\(P(E \\,y \\, G) = P(E)P(G)\\)\n\\(P(F \\, y \\, G) = P(F)P(G)\\)\n\ny así sucesivamente para un número mayor de eventos: si los eventos son independientes, la probabilidad de que ocurran cualquier subconjunto de ellos es es el producto de la probabilidades de que cada uno de ellos ocurra.\nEn general, si \\(E_1, E_2, \\ldots, E_n\\) son eventos independientes, entonces\n\\[P(E_1\\, y \\, E_2 \\, y \\cdots \\,y E_n) = P(E_1)P(E_2)\\cdots P(E_n)\\]\n\nEjercicio\nHacemos un número indefinido de pruebas independientes, y cada una de ellas puede resultar en éxito con probabilidad \\(p\\) y fracaso con probabilidad \\(1-p\\). Calcula la probabilidad de que 1) al menos un éxito suceda en la primeras 20 pruebas. 2) todas las pruebas sean éxito y 3) exactamente 5 de las 2o pruebas sean éxito.\n\n\nEjemplo: número de seises\nConstruye un modelo para el número de seises en dos tiradas de dados. Escribimos \\(X\\)= número de seises que obtenemos en dos tiradas de dado.\nLos resultados posibles son 0, 1 y 2 seises. Para calcular la probablidad de tirar dos seises hacemos:\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2 | S_1) P(S_1).\\]\nahora, si suponemos que el primer tiro no afecta de ninguna manera el resultado del segundo tiro, entonces \\(P(S_2|S_1) = P(S_2)\\), y la fórmula es\n\\[P(X=2) = P(S_2\\, y \\, S_1) = P(S_2) P(S_1) = (1/6)(1/6) = 1/36.\\]\nUsando el mismo argumento podemos calcular de la probabilidad de obtener ningún seis es\n\\[P(X=0) = P(S_2^c \\, y \\, S_1^c ) = P(S_2^c)P(S_1^c) = (5/6)(5/6) = 25/36\\]\nLa probabilidad restante se puede calcularse directamente, o notar que como 0, 1 y 2 son los únicos posibles resultados, entonces\n\\[P(X=1) = 1- P(X=0) - P(X=2) = 1 - 25/36 - 1/36 = 10/36\\]\nCheca tus resultados usando simulación.\nObservación: en muchos casos, la independencia se construye como un supuesto para construir modelos más complejos, cuando este supuesto es adecuado. Un ejemplo es cuando tomamos muestras de una población: si tomamos cada muestra independientemente de las otras, analizar los resultados es mucho más fácil que cuando hay esquemas complejos de dependencias entre los datos que xtraemos."
  },
  {
    "objectID": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "href": "prob-condicional.html#intro-a-estimación-por-máxima-verosimilitud",
    "title": "6  Probabilidad condicional e independencia",
    "section": "6.11 Intro a estimación por máxima verosimilitud",
    "text": "6.11 Intro a estimación por máxima verosimilitud\nUno de los procedimientos más estándar para estimar cantidades desconocidas es el método de máxima verosimilitud. Los estimadores de máxima verosimilitud tienen propiedades convenientes, y dan en general resultados razonables siempre y cuando los supuestos sean razonables.\nMáxima verosimilitud es un proceso intuitivo, y consiste en aprender o estimar valores de parámetros desconocidos suponiendo para los datos su explicación más probable. Para esto, usando supuestos y modelos requeriremos calcular la probabilidad de un conjunto de observaciones.\n\nEjemplo 1\nSupongamos que una máquina produce dos tipos de bolsas de 25 galletas: la mitad de las veces produce una bolsa con 5 galletas de avena y 20 de chispas de chocolate, y la otra mitad produce galletas con 23 galletas de avena y 2 de chispas de chocolate.\nTomamos una bolsa, y no sabemos qué tipo de bolsa es (parámetro desconocido). Extraemos al azar una de las galletas, y es de chispas de chocolate (observación).\nPor máxima verosimilitud, inferimos que la bolsa que estamos considerando tiene 5 galletas de avena. Esto es porque es más probable observar una galleta de chispas en las bolsas que contienen 5 galletas de avena que en las bolsas que contienen 23 galletas de avena. Podemos cuantificar la probabilidad que “acertemos” en nuestra inferencia.\n\nCómo se aprecia en el ejemplo anterior, el esquema general es:\n\nExiste un proceso del que podemos obtener observaciones de algún sistema o población real.\nTenemos un modelo probabilístico que dice cómo se producen esas observaciones a partir del sistema o población real.\nUsualmente este modelo tiene algunas cantidades que no conocemos, que rigen el proceso y cómo se relaciona el proceso con las observaciones.\n\nNuestro propósito es:\n\nExtraemos observaciones del proceso:\n\n\\[x_1, x_2, \\ldots, x_n\\]\n\nQueremos aprender de los parámetros desconocidos del proceso para calcular cantidades de interés acerca del sistema o población real\n\nEn principio, los modelos que consideramos pueden ser complicados y tener varias partes o parámetros. Veamos primero un ejemplo clásico con un solo parámetro, y cómo lo resolveríamos usando máxima verosimilitud.\nNota: Cuando decimos muestra en general nos referimos a observaciones independientes obtenidas del mismo proceso (ver la sección anterior para ver qué significa que sea independientes). Este esquema es un supuesto que simplifica mucho los cálculos, como discutimos antes. Muchas veces este supuesto sale del diseño de la muestra o del estudio, pero en todo caso es importante considerar si es razonable o no para nuestro problema particular.\n\n\nEjemplo\nSupongamos que queremos saber qué proporción de registros de una base de datos tiene algún error menor de captura. No podemos revisar todos los registros, así que tomamos una muestra de 8 registros, escogiendo uno por uno al azar de manera independiente. Revisamos los 8 registros, y obtenemos los siguientes datos:\n\\[x_1 = 0, x_2 = 1, x_3 = 0, x_4 = 0, x_5 =1, x_6 =0, x_7 =0, x_8 =0\\]\ndonde 1 indica un error menor. Encontramos dos errores menores. ¿Cómo estimamos el número de registros con errores leves en la base de datos?\nYa sabemos una respuesta razonable para nuestro estimador puntual, que sería \\(\\hat{p}=2/8=0.25\\). Veamos cómo se obtendría por máxima verosimilitud.\nSegún el proceso con el que se construyó la muestra, debemos dar una probabilidad de observar los 2 errores en 8 registros. Supongamos que en realidad existe una proporción \\(p\\) de que un registro tenga un error. Entonces calculamos\nProbabilidad de observar la muestra:\n\\[P(X_1 = 0, X_2 = 1, X_3 = 0, X_4 = 0, X_5 =1, X_6 =0, X_7 =0, X_8 =0)\\]\nes igual a\n\\[P(X_1 = 0)P(X_2 = 1)P(X_3 = 0)P( X_4 = 0)P(X_5 =1)P(X_6 =0)P(X_7 =0)P(X_8 =0)\\]\npues la probabilidad de que cada observación sea 0 o 1 no depende de las observaciones restantes (la muestra se extrajo de manera independiente).\nEsta ultima cantidad tiene un parámetro que no conocemos: la proporcion \\(p\\) de registros con errores. Así que lo denotamos como una cantidad desconocida \\(p\\). Nótese entonces que \\(P(X_2=1) = p\\), \\(P(X_3=0) = 1-p\\) y así sucesivamente, así que la cantidad de arriba es igual a\n\\[p(1-p)p(1-p)(1-p)p(1-p)(1-p)(1-p) \\]\nque se simplifica a\n\\[ L(p) = p^2(1-p)^6\\]\nAhora la idea es encontrar la p que maximiza la probabilidad de lo que observamos. En este caso se puede hacer con cálculo, pero vamos a ver una gráfica de esta función y cómo resolverla de manera numérica.\n\n\nCódigo\nverosimilitud <- function(p){\n  p^2 * (1-p)^6\n}\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosimilitud))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.25, color = \"red\") +\n  xlab(\"p\")\n\n\n\n\n\nNótese que esta gráfica:\n\nDepende de los datos, que pensamos fijos.\nCuando cambiamos la \\(p\\), la probabilidad de observar la muestra cambia. Nos interesa ver las regiones donde la probabilidad es relativamente alta.\nEl máximo está en 0.25.\nAsí que el estimador de máxima verosimilitud es \\(\\hat{p} = 0.25\\)\n\n\nObsérvese que para hacer esto usamos:\n\nUn modelo teórico de cómo es la población con parámetros y\nInformación de como se extrajo la muestra,\n\ny resolvimos el problema de estimación convirtiéndolo en uno de optimización.\nProbamos esta idea con un proceso más complejo:\n\n\n6.11.1 Ejemplo 2\nSupongamos que una máquina puede estar funcionando correctamente o no en cada corrida. Cada corrida se producen 500 productos, y se muestrean 10 para detectar defectos. Cuando la máquina funciona correctamente, la tasa de defectos es de 3%. Cuando la máquina no está funcionando correctamente la tasa de defectos es de 10%\nSupongamos que escogemos al azar 11 corridas, y obervamos los siguientes datos de número de defectuosos:\n\\[1, 0, 0, 3 ,0, 0, 0, 2, 1, 0, 0\\]\nLa pregunta es: ¿qué porcentaje del tiempo la máquina está funcionando correctamente?\nPrimero pensemos en una corrida. La probabilidad de observar una sucesión particular de \\(r\\) defectos es\n\\[0.03^r(0.97)^{(10-r)}\\]\ncuando la máquina está funcionando correctamente.\nSi la máquina está fallando, la misma probabilidad es\n\\[0.2^r(0.8)^{(10-r)}\\]\nAhora supongamos que la máquina trabaja correctamente en una proporción \\(p\\) de las corridas. Entonces la probabilidad de observar \\(r\\) fallas se calcula promediando (probabilidad total) sobre las probabilidades de que la máquina esté funcionando bien o no:\n\\[0.03^r(0.97)^{(10-r)}p + 0.2^r(0.8)^{(10-r)}(1-p)\\]\nY esta es nuestra función de verosimilitud para una observación.\nSuponemos que las \\(r_1,r_2, \\ldots, r_{11}\\) observaciones son independientes (por ejemplo, después de cada corrida la máquina se prepara de una manera estándar, y es como si el proceso comenzara otra vez). Entonces tenemos que multiplicar estas probabilidades para cada observación \\(r_1\\):\n\n\nCódigo\ncalc_verosim <- function(r){\n  q_func <- 0.03^r*(0.97)^(10-r)\n  q_falla <- 0.2^r*(0.8)^(10-r)\n  function(p){\n    #nota: esta no es la mejor manera de calcularlo, hay \n    # que usar logaritmos.\n    prod(p*q_func + (1-p)*q_falla)\n  }\n}\nverosim <- calc_verosim(c(1, 0, 0, 3, 0, 0, 0, 2, 1, 0, 0))\nverosim(0.1)\n\n\n[1] 2.692087e-14\n\n\n\n\nCódigo\ndat_verosim <- tibble(x = seq(0,1, 0.001)) %>% mutate(prob = map_dbl(x, verosim))\nggplot(dat_verosim, aes(x = x, y = prob)) + geom_line() +\n  geom_vline(xintercept = 0.8, color = \"red\") +\n  xlab(\"prop funcionado\")\n\n\n\n\n\nY nuestra estimación puntual sería de alrededor de 80%.\n\n\nAspectos numéricos\nCuando calculamos la verosimilitud arriba, nótese que estamos multiplicando números que pueden ser muy chicos (por ejemplo \\(p^6\\), etc). Esto puede producir desbordes numéricos fácilmente. Por ejemplo para un tamaño de muestra de 1000, podríamos tener que calcular\n\n\nCódigo\np <- 0.1\nproba <- (p ^ 800)*(1-p)^200\nproba\n\n\n[1] 0\n\n\nEn estos casos, es mejor hacer los cálculos en la escala logarítmica. El logaritmo convierte productos en sumas, y baja exponentes multiplicando. Si calculamos en escala logaritmica la cantidad de arriba, no tenemos problema:\n\n\nCódigo\nlog_proba <- 800 * log(p) + 200 * log(1-p)\nlog_proba\n\n\n[1] -1863.14\n\n\nAhora notemos que\n\nMaximizar la verosimilitud es lo mismo que maximizar la log-verosimilitud, pues el logaritmo es una función creciente. Si \\(x_{max}\\) es el máximo de \\(f\\), tenemos que \\(f(x_{max})>f(x)\\) para cualquier \\(x\\), entonces tomando logaritmo, \\[log(f(x_{max}))>log(f(x))\\] para cualquier \\(x\\), pues el logaritmo respeta la desigualdad por ser creciente.\nUsualmente usamos la log verosimilitud para encontrar estimador de máxima verosimilitud\nHay razónes teóricas y de interpretación por las que también es conveniente hacer esto.\n\n\n\n\n\nRoss, Sheldon M. 1998. A First Course in Probability. Fifth. Upper Saddle River, N.J.: Prentice Hall."
  },
  {
    "objectID": "modelos-continuos.html",
    "href": "modelos-continuos.html",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "",
    "text": "En la parte anterior consideramos un númeroo fijo de resultados numéricos de experimentos aleatorios, por ejemplo, cuando \\(X\\) el resultado de una tirada de dado. En este caso, un modelo de probabilidad para \\(X\\) asigna una probabilidad dada a cada posible resultado, por ejemplo\n\\[P(X=1) = 1/6\\]\ne igualmente \\(P(X=2)=\\cdots = P(X=6) = 1/6\\). En muchos casos, la cantidad \\(X\\) que nos interesa puede tomar valores numéricos arbitrarios, y en este esquema no está claro cómo asignaríamos probabilidades."
  },
  {
    "objectID": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "href": "modelos-continuos.html#modelo-equiprobable-o-uniforme",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.1 Modelo equiprobable o uniforme",
    "text": "7.1 Modelo equiprobable o uniforme\nLos modelos más simple para una medición continua \\(X\\) son los modelos uniforme.\nPara nuestra ruleta, por ejemplo, \\(X\\) puede tomar valores en el intervalo \\([0, 360)\\). Si la ruleta es justa, entonces la probabilidad de que la flecha caiga en cualquier sector \\([a,b]\\) debe ser igual. Una manera de lograr esto usar como probabilidad la proporción de la longitud de \\([a,b]\\) con respecto al total de \\([0, 360)\\):\n\\(P(X\\in [a,b]) = \\frac{b-a}{360}.\\)\n\nDiscute por qué esta asignación de probabilidades satisface las tres reglas básicas de probabilidad (axiomas) que presentamos anteriormente.\nEste es el equivalente continuos para espacios equiprobables con un número finito de resultados.\n\n::: {.cell type=‘comentario’}\n\nSupongamos que una variable aleatoria puede tomar valores en el intervalo \\([L,U]\\). La variable aleatoria es uniforme en \\([L,U]\\) cuando\n\\[P(X \\in [a,b]) = \\frac{b-a}{L-U}\\]\n</div>\\EndKnitrBlock{comentario}\n:::\n\n7.1.1 Ejemplo: ruleta sesgada\nAhora supongamos que nuestra ruleta no está del todo balanceada. Por ejemplo, podría ser estuviera colgada en una pared, y al girar la flecha es un poco más probable que la flecha apunte hacia el piso en lugar de hacia el cielo.\nEn este caso, si la dirección hacia arriba es 90 grados y hacia abajo es 270 grados, quisiéramos por ejemplo que\n\\[P(260 < X <280) < P(80 < X < 100)\\]\nY nótese que debe ser posible asignar probabilidades a cualquier sector de la ruleta con el nuevo modelo que propongamos. ¿Cómo podríamos modificar nuestra asignación de probabilidades?\nUna de las maneras más fáciles es pensando que nuestra probabilidad se obtiene integrando una funcion constante:\n\nSi \\([a,b]\\) es un sector de la ruleta con \\(a<b\\), podríamos poner\n\n\\[P(X\\in [a,b]) = \\int_a^b \\frac{1}{360} \\,dx = \\frac{b-a}{360}\\]\nDe forma que si \\(f(x)= 1/360\\) para valores \\(0 \\leq x < 360\\), nuestra probabilidad se escribe como la integral\n\\[P(X\\in [a,b]) = \\int_a^b f(x) \\,dx \\]\n\nEn este caso, probabilidad es área bajo la curva \\(f(x)=1/360\\) que se calcula integrando sobre el intervalo de interés\n\nPara generalizar la idea es la siguiente:\n\nUsamos la fórmula anterior, pero modificamos o perturbamos la función \\(f(x) = 1/360\\) para que \\(f\\) sea un poco más alta alrededor de 270 grados (abajo), y un poco más baja alrededor de 90 grados (arriba).\nLo único que necesitamos es que \\(f(x)\\) no puede tomar valores negativos (por que si no obtendríamos probabilidades negativas en algunos sectores), y la integral sobre la ruleta completa debe ser uno:\n\n\\[P(X\\in [0, 360]) = \\int_0^{360} f(x)\\,dx = 1\\]\nPodríamos utilizar por ejemplo:\n\n\nCódigo\nf_dens <- function(x){\n    x_rad <- 2 * pi * x / 360\n    (1/360) +  0.0002 * cos(x_rad - 3 * pi  / 2)\n}\ngraf_1_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"uniforme\",\n                   f = 1 / 360) \ngraf_2_tbl <- tibble(angulo = seq(0, 360, 1), tipo = \"colgada\") %>% \n    mutate(f = f_dens(angulo))\ngraf_tbl <- bind_rows(graf_1_tbl, graf_2_tbl)\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1)\n\n\n\n\n\nEl cálculo se hace ahora con área bajo la curva. Para calcular la probabilidad\n\\[P(X\\in [50, 130]),\\]\nintegramos la función \\(f\\) correspondiente, que corresponde a calcular área bajo la curva:\n\n\nCódigo\nggplot(graf_tbl, aes(x = angulo, y = f, colour = tipo)) +\n    geom_line() +\n    ylim(c(0, 0.003)) + facet_wrap(~tipo, nrow = 1) +\n    geom_area(aes(x = ifelse(angulo > 50 & angulo < 130, angulo, 0)), \n              fill=\"salmon\", alpha = 0.5)\n\n\n\n\n\nY ahora vemos que para la versión perturbada, más de la probabilidad se concentra alrededor de 270 grados que alrededor de 90. Por las propiedades de la integral, todas las propiedades usuales de probabilidad se cumplen."
  },
  {
    "objectID": "modelos-continuos.html#funciones-de-densidad",
    "href": "modelos-continuos.html#funciones-de-densidad",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.2 Funciones de densidad",
    "text": "7.2 Funciones de densidad\nCuando trabajamos con mediciones de tipo continuo, es más conveniente definir asignaciones de probabilidad utilizando funciones de densidad de probabilidad:\n\n\n\nUna función \\(f(x)\\) no negativa cuya integral es igual a 1 es una función de densidad de probabilidad. Las probabilidades asociadas se calculan integrando:\n\\[P(X\\in [a,b]) = \\int_a^b f(x)\\,dx\\]\nEn este caso decimos que \\(f\\) es la función de densidad de probabilidad asociada a la variable aleatoria \\(X\\). A este tipo de variables aleatorias les llamamos continuas."
  },
  {
    "objectID": "modelos-continuos.html#ejemplo-densidad-triangular",
    "href": "modelos-continuos.html#ejemplo-densidad-triangular",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.3 Ejemplo: densidad triangular",
    "text": "7.3 Ejemplo: densidad triangular\nSupongamos que tenemos una variable aleatoria que tiene mediana 2, y puede tomar valores entre 0 y 4. Podríamos definir una densidad como sigue: Si \\(x\\) está entre 0 y 2, entonces\n\\[f(x) = \\frac{x}{4}\\]\ny si \\(x\\) está entre 2 y 4, entonces\n\\[f(x) = 1 - \\frac{x}{4}\\]\n\n\nCódigo\ndens_triangular <- function(x){\n    (x > 0) * (x < 4) * ifelse(x < 2, x/4, 1 - x/4)\n}\ntriangular_tbl <- tibble(x = seq(-1, 5, 0.001)) %>% \n    mutate(f = dens_triangular(x)) \nggplot(triangular_tbl, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\n\nEjemplo\nSupongamos que una variable \\(X\\) tiene mediana 2 y rango de 0 a 4, con densidad triangular. ¿Cuál es la probabilidad \\(P(X>1)\\)?\nSolución: Por reglas usuales de probabilidad, \\(P(X>1) = P(1<X<2) + P(X\\geq2)\\). Tenemos que \\(P(X\\geq 2) = 0.5\\). Ahora usamos la fórmula de la densidad triangular para obtener\n\\[P(1<X<2) = \\int_{1}^{2} f(x)\\,dx = \\int_1^2 \\frac{x}{4}\\,dx =\n\\left [\\frac{x^2}{8}\\right ]_1^2 = 1/2 - 1/8 = 3/8 = 0.375\\]\nde modo que\n\\[P(X<1) = 0.375 +0.500 = 0.875\\]\nEn general, podemos dar una fórmula para una densidad triangular en el intervalo \\([A,B]\\) con mediana en \\((A + B)/2\\). ¿Cómo sería la fórmula?"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "href": "modelos-continuos.html#cuantiles-de-variables-aleatorias",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.4 Cuantiles de variables aleatorias",
    "text": "7.4 Cuantiles de variables aleatorias\nAntes vimos la definición de cuantiles para datos numéricos. Podemos definirlos también para variables aleatorias numéricas:\n::: {.cell type=‘comentario’}\n\nSea \\(p\\in (0,1)\\). El cuantil-\\(p\\) de la variable \\(X\\) con función de densidad \\(f(x)\\) es el valor \\(x(p)\\) tal que\n\\[\\int_{-\\infty}^{x(p)} f(x)\\,dx = p\\]\n</div>\\EndKnitrBlock{comentario}\n:::\nObservación: nótese que usamos como límite inferior \\(-\\infty\\) para indicar que integramos \\(f\\) sobre toda la densidad que esté a la izquierda de \\(x(p)\\).\n\nEjemplo: densidad triangular\nSupongamos que \\(X\\) tiene la densidad triangular mostrada arriba. Calcula el cuartil inferior y superior (es decir, los cuantiles 0.25 y 0.75). Para el cuartil superior, por ejemplo, buscamos al \\(x(0.75)\\) de la siguiente gráfica:\n\n\nCódigo\nsource(\"R/triangular.R\")\nggplot(triangular_tbl, aes(x = x, y = f)) +\n        geom_line() +\n    geom_area(aes(x = ifelse(x > 0 & x < qtri(0.75, 0, 4), x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0, 0.7)) +\n    annotate(\"text\", x = qtri(0.75, 0, 4), y = 0.03, label = \"x(0.75)\") +\n    annotate(\"point\", x = qtri(0.75, 0, 4), y = 0.0) \n\n\n\n\n\nComenzaremos por el cuartil inferior. Buscamos una \\(x(0.25)\\) tal que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = 0.25\\]\nSabemos que \\(x(0.25)< 2\\), pues la integral hasta 2 es 0.5, así que\n\\[\\int_0^{x(0.25)} f(x)\\,dx = \\int_0^{x(0.25)} x/4 \\,dx = \\left [ x^2/8\\right]_0^{x(0.25)} = (x(0.25))^2/8\\]\nSi queremos que este valor sea igual a 0.25, entonces despejando obtenemos\n\\[x(0.25) = \\sqrt{0.25(8)} = \\sqrt{2}\\approx 1.4142\\]\nAhora podríamos calcular la otra integral, pero por simetría podemos concluir que\n\\[x(0.75) = 2 + (2 - 1.4142) \\approx 2.5858\\]\ny concluimos que los cuartiles inferiores y superiores son aproximadamente 1.41 y 2.59\n\n\nEjercicio: densidad uniforme\nCalcula la mediana, y los percentiles 0.10 y 0.90 de una variable uniforme en \\([0, 10]\\)."
  },
  {
    "objectID": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "href": "modelos-continuos.html#comparando-cuantiles-teóricos-y-empíricos",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.5 Comparando cuantiles teóricos y empíricos",
    "text": "7.5 Comparando cuantiles teóricos y empíricos\nLos cuantiles que vimos en la parte de descriptivos para datos numéricos se llaman usualmente cuantiles empíricos. Estos cuantiles podemos compararlos con cuantiles teóricos para ver qué tan similares son, y si el modelo teórico describe adecuadamente los datos.\n\n7.5.1 Ejemplo: distribución uniforme\nSimularemos 500 datos uniformes en \\([0, 10]\\):\n\n\nCódigo\nx_sim_u <- runif(500, 0, 10)\n\n\nPodríamos calcular algunos cuantiles empíricos:\n\n\nCódigo\nquantile(x_sim_u, c(0.10, 0.50, 0.90))\n\n\n      10%       50%       90% \n0.9435257 4.8856843 8.9657996 \n\n\nPor el ejercicio anterior sabemos cuáles son los cuantiles teóricos correspondientes a una uniforme en \\([0,10]\\). Podemos calcularlos también como sigue:\n\n\nCódigo\nqunif(c(0.10, 0.5, 0.90), 0, 10)\n\n\n[1] 1 5 9\n\n\nY vemos que son muy similares los cuantiles empíricos y teóricos. Una mejor manera de considerar esta similitud es graficando todos los cuantiles empíricos y comparándolos con los teóricos:\n\n\nCódigo\nggplot(tibble(x = x_sim_u), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = stats::qunif, dparams = list(min = 0, max = 10)) +\n    xlab(\"Cuantiles Teóricos U(0,10)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\n\nY vemos que la forma de las dos distribuciones es muy similar: los cuantiles empíricos son muy similares a los teóricos. Existen algunas fluctuaciones debidas al muestreo aleatorio.\n\n\n7.5.2 Ejemplo: distribución triangular\nRepetimos para la distribución triangular. Los cuantiles que calculamos arriba son:\n\n\nCódigo\nqtri(c(0.25, 0.75), a = 0, b = 4)\n\n\n[1] 1.414214 2.585786\n\n\n\n\nCódigo\nx_sim_tri <- rtri(500, 0, 4)\nggplot(tibble(x = x_sim_tri), aes(sample = x)) +\n    geom_abline(colour = \"red\") +\n    geom_qq(distribution = qtri, dparams = list(a = 0, b = 4)) +\n    xlab(\"Cuantiles Teóricos triangular(0,4)\") + ylab(\"Cuantiles de datos\")\n\n\n\n\n\nNótese que otra vez, los cuantiles teóricos se alinean bien con los teóricos."
  },
  {
    "objectID": "modelos-continuos.html#histogramas-y-densidades",
    "href": "modelos-continuos.html#histogramas-y-densidades",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.6 Histogramas y densidades",
    "text": "7.6 Histogramas y densidades\nPara el análisis de datos usual, las gráficas cuantil-cuantil tienden a ser útiles para entender si unos datos se comportan según alguna densidad teórica. Sin embargo, muchas veces se usan histogramas, como en las siguientes gráficas:\n\n\nCódigo\nhist_1 <- ggplot(tibble(x = x_sim_u),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 10, 0.01)) %>% \n                  mutate(f = dunif(x, 0, 10)),\n              aes(x = x, y = f), colour = \"red\")\nhist_2 <- ggplot(tibble(x = x_sim_tri),\n                 aes(x = x)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.25, boundary = 0) +\n    geom_line(data = tibble(x = seq(0, 4, 0.01)) %>% \n                  mutate(f = dtri(x, 0, 4)),\n              aes(x = x, y = f), colour = \"red\")\nhist_1 + hist_2\n\n\n\n\n\nNótese la escala vertical de estos histogramas, que no es simplemente el conteo de casos que caen en cada intervalo del histograma. Para poder comparar los conteos con las densidades correspondientes, es necesario observar lo siguiente:\nSi \\(I = [a,b]\\) es un intervalo del histograma, según la densidad (teórica), la probabilidad de que un dato \\(x\\) caiga en \\(I\\) es\n\\[P(x\\in I) = \\int_{a}^b f(x)\\,dx \\approx f(a) (b-a)\\]\nLa última aproximación se debe a que en un intervalo chico \\([a,b]\\), el área bajo la curva de \\(f(x)\\) es aproximadamente igual a la base (el ancho del intervalo) por la altura en un punto de la curva (\\(f(a)\\), aunque también podríamos usar \\(f(\\frac{a+b}{2})\\) por ejemplo).\nSi tenemos \\(n\\) observaciones, esperamos entonces que caigan \\(nP(X\\in I)\\) en el intervalo \\(I\\), de forma que si \\(n([a,b])\\) es el número de observaciones que caen en \\(I = [a,b]\\), esperamos\n\\[\\frac{n([a,b])}{n} \\approx f(a)(b-a),\\]\ny despejando obtenemos\n\\[f(a)\\approx \\frac{n([a, b])}{n(b-a)}.\\]\nEsto implica que para aproximar la densidad, es necesario dividir las frecuencias relativas entre el ancho de los intervalos correspondientes, y de ahí la escala vertical de las gráficas de arriba.\nObservación: las gráficas de cuantiles son generalmente más prácticas para evaluar el ajuste a un modelo teórico, aunque son menos comunes."
  },
  {
    "objectID": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "href": "modelos-continuos.html#más-descriptivos-media-y-desviación-estándar",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.7 Más descriptivos: media y desviación estándar",
    "text": "7.7 Más descriptivos: media y desviación estándar\nPodemos utilizar cuantiles para describir modelos teóricos y también conjuntos de datos (por ejemplo, mediana para centralidad y diferencia entre cuantiles 0.9 y 0.10 para dispersión), y funcionan bien en general. Sin embargo, para modelos teóricos y conjuntos de datos particulares muchas veces es más conveniente usar medidas como la media y desviación estándar.\n\n7.7.1 Media teórica y empírica\nSabemos que la media de un conjunto de datos \\(x_1,x_2,\\ldots,x_n\\) está dada por\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots x_n}{n}.\\]\nAhora consideremos qué pasa con esta cantidad cuando las \\(x_i's\\) son observaciones independientes de una distribución con densidad teórica \\(f(x)\\). Utilizaremos simulaciones de la distribución triangular que vimos arriba\n\n\nCódigo\ndatos <- tibble(x = rtri(5000, 0, 4))\nggplot(datos, aes(x = x)) + stat_bin(breaks = seq(0, 4, 0.25))\n\n\n\n\n\nPodemos aproximar la media de estos datos promediando los valores iniciales de los intervalos de corte ponderado por el número de casos. Lo hacemos así:\n\n\nCódigo\nparticion <- seq(0, 4, 0.25)\niniciales <- head(particion, -1) # quitar último\nagrupados_cubeta <- datos %>% \n  mutate(inicial = cut(x, breaks = particion, labels = iniciales)) %>%  \n  mutate(inicial = as.numeric(as.character(inicial))) %>% \n  group_by(inicial) %>%\n  count() %>% \n  ungroup() \nagrupados_cubeta\n\n\n# A tibble: 16 × 2\n   inicial     n\n     <dbl> <int>\n 1    0       31\n 2    0.25   119\n 3    0.5    198\n 4    0.75   289\n 5    1      365\n 6    1.25   451\n 7    1.5    473\n 8    1.75   596\n 9    2      583\n10    2.25   500\n11    2.5    436\n12    2.75   354\n13    3      268\n14    3.25   176\n15    3.5    121\n16    3.75    40\n\n\nY calculamos la aproximación a la media como sigue:\n\n\nCódigo\nagrupados_cubeta %>% \n  summarise(media_aprox = sum(inicial * n) / sum(n))\n\n\n# A tibble: 1 × 1\n  media_aprox\n        <dbl>\n1        1.87\n\n\nApliquemos esta idea cuando tenemos una densidad \\(f(x)\\). Dividimos el rango de la densidad en cubetas, y aproximamos la densidad por rangos, por ejemplo:\n\n\nCódigo\nparticion <- seq(0, 4, 0.25)\nvalor <- dtri(particion, 0, 4)\napprox_tbl <- tibble(x = particion, densidad = valor)\ndensidad_tri <- tibble(x = seq(0, 4, 0.01)) %>% \n  mutate(densidad = dtri(x, 0, 4))\nggplot(densidad_tri) + \n  ylab('f(x)') + \n  geom_line(aes(x = x, y= densidad), alpha = 0.8) +\n  geom_step(data = approx_tbl, aes(x = x, y = densidad), colour = \"red\") +\n  theme_minimal() \n\n\n\n\n\nY repetimos el mismo proceso: ponderamos los valores iniciales por la altura de la densidad:\n\n\nCódigo\napprox_tbl %>% \n  summarise(media_approx = sum(x * densidad) / sum(densidad))\n\n\n# A tibble: 1 × 1\n  media_approx\n         <dbl>\n1            2\n\n\nY esta es una aproximación a la media de esta distribución.\nNótese que la cantidad que estamos calculando es\n\\[\\sum_i x_i f(x_i) \\Delta\\]\ndonde \\(\\Delta\\) es igual a\n\n\nCódigo\napprox_tbl %>% summarise(suma_densidad = 1 / sum(densidad))\n\n\n# A tibble: 1 × 1\n  suma_densidad\n          <dbl>\n1          0.25\n\n\nque es el ancho del intervalo de las particiones. Recordamos por cálculo que esta es una la aproximación a la siguiente integral:\n\\[\\sum_i x_i f(x_i) \\Delta \\approx  \\int xf(x)\\,dx\\]\nDe modo que para pasar de media de los datos \\(\\bar{x}\\) a la media de \\(\\mu_f\\) de una distribución la equivalencia es:\n$$\n{x} = x_i xf(x) = \n$$ donde\n\nUsamos la densidad en lugar de frecuencias relativas para ponderar\nUsamos integral en lugar de suma\n\nEs decir, cuando tenemos una densidad teórica continua, es necesario integrar en lugar de sumar para calcular su media.\n\n\n7.7.2 Varianza y desviación estándar\nOtra cantidad importante es la varianza de una muestra. Es una medida de dispersión, y se calcula como\n$$\n^2 = _{i=1}^n (x_i - {x})^2\n$$\nNótese que cuando los datos están altamente concentrados alrededor del valor de la media, la varianza es chica, y cuando hay más dispersión alrededor de la media, la varianza es grande.\nLa desviación estándar es la raíz cuadrada de esta cantidad:\n$$\n = \n$$ que tiene la ventaja de que está en las mismas unidades que la variable original.\nSiguiendo el mismo patrón que arriba, tenemos que integrar en lugar de sumar, y ponderar por la densidad en lugar de la frecuencia: El equivalente en una distribución teórica es también una integral, y la varianza está definida por\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x) \\,dx\\]\nLa desviación estándar \\(\\sigma\\) es la raíz cuadrada de esta cantidad.\n\\[\\sigma = \\sqrt{\\int (x - \\mu)^2 f(x)\\,dx}\\]\nEn resumen la densidad indica la frecuencia relativa de datos que esperamos observar alrededor de cada punto, y\n\nResúmenes de una distribución teórica (como cuantiles, media, varianza, etc.) se calculan integrando ponderado por la densidad.\nResúmenes de una distribución empírica se calculan sumando ponderando por la frecuencia relativa.\n\n\n\nNotación\nPara la media de una variable aleatoria \\(X\\) con densidad \\(f\\), utilizamos la notación siguiente:\n\\[\\mu = \\int x f(x)\\,dx = E(X)\\]\ny decimos también que \\(E(X)\\) es el valor esperado de \\(X\\). Igualmente, la varianza podemos escribirla como el valor esperado de la variable \\((X-\\mu)^2\\):\n\\[\\sigma^2 = \\int (x - \\mu)^2 f(x)  = E\\left ( (X-\\mu)^2 \\right)\\]\nAunque esto requiere de un teorema simple (el teorema del estadístico inconsciente) que establece que para cualquier función \\(h\\), si \\(Y=h(X)\\) y \\(f\\) es la función de densidad de \\(X\\), entonces\n\\[E(Y) = E(h(X)) = \\int h(x) f(x) \\,dx\\]\n\n\n7.7.3 ¿Cuándo usar media y desviación estándar?\nAlgunos modelos probabilísticos son más fáciles de tratar analíticamente usando media y varianza. En esos casos, conviene usar estas medidas. Esto es especialmente cierto cuando las distribuciones son simétricas y no tienen colas muy largas.\nEn cuanto a datos observados, conviene usar media y varianza cuando pretendemos modelarlos con densidades como las del párrafo anterior. En este caso, la interpretación de estos valores se hace a través de la forma de la densidad. Es importante checar (por ejemplo usando gráficas de cuantiles) que esas densidades describen apropiadamente a los datos.\n\n\nEjercicio\n\nCalcula media y desviación estándar para una densidad triangular (0,4) y otra (0,10)\nCalcula media y desviación estándar para una densidad uniforme en (0,4) y otra uniforme en (0, 10)\nContrasta tus resultados. ¿Las medias ocurren en el lugar que esperabas? ¿Qué distribuciones presentan más dispersión de acuerdo a la desviacion estándar?"
  },
  {
    "objectID": "modelos-continuos.html#la-distribución-normal",
    "href": "modelos-continuos.html#la-distribución-normal",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.8 La distribución normal",
    "text": "7.8 La distribución normal\nLa distribución normal es una que aparece naturalmente en al teoría de probabilidad.\n\n7.8.1 Promedio de variables\nConsideremos que tiramos 40 dados justos de 6 lados, y consideramos su promedio \\(\\bar{X}\\) como resultado de nuestro experimento aleatorio. ¿Cómo se ve la distribución de probabilidades de esta variable \\(\\bar{X}\\)?\nComenzamos haciendo simulacion:\n\n\nCódigo\nsimular_bolsa <- function(num_dados = 40){\n    dados <- sample(1:6, num_dados, replace = TRUE)\n    media <- mean(dados)\n    media\n}\nsimular_bolsa()\n\n\n[1] 3.575\n\n\nVeamos cómo se ven los resultados si hacemos este experimento un número grande de veces:\n\n\nCódigo\nset.seed(23)\nsims_dados <- map_dbl(1:10000, ~ simular_bolsa())\nggplot(tibble(resultado = sims_dados), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.1)\n\n\n\n\n\nY notamos una forma de “campana” interesante. Esto se explica porque típicamente tendremos tiros bajos y altos, de modo que muchos resultados de este experimento se concentran alrededor de la media de un dado (3.5). Además, existen fluctuaciones aleatorios, y a veces tenemos un poco más de tiros altos o de tiros bajos, de forma que existe dispersión alrededor de 3.5.\nSin embargo, estas desviaciones de 3.5 no pueden ser muy grandes: por ejemplo, para tener un promedio de 1, todas las tiradas de los 40 dados tendrían que dar 1, y eso es muy poco probable. Igualmente, para que el promedio fuera cercano a 6, la gran mayoría de los 40 dados deberían de dar 6, lo cual otra vez es muy poco probable. Esto explica al menos la forma general de la forma de las colas derecha e izquierda de esta distribución.\nLos dados podrían ser diferentes (por ejemplo, un poco cargados a 1 o 6, o más cargados a valores centrales), y los argumentos de arriba también se cumplirían. Lo que es sorprendente es que, independientemente de cómo sean las particularidades de los dados, la forma analítica de esta distribución que acabamos de mostrar es la misma.\nEsta forma está descrita por la densidad normal estándar:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\]\ncuya gráfica presentamos a continuación:\n\n\nCódigo\ntibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = (1/sqrt(2*pi)) * exp(-x^2 / 2)) %>% \nggplot(aes(x = x, y = f)) + geom_line()\n\n\n\n\n\nA una variable \\(Z\\) que tiene esta densidad le llamamos una variable con distribución normal estándar. Comparemos cuantiles en nuestro ejemplo:\n\n\nCódigo\nggplot(tibble(resultado = sims_dados),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 40 dados\")\n\n\n\n\n\nY notamos que los cuantiles no corresponden, pero el espaciamiento entre los cuantiles de los datos y los teóricos de la normal estándar es el mismo. Quiere decir que estas dos distribuciones tienen la misma forma, aunque estén escaladas y centradas en distintos valores.\nProbemos con promedios de 20 observaciones triangulares en \\((0,1)\\) por ejemplo. El resultado es el mismo:\n\n\nCódigo\nset.seed(23)\nsims_tri <- map_dbl(1:10000, ~ mean(rtri(20, 0 ,1)))\nggplot(tibble(resultado = sims_tri), \n       aes(x = resultado)) +\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\nCódigo\nggplot(tibble(resultado = sims_tri),\n       aes(sample = resultado)) +\n    geom_qq(distribution = stats::qnorm) +\n    xlab(\"Normal estándar teórica\") +\n    ylab(\"Promedio de 20 triangulares (0,1)\")\n\n\n\n\n\nOtra vez, la forma general es la misma, aún cuando los datos están centrados y escalados de manera distinta."
  },
  {
    "objectID": "modelos-continuos.html#la-densidad-normal-estándar",
    "href": "modelos-continuos.html#la-densidad-normal-estándar",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.9 La densidad normal estándar",
    "text": "7.9 La densidad normal estándar\nComo expicamos, la densidad normal estándar está dada por\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}},\\]\ncuya gráfica es como sigue:\n\n\nCódigo\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.01)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\nLas probabilidades asociadas a una normal estándar se calculan integrando esta curva (que tiene que hacerse de forma numérica). Por ejemplo, para calcular\n\\[P(Z < 1.5),\\]\npodemos usar\n\n\nCódigo\npnorm(1.5)\n\n\n[1] 0.9331928\n\n\nQue es el área bajo la curva mostrada abajo:\n\n\nCódigo\nnormal_std_graf <- tibble(x = seq(-3.5, 3.5, 0.005)) %>% \n    mutate(f = dnorm(x, 0, 1))\nggplot(normal_std_graf, aes(x = x, y = f)) +\n    geom_line() +\n    geom_area(aes(x = ifelse(x > -3.5 & x < 1.5, x, 0)), \n              fill=\"salmon\", alpha = 0.5) +\n    ylim(c(0,0.4)) +\n    scale_x_continuous(breaks = seq(-3.5, 3.5, 0.5))\n\n\n\n\n\nEsta es la forma de la densidad estándar. Podemos centrar esta campana en otro valor \\(\\mu\\) y aumentar la dispersión por un factor \\(\\sigma\\). Si \\(Z\\) es una variable normal estándar, la variable\n\\[X = \\mu + \\sigma Z\\]\nes una variable normal con parámetros \\((\\mu, \\sigma)\\), o de manera más compacta, decimos que \\(X\\) es \\(N(\\mu, \\sigma)\\). La distribución normal estándar es \\(N(0,1)\\).\nPor ejemplo, si escogemos \\(\\mu=5\\) y \\(\\sigma = 0.5\\), obtenemos:\n\n\nCódigo\nnormal_graf <- tibble(x = seq(3, 7, 0.01)) %>% \n    mutate(f = dnorm(x, 5, 0.5))\nggplot(normal_graf, aes(x = x, y = f)) +\n    geom_line()\n\n\n\n\n\nPodemos mostrar juntas estas dos distribuciones:\n\n\nCódigo\ndensidad_normal <- tibble(x = seq(3, 7, 0.1)) %>% \n  mutate(densidad = dnorm(x, 5, 0.5))\ndensidad_normal_estandar <- tibble(x = seq(-3, 3, 0.1)) %>% \n  mutate(densidad = dnorm(x))\ng_2 <- ggplot(densidad_normal_estandar, aes(x = x, y = densidad)) + geom_line()\ng_3 <- g_2 + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_1 <- ggplot(densidad_normal, aes(x = x, y = densidad)) + geom_line() + xlim(c(-3, 7)) + ylim(c(0, 1))\ng_3 + g_1\n\n\n\n\n\nSe puede demostrar que:\n\n\n\nDistribución normal\n\nLa distribución normal estándar \\(N(0,1)\\) tiene media 0 y desviación estándar 1\nLa distribución normal \\(N(\\mu,\\sigma)\\) tiene media \\(\\mu\\) y desviación estándar \\(\\sigma\\)"
  },
  {
    "objectID": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "href": "modelos-continuos.html#cuantiles-y-concentración-de-la-densidad-normal",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.10 Cuantiles y concentración de la densidad normal",
    "text": "7.10 Cuantiles y concentración de la densidad normal\nCon un poco de cálculo podemos ver qué tan fuertemente se concentra la densidad alrededor de la media para una distribución normal. La regla es la siguiente:\n\n68% de la densidad se concentra en el intervalo \\([\\mu-\\sigma, \\mu+\\sigma]\\)\n95% de la densidad se concentra en el intervalo \\([\\mu-2\\sigma, \\mu+2\\sigma]\\)\n99.7% de la densidad se concentra en el intervalo \\([\\mu-3\\sigma, \\mu+3\\sigma]\\)\n\n\n\nCódigo\ngrafica_concentracion <- function(mu, sigma, z){\n  x <- seq(mu - 3.1 * sigma, mu + 3.1 * sigma, 0.01)\n  valor <- dnorm(x, mu, sigma)\n  datos <- tibble(x = x, `f(x)`=valor)\n  texto <- round(100*(pnorm(z) - pnorm(-z)), 1)\n  texto <- paste0(texto, \"%\")\n  ggplot(datos, aes(x = x, y = `f(x)`)) +\n    geom_area(data = filter(datos, x < mu + z*sigma, x > mu - z*sigma), \n      fill = \"salmon\") +\n        geom_line() +\n    annotate(\"text\", x = mu, y = 0.1, label = texto) +\n    scale_x_continuous(breaks = mu + sigma*seq(-3, 3, 1)) +\n    theme_minimal() + ylab(\"\") \n}\ng_68 <- grafica_concentracion(10, 2, 1)\ng_95 <- grafica_concentracion(10, 2, 2)\ng_997 <- grafica_concentracion(10, 2, 3)\npaneles <- g_68 + g_95 + g_997\npaneles + plot_annotation(title = \"Concentración alrededor de la media (normal)\")\n\n\n\n\n\nNota: esto aplica para cualquier densidad normal, independientemente de los parámetros.\nObsérvese que esto nos da una interpretación natural de la desviación estándar de una distribución normal en términos de percentiles de los datos, y la manera usual con la que entendemos la desviación estándar de la distribución normal."
  },
  {
    "objectID": "modelos-continuos.html#el-teorema-central-del-límite",
    "href": "modelos-continuos.html#el-teorema-central-del-límite",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.11 El teorema central del límite",
    "text": "7.11 El teorema central del límite\nUna de las razones por las que el modelo normal es tan importante es el siguiente resultado:\n::: {.cell type=‘comentario’}\n\nTeorema central del límite\nSi \\(X_1,X_2,\\ldots, X_n\\) son variables aleatorias independientes con media \\(\\mu\\) y desviación estándar \\(\\sigma\\) con una densidad \\(f(x)\\):\n\n\\(S_n = X_1 + X_2 + \\cdots X_n\\) es aproximadamente normal cuando \\(n\\) es suficientemente grande\n\n:::\nMuchas cantidades de interés en la estadística se pueden definir como sumas o promedios de un número grande de variables aleatorias (por ejempo, cuando queremos estimar el total de ingreso de los hogares, estatura promedio en una población, etc.) Los percentiles de una muestra grande también cumplen un teorema central del límite de este tipo.\nLa aproximación del teorema central del límite mejora cuando \\(n\\) es más grande. Aunque una regla de dedo dice que \\(n\\geq 30\\) es suficiente para muchas distribuciones, puede ser que sea necesaria usar una \\(n\\) más grande.\n\nEsto nos permite, por ejemplo, considerar nuestro primera técnica de estimación por intervalos:\n\nObservamos una muestra grande \\(x_1,\\ldots, x_n\\) de datos de una población (no necesariamente con distribución normal). Supongamos que buscamos estimar la media \\(\\mu\\) de la población con un intervalo.\nEstimamos la media con\n\n\\[\\bar{x} = \\frac{1}{n}(x_1+\\cdots + x_n) = \\frac{1}{n}\\sum_i x_i,\\]\n\nPor el teorema del límite central, {x} es aproximadamente normal, y su media es \\(\\mu\\). Esto implica que\n\n\\[P(\\mu - 2\\sigma  \\leq \\bar{x} \\leq\\mu + 2\\sigma)\\approx 0.95\\]\nDespejando \\(\\mu\\) obtenemos\n\\[P(\\bar{x} - 2\\sigma  \\leq \\mu  \\leq\\bar{x} + 2\\sigma)\\approx 0.95\\]\nFinalmente, no conocemos \\(\\sigma\\), pero la estimamos con\n\\[\\hat{\\sigma}^2 = \\frac{1}{n}((x_1 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2) = \\frac{1}{n}\\sum_i(x_i - \\bar{x})^2\\]\nY aproximamos sustituyendo nuestra estimación:\n\\[P(\\bar{x} - 2\\hat{\\sigma } \\leq \\mu  \\leq\\bar{x} + 2\\hat{\\sigma})\\approx 0.95\\]\nEsto nos da un intervalo (llamado el intervalo de Wald) con 95% de confianza para la media poblacional:\n\\[[\\bar{x} - 2\\hat{\\sigma }  , \\bar{x} + 2\\hat{\\sigma}]\\]\nNotas:\n\nPor otras razones técnicas, a veces se usa \\(s^2 = \\frac{1}{n-1}\\sum_i (x_i-\\bar{x})^2\\) en lugar de \\(\\hat{\\sigma}^2\\). Si la muestra es grande esto no es importante.\nEstos intervalos tienen cobertura nominal de 95%, sin embargo, puede variar dependiendo del tamaño de muestra y la forma de la distribución teórica (poblacional). Existen métodos como el bootstrap donde podemos checar qué tan razonable es hacer esta aproximación. También se puede hacer simulación modelando la distribución \\(f(x)\\)."
  },
  {
    "objectID": "modelos-continuos.html#otras-densidades-comunes",
    "href": "modelos-continuos.html#otras-densidades-comunes",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.12 Otras densidades comunes",
    "text": "7.12 Otras densidades comunes\nComo vimos arriba, consideraciones teóricas hacen razonable suponer que una variable aleatoria tiene cierta distribución: por ejemplo, si una variable aleatoria es suma de muchas perturbaciones independientes, la suma o promedio resultante puede modelarse como una distribución normal.\nOtras consideraciones teóricas sugieren otro tipo formas útiles de densidades. Un primer ejemplo es la distribución exponencial.\n\n7.12.1 Variables aleatorias exponenciales\nSupongamos que estamos modelando tiempos a la ocurrencia \\(X\\) de un evento (por ejemplo en análisis de supervivencia en estudios clínicos). Esta es una variable que toma valores en los números positivos. ¿Cómo podría ser su forma?\nConsideremos por ejemplo que el tiempo de espera no tienen memoria. Es decir: si hay esperamos un periodo de \\(t\\) días por ejemplo, la distribución del tiempo restante que tenemos que esperar no depende de \\(s\\). En términos de probabilidad, podríamos escribir:\n\\[P(X > s + t | X > t) = P(X > s)\\]\nEsto se lee: dado que el evento ocurre en más de \\(t\\) días, la probabilidad de que tarde al menos otros \\(s\\) días no dependen de \\(t\\). A la función \\(S(t) = P(X>t)\\) muchas veces se le llama la función de supervivencia.\nEste es un modelo base útil, que después puede extenderse a procesos donde los eventos ocurren aceleradamente (envejecen), o donde los eventos dan evidencia de robustez (los que han sobrevivido hasta cierto tiempo se espera que duren más que lo que inicialmente), o quizá una combinación de los dos dependiendo de el valor de \\(t\\).\nCon el supuesto de falta de memoria, la ecuación de arriba se cumple, y entonces (¿por qué?):\n\\[P(X > s + t) = P(X > s) P(X > t),\\]\nasí que\n\\[\\frac{1}{s}(P(X > s+ t) - P(X>t)) = \\frac{1}{s}(P(X>s) - 1)P(X>t).\\]\nQue podemos reescribir usando la densidad \\(f(x)\\) como\n\\[\\frac{1}{s}\\int_t^{t+s} f(x)\\,dx = P(X>t)\\frac{1}{s}\\int_0^s f(x)\\,dx\\]\nConforme \\(s\\) se hace más chica, el lado izquierdo converge a \\(f(t)\\). El lado derecho, por otra parte, converge a \\(f(0)\\), y obtenemos\n\\[f(t) = f(0)\\int_t^\\infty f(x)\\,dx,\\]\ny ahora derivamos de ambos lados para obtener\n\\[f'(t) = -f(0)f(t)\\]\nLa única función que satisface esta propiedad (su derivada es proporcional a ella misma) es\n\\[f(t) = \\lambda e^{-\\lambda t}\\]\nPuedes checar que efectivamente esta densidad cumple que la ecuación anterior. A esta densidad le llamamos la densidad exponencial con tasa \\(\\lambda\\).\n\n\nCódigo\nlambda <- 1\nsim_exp <- rexp(1000, rate = lambda)\nggplot(tibble(x = sim_exp), aes(x = x)) +\n    geom_histogram(boundary = 0)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEjercicio: demuestra que una variable aleatoria exponencial con tasa \\(\\lambda\\) tiene media \\(E(X) = 1/\\lambda\\) y $Var(X) = 1 / ^2\nEl parámetro \\(\\lambda\\) se llama tasa por la interpretación de tiempo de espera que mostramos arriba. Supongamos que \\(\\lambda = 10\\). Eso quiere decir que esperamos observar el evento en \\(1/\\lambda = 1/10\\) minutos, por ejemplo, o lo que es lo mismo, a una tasa de \\(\\lambda = 10\\) eventos por minuto.\nNótese finalmente que todas las variables exponenciales son escalamientos de una variable exponencial con \\(\\lambda = 1\\). Esto es porque si \\(Y=kX\\), y \\(X\\) es exponencial con \\(\\lambda= 1\\) entonces:\n\\[P(Y>t)=P(kX > t) = P(X > t/k) = \\int_{t/k}^\\infty e^{-x}\\,dx = e^{-t/k}\\]\nDe modo que derivando, encontramos que la densidad de \\(Y\\) es \\(\\frac{1}{k} e^{-t/k}\\), que es una exponencial con tasa \\(\\lambda = 1/k\\).\n\n\n7.12.2 Ejemplo: exponencial\nSupongamos que un tipo de focos tienen tiempos de vida exponencial con una vida media de 10 años. ¿Cuál es la probabilidad de que un foco dure más de 15 años? Si tenemos un foco que ya vivió 10 años, cuál es la probabilidad de que dure otros 15 años.\nTenemos que la vida de un foco es una variable \\(X\\) exponencial con parámetro \\(\\lambda = 1/10\\). La probabilidad de que dure más de 15 años es entonces\n\\[P(X>15) = \\int_{15}^\\infty \\frac{1}{10} e^{x/10}\\,dx\\]\nPodemos calcular a mano, o usar rutinas usuales de R:\n\n\nCódigo\npexp(15, rate = 1/10, lower.tail = FALSE)\n\n\n[1] 0.2231302\n\n\n\n\n7.12.3 Variables aleatorias gamma\nEsta es otra familia que extiende la familia de distribuciones exponenciales. La forma analítica de una densidad gamma con parámetro de forma \\(k>0\\) y tasa \\(\\lambda\\)\n\\[f(x) = C x^{k-1} e^{-\\lambda x}\\]\ndonde la constante \\(C\\) de normalización depende de \\(k\\) y \\(\\lambda\\).\nAbajo vemos datos simulados de densidades Gamma con distintas combinaciones de parámetros:\n\n\nCódigo\nparams_tbl <- crossing(k = c(1, 2, 5, 10), lambda = c(1/4, 1/2))\nsims_tbl <- params_tbl %>% \n    mutate(sims = map2(k, lambda, \n                       ~ rgamma(10000, shape = .x, rate = .y))) %>% \n    unnest(cols = sims)\n\n\n\n\nCódigo\nggplot(sims_tbl, aes(x = sims)) + \n    geom_histogram(boundary = 0, bins = 50) +\n    facet_grid(k~lambda)\n\n\n\n\n\nNótese que cada columna es un rescalamiento de la otra, pero las densidades de los renglones tienen distinta forma. Puedes ver aquí parámetros como esperanza, varianza de esta estas distribuciones, junto con otras propiedades y aplicaciones."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "href": "modelos-continuos.html#modelos-conjuntos-de-probabilidad",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.13 Modelos conjuntos de probabilidad",
    "text": "7.13 Modelos conjuntos de probabilidad\nUsualmente no nos interesa una sola variable aleatoria, sino varias. Nos interesa entender cómo están relacionadas o cómo depende una de otra.\nPor ejemplo, ¿cuál es la mediana de peso para un infante de 2 meses, y qué tan diferente es de la mediana de peso para un infante de 5 meses? ¿qué relación hay entre temperatura y presión atmosférica? ¿qué relación hay entre creencias religiosas y afiliación política? En todos estos casos quisiéramos describir de distintas formas cómo ser relaciona una cantidad aleatoria con otra.\nAl principio de este curso, vimos algunas técnicas descriptivas para mostrar y explorar estas relaciones. Por ejemplo:\n\n¿Cómo cambian las preferencias de forma de tomar té dependiendo del tipo de té una persona acostumbra tomar? ¿Qué tan probable es que alguien que toma té negro use azúcar vs alguien que toma té verde? (relación entre dos variables categóricas o discretas)\n¿Cómo cambian los precios medios de las casas dependiendo del vecindario donde se ubican? (describir la dependencia de una variable numérica si sabemos el valor de una variable categórica)\n¿Cómo cambia la mediana y los cuartiles de peso de un infante dependiendo de los meses desde que nació (describir cómo depende una variable numérica de otra variable numérica)\n\nEn esta parte veremos una introducción cómo se formalizan estos conceptos en modelos probabilísticos."
  },
  {
    "objectID": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "href": "modelos-continuos.html#estaturas-modelando-relaciones-de-dependencia",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.14 Estaturas: modelando relaciones de dependencia",
    "text": "7.14 Estaturas: modelando relaciones de dependencia\nSupongamos que \\(X\\) es la edad de una persona entre 4 y 15 años. y \\(Y\\) es su estatura. La relación entre \\(X\\) y \\(Y\\) no es determinística, pues existe variación en el crecimiento para las personas por distintas razones. Por esta razón, no tiene mucho sentido dar una relación como \\(Y = 80 + 5.5 X\\), por ejemplo, pues esta relación no se cumple para prácticamente ninguna persona.\nTiene más sentido, sin embargo, decir cómo es la distribución condicional de \\(Y\\) dado que conocemos \\(X\\). Por ejemplo, podríamos hacer la hipótesis de que la mediana de estatura para una persona de edad \\(X\\) está dada por\n\\[med(Y|X) = 80 + 5.5 X\\]\nNótese que escribimos la mediana condicional de \\(Y\\) dado que conocemos el valor de \\(X\\). También podríamos escribir la media condicional de \\(Y\\) dada \\(X\\) como\n\\[E(Y|X) = 80 + 5.5 X\\]\nY estas dos cantidades tienen sentido.\nEstas cantidades claramente no determinan la variabilidad que hay de la estatura cuando conocemos \\(X\\). Podríamos entonces también especificar por ejemplo la desviación estándar condicional:\n\\[\\sigma(Y|X) = 3\\sqrt{X} \\]\nGeneralmente estas relaciones se estiman empíricamente con datos observados, pero para este ejemplo utilizaremos estos modelos fijos.\nSimulamos algunos datos con estas propiedades:\n\n\nCódigo\nedades <- runif(800, 2, 15) # edad distribuida uniforme, o grupos de edad del mismo tamaño\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rnorm(n(), mean = media, sd = desv_est))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point()\n\n\n\n\n\n\nObsérvese cómo en efecto la estatura esperada aumenta con la edad (condicional a la edad), y que la dispersión de estatura aumenta conforme la edad aumenta.\n\nNótese que si usamos un suavizador, podemos estimar la media condicional de nuestro modelo, que en este caso está cercana a la fórmula que establecimos en nuestro modelo:\n\n\nCódigo\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_smooth(se = FALSE)\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nPodemos estimar cuantiles también como vimos en secciones anteriores:\n\n\nCódigo\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 20, quantiles = c(0.10, 0.5, 0.90)) \n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 20)\n\n\n\n\n\nY observamos igualmente que la dispersión para el grupo de 15 años es cercana al doble que la dispersión para el grupo de 4 años."
  },
  {
    "objectID": "modelos-continuos.html#distribuciones-condicionales",
    "href": "modelos-continuos.html#distribuciones-condicionales",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.15 Distribuciones condicionales",
    "text": "7.15 Distribuciones condicionales\nTodo el trabajo de arriba de modelación teórica consiste entonces en definir distribuciones condicionales. En el ejemplo anterior,\n\nDimos una distribución para \\(X\\), que en este caso la tomamos uniforme en [4, 15], pues suponemos que esta es la estructura de nuestra población (hay el aproximadamente el mismo número de personas de cada edad)\nDimos una distribución para \\(Y\\) condicionada a \\(X\\). En este caso, establecimos que \\(Y|X\\) es normal con media \\(80 + 5.5X\\) y desviación estándar \\(3 * sqrt(X)\\).\n\nEstas dos partes dan un modelo conjunto para \\(X\\) y \\(Y\\): sabemos que está completamente determinado pues pudimos simular del modelo. Otra manera de entender esto es que cualquier probabilidad que involucra a \\(X\\) y \\(Y\\) puede ser calculada con la regla del producto. Aunque no entraremos en detalles, la densidad conjunta de \\(X\\) y \\(Y\\) puede definirse en este caso como\n\\[f(x,y) = f_X(x)f_{Y|X}(y|x)\\]\nSabemos que \\(f_X(x) = 1/(15- 3)\\) para \\(x\\) entre 4 y 15 años, y la forma de \\(f_{Y|X}(y|x)\\) sabemos que es normal con media y varianza conocida (en términos de x). Esta conjunta puede ser integrada sobre cualquier región (al menos en teoría) para calcular la probabilidad de interés"
  },
  {
    "objectID": "modelos-continuos.html#estaturas-variación-gamma",
    "href": "modelos-continuos.html#estaturas-variación-gamma",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.16 Estaturas: variación gamma",
    "text": "7.16 Estaturas: variación gamma\nPodemos juntar estos bloques de densidades condicionales para construir otro tipo de modelos. Por ejemplo, supondremos que la estatura dada la edad es una distribución gamma \\((k,\\lambda)\\) y la misma media y desviación estándar que vimos arriba. Como la media de una gamma de este tipo es \\(\\mu = k/\\lambda\\) y su desviación estándar es \\(\\sigma = \\sqrt{k}/\\lambda\\), podemos despejar \\(k\\) y \\(lambda\\) y hacer:\n\n\nCódigo\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 80 + 5.5*edad, desv_est = 3 * sqrt(edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(estatura_cm = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = estatura_cm)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\nEste modelo es muy similar al normal. Sin embargo, podríamos intentar otras variaciones si cambiamos la magnitud de la desviación estándar en relación a la media, por ejemplo:\n\n\nCódigo\ndatos_tbl <- \n   tibble(edad = edades) %>% \n   mutate(media = 30 + 10*edad, desv_est = 7 * (edad)) %>%\n   mutate(k = (media/desv_est)^2, lambda = media / desv_est^2) %>% \n   # para este ejemplo, simulamos con la distribución normal.\n   mutate(medicion_y = rgamma(n(), shape = k, rate = lambda))\nggplot(datos_tbl, aes(x = edad, y = medicion_y)) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 10, \n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\nEste caso claramente no serviría para modelar estaturas, pero podemos ver cómo introdujimos asimetría considerable en las distribuciones condicionales de y dado x una vez que especificamos la media y la varianza condicionales."
  },
  {
    "objectID": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "href": "modelos-continuos.html#modelos-conjuntos-para-factor-categórico",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.17 Modelos conjuntos para factor categórico",
    "text": "7.17 Modelos conjuntos para factor categórico\nSupongamos ahora que la variable \\(Y\\) es numérica y la variable \\(X\\) es categórica. En este caso, un modelo conjunto está definido por las probabilidades \\(P(X=x)\\) junto con densidades condicionales \\(Y|X\\).\n\n7.17.1 Ejemplo: cuentas y propinas\nSupongamos que \\(X\\) es la hora del día (comida y cena) y que \\(Y\\) es el tamaño de la cuenta.\nPodríamos establecer por ejemplo, que \\(Y|X=comida\\) es Gamma con media 10 dólares y desviacion estándar 10 dólares. Sin embargo, \\(Y|X=cena\\) es Gamma con media 25 dólares y desviación estándar 20 dólares. Simulamos y analizamos:\n\n\nCódigo\ndatos_tbl <- \n   tibble(hora = sample(c(\"comida\", \"cena\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(hora == \"comida\", 10, 25),\n          desv_est = ifelse(hora == \"comida\", 10, 20)) %>% \n   mutate(cuenta = rgamma(n(), shape = (media / desv_est)^2,\n                               rate = media / (desv_est^2)))\nggplot(datos_tbl, aes(x = hora, y = cuenta)) +\n   geom_boxplot()\n\n\n\n\n\nY esta gráfica busca mostrar una estimación de las distribuciones condicionales de cuenta dado el turno donde el cliente consumió."
  },
  {
    "objectID": "modelos-continuos.html#modelos-multivariados",
    "href": "modelos-continuos.html#modelos-multivariados",
    "title": "7  Modelos probabilísticos para variables continuas",
    "section": "7.18 Modelos multivariados",
    "text": "7.18 Modelos multivariados\nAhora consideremos que queremos construir un modelo conjunto para tres variables \\(X, Y\\) y \\(Z\\). La forma en que procedemos dependerá del problema particular, pero podemos usar la regla del producto como guía. Por ejemplo, podríamos dar una distribución para \\(Z\\), luego una densidad condicional de Y dado \\(Z\\), y finalmente una condicional de \\(Y\\) dada tanto \\(X\\) como \\(Z\\).\n\n\nCódigo\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    Z -> X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\n\nEn este caso, cada variable aleatorio es nodo, y representa una distribución condicional dado sus antecesores. Esta gráfica por ejemplo, muestra una manera de escribir con la regla del producto un modelo conjunto, pero podríamos cambiar de posición los nodos dependiendo de nuestro conocimiento y el problema que queremos resolver.\nEn algunos casos, es posible simplificar la construcción del modelo eliminando algunas aristas. Supongamos por ejemplo que\n\nX es la edad de la persona\nZ es “M” of “F”\nY es su estatura\n\nEn este caso, no es necesario especificar la condicional de \\(X\\) dado \\(Z\\), pues estas dos son variables independientes. Pondríamos entonces simplemente\n\n\nCódigo\nlibrary(dagitty)\ng <- dagitty('dag {\n    Z [pos=\"0,1\"]\n    X [pos=\"1,0\"]\n    Y [pos=\"2,1\"]\n    \n    X -> Y\n    Z -> Y\n}')\nplot(g)\n\n\n\n\n\nY solo necesitamos especificar las distribuciones de \\(X\\), de \\(Z\\), y la condicional de \\(Y\\) dado \\(Z\\). Siguiendo nuestro ejemplo anterior, consideraremos a\n\n\\(X\\) como uniforme en \\([4,15]\\) (que es nuestro rango de edad de interés)\n\\(Z\\) es \\(M\\) con probabilidad 0.5 y \\(F\\) con probabilidad 0.5\n\nY podríamos especificar ahora: la condicional de \\(Y\\) (estatura) es normal con los siguientes parámetros:\n\nSi \\(X\\) es la edad y \\(Z=\"F\"\\), entonces la media es \\(70 + 6.5 X\\)\nSi \\(X\\) es la edad y \\(Z=\"M\"\\), entonces la media es \\(80 + 4.5 X\\)\nLa desviación estándar sólo depende de \\(X\\), y es igual a \\(4\\sqrt{X}\\).\n\nSimulamos ahora de este modelo probabilístico:\n\n\nCódigo\ndatos_tbl <- tibble(x = runif(1000, 4, 15)) %>%\n   # independientemente simulamos M o F\n   mutate(z = sample(c(\"m\", \"f\"), 1000, replace = TRUE)) %>% \n   mutate(media = ifelse(z==\"f\", 70 + 6.5 * x, 80 + 4.5 * x)) %>% \n   mutate(desv_est = 4 * sqrt(x)) %>% \n   mutate(estatura = rnorm(n(), media, desv_est))\ndatos_tbl %>% head(20) %>% kable()\n\n\n\n\n \n  \n    x \n    z \n    media \n    desv_est \n    estatura \n  \n \n\n  \n    13.917312 \n    m \n    142.62790 \n    14.922366 \n    106.17567 \n  \n  \n    9.655767 \n    m \n    123.45095 \n    12.429492 \n    118.95931 \n  \n  \n    4.178553 \n    m \n    98.80349 \n    8.176603 \n    102.90009 \n  \n  \n    4.085706 \n    m \n    98.38568 \n    8.085252 \n    99.27880 \n  \n  \n    7.510502 \n    f \n    118.81826 \n    10.962118 \n    120.93123 \n  \n  \n    9.506507 \n    m \n    122.77928 \n    12.333049 \n    131.72703 \n  \n  \n    8.311886 \n    f \n    124.02726 \n    11.532137 \n    127.80079 \n  \n  \n    10.336809 \n    f \n    137.18926 \n    12.860363 \n    138.18963 \n  \n  \n    8.578672 \n    m \n    118.60402 \n    11.715748 \n    133.69638 \n  \n  \n    11.500391 \n    f \n    144.75254 \n    13.564891 \n    121.21221 \n  \n  \n    6.668017 \n    f \n    113.34211 \n    10.329002 \n    123.89877 \n  \n  \n    12.935783 \n    f \n    154.08259 \n    14.386540 \n    135.22746 \n  \n  \n    14.904618 \n    m \n    147.07078 \n    15.442600 \n    128.11113 \n  \n  \n    11.579196 \n    m \n    132.10638 \n    13.611287 \n    143.13437 \n  \n  \n    14.921303 \n    m \n    147.14586 \n    15.451241 \n    162.54670 \n  \n  \n    8.863920 \n    m \n    119.88764 \n    11.908934 \n    114.67713 \n  \n  \n    8.487825 \n    m \n    118.19521 \n    11.653549 \n    105.22789 \n  \n  \n    4.221525 \n    m \n    98.99686 \n    8.218540 \n    94.52403 \n  \n  \n    6.803164 \n    m \n    110.61424 \n    10.433150 \n    108.54392 \n  \n  \n    14.458625 \n    m \n    145.06381 \n    15.209799 \n    145.96106 \n  \n\n\n\n\n\nY hacemos algunas gráficas descriptivas:\n\n\nCódigo\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point()\n\n\n\n\n\n\nDiscute qué otras cosas podrías cambiar en este modelo probabilístico para que fuera más flexible o más simple. ¿Cómo ajustarías un modelo así a datos reales?\n\n\n\nCódigo\nggplot(datos_tbl, aes(x = x, y = estatura, colour = z)) +\n   geom_point() +\n   facet_wrap(~z) +\n   geom_quantile(method = \"rqss\", lambda = 10,\n                 quantiles = c(0.10, 0.5, 0.9))\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\nWarning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct\n\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 10)\n\n\n\n\n\n\n\n\nEn la modelación probabilística generalmente usamos estos mecanismos (dependencia condicional, independencia) y estos bloques (distribuciones de probabilidad dadas en términos de parámetros) para obtener estimaciones de parámetros de interés.\nLas decisiones de cómo usar estos mecanismos y bloques se desprenden de conocimiento de dominio, alcances del análisis, y siempre están sujetos a revisión dependiendo del tipo de desajustes que presenten frente a los datos reales."
  },
  {
    "objectID": "inferencia-modelos.html",
    "href": "inferencia-modelos.html",
    "title": "8  Inferencia con modelos probabilísticos",
    "section": "",
    "text": "Intro"
  },
  {
    "objectID": "modelos-bayesianos.html",
    "href": "modelos-bayesianos.html",
    "title": "9  Inferencia bayesiana",
    "section": "",
    "text": "Para esta sección seguiremos principalmente Kruschke (2015). Adicionalmente puedes ver la sección correspondiente de Chihara y Hesterberg (2018).\nEn las secciones anteriores estudiamos el método de máxima verosimilitud y métodos de remuestreo. Esto lo hemos hecho para estimar parámetros, y cuantificar la incertidumbre qué tenemos acerca de valores poblacionales. La inferencia bayesiana tiene objetivos similares.\nEl concepto probabilístico básico que utilizamos para construir estos modelos y la inferencia es el de probabilidad condicional: la probabilidad de que ocurran ciertos eventos dada la información disponible del fenómeno que nos interesa."
  },
  {
    "objectID": "modelos-bayesianos.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "href": "modelos-bayesianos.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "title": "9  Inferencia bayesiana",
    "section": "Un primer ejemplo completo de inferencia bayesiana",
    "text": "Un primer ejemplo completo de inferencia bayesiana\nConsideremos el siguiente problema: Nos dan una moneda, y solo sabemos que la moneda puede tener probabilidad \\(3/5\\) de tirar sol (está cargada a sol) o puede ser una moneda cargada a águila, con probabilidad \\(2/5\\) de tirar sol.\nVamos a lanzar la moneda dos veces y observamos su resultado (águila o sol). Queremos decir algo acerca de qué tan probable es que hayamos tirado la moneda cargada a sol o la moneda cargada a águila.\nEn este caso, tenemos dos variables: \\(X\\), que cuenta el número de soles obtenidos en el experimento aleatorio, y \\(\\theta\\), que da la probabilidad de que un volado resulte en sol (por ejemplo, si la moneda es justa entonces \\(\\theta = 0.5\\)).\n¿Qué cantidades podríamos usar para evaluar qué moneda es la que estamos usando? Si hacemos el experimento, y tiramos la moneda 2 veces, podríamos considerar la probabilidad\n\\[P(\\theta = 0.4 | X = x)\\]\ndonde \\(x\\) es el número de soles que obtuvimos en el experimento. Esta es la probabilidad condicional de que estemos tirando la moneda con probabilidad de sol 2/5 dado que observamos \\(x\\) soles. Por ejemplo, si tiramos 2 soles, deberíamos calcular\n\\[P(\\theta=0.4|X=2).\\]\n¿Cómo calculamos esta probabilidad? ¿Qué sentido tiene?\nUsando reglas de probabildad (regla de Bayes en particular), podríamos calcular\n\\[P(\\theta=0.4|X=2) = \\frac{P(X=2 | \\theta = 0.4) P(\\theta =0.4)}{P(X=2)}\\]\nNota que en el numerador uno de los factores, \\(P(X=2 | \\theta = 0.4),\\) es la verosimilitud. Así que primero necesitamos la verosimilitud:\n\\[P(X=2|\\theta = 0.4) = (0.4)^2 = 0.16.\\]\nLa novedad es que ahora tenemos que considerar la probabilidad \\(P(\\theta = 0.4)\\). Esta cantidad no la habíamos encontrado antes. Tenemos que pensar entonces que este parámetro es una cantidad aleatoria, y puede tomar dos valores \\(\\theta=0.4\\) ó \\(\\theta = 0.6\\).\nConsiderar esta cantidad como aleatoria requiere pensar, en este caso, en cómo se escogió la moneda, o qué sabemos acerca de las monedas que se usan para este experimento. Supongamos que en este caso, nos dicen que la moneda se escoge al azar de una bolsa donde hay una proporción similar de los dos tipos de moneda (0.4 ó 0.6). Es decir el espacio parametral es \\(\\Theta = \\{0.4, 0.6\\},\\) y las probabilidades asociadas a cada posibilidad son las mismas. Es decir, tenemos\n\\[P(\\theta = 0.4) = P(\\theta = 0.6) =0.5,\\]\nque representa la probabilidad de escoger de manera aleatoria la moneda con una carga en particular.\nAhora queremos calcular \\(P(X=2)\\), pero con el trabajo que hicimos esto es fácil. Pues requiere usar reglas de probabilidad usuales para hacerlo. Podemos utilizar probabilidad total \\[\\begin{align}\nP(X) &= \\sum_{\\theta \\in \\Theta} P(X, \\theta)\\\\\n&= \\sum_{\\theta \\in \\Theta} P(X\\, |\\, \\theta) P(\\theta),\n\\end{align}\\] lo cual en nuestro ejemplo se traduce en escribir\n\\[ P(X=2) = P(X=2|\\theta = 0.4)P(\\theta = 0.4) + P(X=2|\\theta=0.6)P(\\theta =0.6),\\]\npor lo que obtenemos\n\\[P(X=2) = 0.16(0.5) + 0.36(0.5) = 0.26.\\]\nFinalmente la probabilidad de haber escogido la moneda con carga \\(2/5\\) dado que observamos dos soles en el lanzamiento es\n\\[P(\\theta=0.4|X=2) = \\frac{0.16(0.5)}{0.26} \\approx  0.31.\\]\nEs decir, la probabilidad posterior de que estemos tirando la moneda \\(2/5\\) baja de 0.5 (nuestra información inicial) a 0.31.\nEste es un ejemplo completo, aunque muy simple, de inferencia bayesiana. La estrategia de inferencia bayesiana implica tomar decisiones basadas en las probabilidades posteriores.\n\n\n\n\n\n\nTip\n\n\n\n¿Cuál sería la estimación de máxima verosimilitud para este problema? ¿Cómo cuantificaríamos la incertidumbre en la estimación de máxima verosimilitud?\n\n\nFinalmente, podríamos hacer predicciones usando la posterior predictiva. Si \\({X}_{nv}\\) es una nueva tirada adicional de la moneda que estamos usando, nos interesaría saber:\n\\[P({X}_{nv}=\\mathsf{sol}\\, | \\, X=2)\\]\nNotemos que un volado adicional es un resultado binario. Por lo que podemos calcular observando que \\(P({X}_{nv}|X=2, \\theta)\\) es una variable Bernoulli con probabilidad \\(\\theta\\), que puede valer 0.4 ó 0.6. Como tenemos las probabilidades posteriores \\(P(\\theta|X=2)\\) podemos usar probabilidad total, condicionado en \\(X=2\\): \\[\\begin{align*}\nP({X}_{nv}=\\mathsf{sol}\\, | \\, X=2) & = \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}, \\theta \\, | \\, X=2) & \\text{(probabilidad total)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta , X=2) P(\\theta \\, | \\, X=2) & \\text{(probabilidad condicional)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta ) P(\\theta \\, | \\, X=2), & \\text{(independencia condicional)}\n\\end{align*}\\]\nlo que nos da el siguiente cálculo\n\\[P(X_{nv}=\\mathsf{sol}\\, |\\, \\theta=0.4) \\,  P(\\theta=0.4|X=2) \\,  +\\, P(X_{nv}=\\mathsf{sol}|\\theta = 0.6) \\, P(\\theta =0.6|X=2)\\]\nEs decir, promediamos ponderando con las probabilidades posteriores. Por lo tanto obtenemos\n\\[P(X_{nv} = \\mathsf{sol}|X=2) =  0.4 ( 0.31) + 0.6 (0.69) = 0.538.\\]\n\nObservación 0\nNótese que en contraste con máxima verosimilitud, en este ejemplo cuantificamos con probabilidad condicional la incertidumbre de los parámetros que no conocemos. En máxima verosimilitud esta probabilidad no tiene mucho sentido, pues nunca consideramos el parámetro desconocido como una cantidad aleatoria.\n\n\nObservación 1\nNótese el factor \\(P(X=2)\\) en la probabilidad posterior puede entenderse como un factor de normalización. Notemos que los denominadores en la distribución posterior son\n\\[P(X=2 | \\theta = 0.4) P(\\theta =0.4) = 0.16(0.5) = 0.08,\\]\ny\n\\[P(X=2 | \\theta = 0.6) P(\\theta =0.6) = 0.36(0.5) = 0.18.\\]\nLas probabilidades posteriores son proporcionales a estas dos cantidades, y como deben sumar uno, entonces normalizando estos dos números (dividiendo entre su suma) obtenemos las probabilidades.\n\n\nObservación 2\nLa nomenclatura que usamos es la siguiente:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, o modelo de los datos.\nA \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior.\n\nPara utilizar inferencia bayesiana, hay que hacer supuestos para definir las primeras dos partes del modelo. La parte de iniciales o previas está ausente de enfoques como máxima verosimlitud usual.\n\n\nObservación 3\n¿Cómo decidimos las probabilidades iniciales, por ejemplo \\(P(\\theta=0.4)\\) ?\nQuizá es un supuesto y no tenemos razón para pensar que se hace de otra manera. O quizá conocemos el mecanismo concreto con el que se selecciona la moneda. Discutiremos esto más adelante.\n\n\nObservación 4\n¿Cómo decidimos el modelo de los datos? Aquí típicamente también tenemos que hacer algunos supuestos, aunque algunos de estos pueden estar basados en el diseño del estudio, por ejemplo. Igual que cuando usamos máxima verosimilitud, es necesario checar que nuestro modelo ajusta razonablemente a los datos.\n\n\nEjercicio\nCambia distintos parámetros del número de soles observados, las probabilidades de sol de las monedas, y las probabilidades iniciales de selección de las monedas.\n\n\nCódigo\nn_volados <- 2\n# posible valores del parámetro desconocido\ntheta = c(0.4, 0.6)\n# probabilidades iniciales\nprobs_inicial <- tibble(moneda = c(1, 2),\n                        theta = theta,\n                        prob_inicial = c(0.5, 0.5))\nprobs_inicial\n\n\n# A tibble: 2 × 3\n  moneda theta prob_inicial\n   <dbl> <dbl>        <dbl>\n1      1   0.4          0.5\n2      2   0.6          0.5\n\n\nCódigo\n# verosimilitud\ncrear_verosim <- function(no_soles){\n    verosim <- function(theta){\n      # prob de observar no_soles en 2 volados con probabilidad de sol theta\n      dbinom(no_soles, 2, theta)\n    }\n    verosim\n}\n# evaluar verosimilitud\nverosim <- crear_verosim(2)\n# ahora usamos regla de bayes para hacer tabla de probabilidades\ntabla_inferencia <- probs_inicial %>%\n  mutate(verosimilitud = map_dbl(theta, verosim)) %>%\n  mutate(inicial_x_verosim = prob_inicial * verosimilitud) %>%\n  # normalizar\n  mutate(prob_posterior = inicial_x_verosim / sum(inicial_x_verosim))\n\ntabla_inferencia %>%\n  mutate(moneda_obs = moneda) %>%\n  select(moneda_obs, theta, prob_inicial, verosimilitud, prob_posterior)\n\n\n# A tibble: 2 × 5\n  moneda_obs theta prob_inicial verosimilitud prob_posterior\n       <dbl> <dbl>        <dbl>         <dbl>          <dbl>\n1          1   0.4          0.5          0.16          0.308\n2          2   0.6          0.5          0.36          0.692\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n¿Qué pasa cuando el número de soles es 0? ¿Cómo cambian las probabilidades posteriores de cada moneda?\nIncrementa el número de volados, por ejemplo a 10. ¿Qué pasa si observaste 8 soles, por ejemplo? ¿Y si observaste 0?\n¿Qué pasa si cambias las probabilidades iniciales (por ejemplo incrementas la probabilidad inicial de la moneda 1 a 0.9)?\n\n\n\nJustifica las siguientes aseveraciones (para este ejemplo):\n\n\n\n\n\n\nTip\n\n\n\n\nLas probabilidades posteriores o finales son una especie de punto intermedio entre verosimilitud y probablidades iniciales.\nSi tenemos pocas observaciones, las probabilidades posteriores son similares a las iniciales.\nCuando tenemos muchos datos, las probabilidades posteriores están más concentradas, y no es tan importante la inicial.\nSi la inicial está muy concentrada en algún valor, la posterior requiere de muchas observaciones para que se pueda concentrar en otros valores diferentes a los de la inicial.\n\n\n\nAhora resumimos los elementos básicos de la inferencia bayesiana, que son relativamente simples:\n\n\n\n\n\n\nInferencia bayesiana.\n\n\n\nCon la notación de arriba:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, proceso generador de datos o modelo de los datos.\nEl factor \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior\n\nHacemos inferencia usando la ecuación\n\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\\]\nque también escribimos:\n\\[P(\\theta | X) \\propto P(X | \\theta) P(\\theta)\\]\ndonde \\(\\propto\\) significa “proporcional a”. No ponemos \\(P(X)\\) pues como vimos arriba, es una constante de normalización.\n\n\nEn estadística Bayesiana, las probablidades posteriores \\(P(\\theta|X)\\) dan toda la información que necesitamos para hacer inferencia. ¿Cuándo damos probablidad alta a un parámetro particular \\(\\theta\\)? Cuando su verosimilitud es alta y/o cuando su probabilidad inicial es alta. De este modo, la posterior combina la información inicial que tenemos acerca de los parámetros con la información en la muestra acerca de los parámetros (verosimilitud). Podemos ilustrar como sigue:"
  },
  {
    "objectID": "modelos-bayesianos.html#ejemplo-estimando-una-proporción",
    "href": "modelos-bayesianos.html#ejemplo-estimando-una-proporción",
    "title": "9  Inferencia bayesiana",
    "section": "Ejemplo: estimando una proporción",
    "text": "Ejemplo: estimando una proporción\nRegresamos ahora a nuestro problema de estimar una proporción \\(\\theta\\) de una población dada usando una muestra iid \\(X_1,X_2,\\ldots, X_n\\) de variables Bernoulli. Ya sabemos calcular la verosimilitud (el modelo de los datos):\n\\[P(X_1=x_1,X_2 =x_2,\\ldots, X_n=x_n|\\theta) = \\theta^k(1-\\theta)^{n-k},\\]\ndonde \\(k = x_1 + x_2 +\\cdots + x_k\\) es el número de éxitos que observamos.\nAhora necesitamos una distribución inicial o previa \\(P(\\theta)\\). Aunque esta distribución puede tener cualquier forma, supongamos que nuestro conocimiento actual podemos resumirlo con una distribución \\(\\mathsf{Beta}(3, 3)\\):\n\\[P(\\theta) \\propto \\theta^2(1-\\theta)^2.\\]\nLa constante de normalización es 1/30, pero no la requerimos. Podemos simular para examinar su forma:\n\n\nCódigo\nsim_inicial <- tibble(theta = rbeta(10000, 3, 3))\nggplot(sim_inicial) + geom_histogram(aes(x = theta, y = ..density..), bins = 15)\n\n\n\n\n\nDe modo que nuestra información inicial es que la proporción puede tomar cualquier valor entre 0 y 1, pero es probable que tome un valor no tan lejano de 0.5. Por ejemplo, con probabilidad 0.95 creemos que \\(\\theta\\) está en el intervalo\n\n\nCódigo\nquantile(sim_inicial$theta, c(0.025, 0.975)) %>% round(2)\n\n\n 2.5% 97.5% \n 0.15  0.85 \n\n\nEs difícil justificar en abstracto por qué escogeriamos una inicial con esta forma. Aunque esto los detallaremos más adelante, puedes pensar, por el momento, que alguien observó algunos casos de esta población, y quizá vio tres éxitos y tres fracasos. Esto sugeriría que es poco probable que la probablidad \\(\\theta\\) sea muy cercana a 0 o muy cercana a 1.\nAhora podemos construir nuestra posterior. Tenemos que\n\\[P(\\theta| X_1=x_1, \\ldots, X_n=x_n) \\propto P(X_1 = x_1,\\ldots X_n=x_n | \\theta)P(\\theta) = \\theta^{k+2}(1-\\theta)^{n-k + 2}\\]\ndonde la constante de normalización no depende de \\(\\theta\\). Como \\(\\theta\\) es un parámetro continuo, la expresión de la derecha nos debe dar una densidad posterior.\nSupongamos entonces que hicimos la prueba con \\(n = 30\\) (número de prueba) y observamos 19 éxitos. Tendríamos entonces\n\\[P(\\theta | S_n = 19) \\propto \\theta^{19 + 2} (1-\\theta)^{30 - 19 +2} = \\theta^{21}(1-\\theta)^{13}\\]\nLa cantidad de la derecha, una vez que normalizemos por el número \\(P(X=19)\\), nos dará una densidad posterior (tal cual, esta expresion no integra a 1). Podemos obtenerla usando cálculo, pero recordamos que una distribución \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) tiene como fórmula\n\\[\\frac{1}{B(a,b)} \\theta^{a-1}(1-\\theta)^{b-1}\\]\nConcluimos entonces que la posterior tiene una distribución \\(\\mathsf{Beta}(22, 14)\\). Podemos simular de la posterior usando código estándar para ver cómo luce:\n\n\nCódigo\nsim_inicial <- sim_inicial %>% mutate(dist = \"inicial\")\nsim_posterior <- tibble(theta = rbeta(10000, 22, 14)) %>% mutate(dist = \"posterior\")\nsims <- bind_rows(sim_inicial, sim_posterior)\nggplot(sims, aes(x = theta, fill = dist)) +\n  geom_histogram(aes(x = theta), bins = 30, alpha = 0.5, position = \"identity\")\n\n\n\n\n\nLa posterior nos dice cuáles son las posibilidades de dónde puede estar el parámetro \\(\\theta\\). Nótese que ahora excluye prácticamente valores más chicos que 0.25 o mayores que 0.9. Esta distribución posterior es el objeto con el que hacemos inferencia: nos dice dónde es creíble que esté el parámetro \\(\\theta\\).\nPodemos resumir de varias maneras. Por ejemplo, si queremos un estimador puntual usamos la media posterior:\n\n\nCódigo\nsims %>% group_by(dist) %>%\n  summarise(theta_hat = mean(theta) %>% round(3))\n\n\n# A tibble: 2 × 2\n  dist      theta_hat\n  <chr>         <dbl>\n1 inicial       0.501\n2 posterior     0.61 \n\n\nNota que el estimador de máxima verosimilitud es \\(\\hat{p} = 19/30 = 0.63\\), que es ligeramente diferente de la media posterior. ¿Por qué?\nY podemos construir intervalos de percentiles, que en esta situación suelen llamarse intervalos de credibilidad, por ejemplo:\n\n\nCódigo\nf <- c(0.025, 0.975)\nsims %>% group_by(dist) %>%\n  summarise(cuantiles = quantile(theta, f) %>% round(2), f = f) %>%\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n\n`summarise()` has grouped output by 'dist'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   dist [2]\n  dist      `0.025` `0.975`\n  <chr>       <dbl>   <dbl>\n1 inicial      0.15    0.85\n2 posterior    0.44    0.76\n\n\nEl segundo renglón nos da un intervalo posterior para \\(\\theta\\) de credibilidad 95%. En inferencia bayesiana esto sustituye a los intervalos de confianza.\n\nEl intervalo de la inicial expresa nuestras creencias a priori acerca de \\(\\theta\\). Este intervalo es muy amplio (va de 0.15 a 0.85)\nEl intervalo de la posterior actualiza nuestras creencias acerca de \\(\\theta\\) una vez que observamos los datos, y es considerablemente más angosto y por lo tanto informativo.\n\nObservaciones:\n\nNótese que escogimos una forma analítica fácil para la inicial, pues resultó así que la posterior es una distribución beta. No siempre es así, y veremos qué hacer cuando nuestra inicial no es de un tipo “conveniente”.\nComo tenemos la forma analítica de la posterior, es posible hacer los cálculos de la media posterior, por ejemplo, integrando la densidad posterior a mano. Esto generalmente no es factible, y en este ejemplo preferimos hacer una aproximación numérica. En este caso particular es posible usando cálculo, y sabemos que la media de una \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) es \\(a/(a+b)\\), de modo que nuestra media posterior es\n\n\\[\\hat{\\mu} = (19 + 2)/(30 + 4) = 21/34 = 0.617 \\]\nque podemos interpretar como sigue: para calcular la media posterior, a nuestras \\(n\\) pruebas iniciales agregamos 4 pruebas adicionales fijas, con 2 éxitos y 2 fracasos, y calculamos la proporción usual de éxitos.\n\n\n\n\n\n\nTip\n\n\n\nRepite el análisis considerando en general \\(n\\) pruebas, con \\(k\\) éxitos. Utiliza la misma distribución inicial.\n\n\n\nLo mismo aplica para el intervalo de 95% (¿cómo se calcularía integrando?). También puedes usar la aproximación de R, por ejemplo:\n\n\n\nCódigo\nqbeta(0.025, shape1 = 22, shape2 = 14) %>% round(2)\n\n\n[1] 0.45\n\n\nCódigo\nqbeta(0.975, shape1 = 22, shape2 = 14) %>% round(2)\n\n\n[1] 0.76"
  },
  {
    "objectID": "modelos-bayesianos.html#ejemplo-observaciones-uniformes",
    "href": "modelos-bayesianos.html#ejemplo-observaciones-uniformes",
    "title": "9  Inferencia bayesiana",
    "section": "Ejemplo: observaciones uniformes",
    "text": "Ejemplo: observaciones uniformes\nAhora regresamos al problema de estimación del máximo de una distribución uniforme. En este caso, consideraremos un problema más concreto. Supongamos que hay una lotería (tipo tradicional) en México y no sabemos cuántos números hay. Obtendremos una muestra iid de \\(n\\) números, ya haremos una aproximación continua, suponiendo que\n\\[X_i \\sim U[0,\\theta]\\]\nLa verosimilitud es entonces\n\\[P(X_1,\\ldots, X_n|\\theta) = \\theta^{-n},\\]\ncuando \\(\\theta\\) es mayor que todas las \\(X_i\\), y cero en otro caso. Necesitaremos una inicial \\(P(\\theta)\\).\nPor la forma que tiene la verosimilitud, podemos intentar una distribución Pareto, que tiene la forma\n\\[P(\\theta) = \\frac{\\alpha \\theta_0^\\alpha}{\\theta^{\\alpha + 1}}\\]\ncon soporte en \\([\\theta_0,\\infty]\\). Tenemos que escoger entonces el mínimo \\(\\theta_0\\) y el parámetro \\(\\alpha\\). En primer lugar, como sabemos que es una lotería nacional, creemos que no puede haber menos de unos 300 mil números, así que \\(\\theta_0 = 300\\). La función acumulada de la pareto es \\(1- (300/\\theta)^\\alpha\\), así que el cuantil 99% es\n\n\nCódigo\nalpha <- 1.1\n(300/(0.01)^(1/alpha))\n\n\n[1] 19738\n\n\nes decir, alrededor de 20 millones de números. Creemos que es un poco probable que el número de boletos sea mayor a esta cota. Nótese ahora que la posterior cumple (multiplicando verosimilitud por inicial):\n\\[P(\\theta|X_1,\\ldots, X_n |\\theta) \\propto \\theta^{-(n + 2.1)}\\]\npara \\(\\theta\\) mayor que el máximo de las \\(X_n\\)’s y 300, y cero en otro caso. Esta distribución es pareto con \\(\\theta_0' = \\max\\{300, X_1,\\ldots, X_n\\}\\) y \\(\\alpha = n + 1.1\\)\nUna vez planteado nuestro modelo, veamos los datos. Obtuvimos la siguiente muestra de números:\n\n\nCódigo\nloteria_tbl <- read_csv(\"datos/nums_loteria_avion.csv\", col_names = c(\"id\", \"numero\")) %>%\n  mutate(numero = as.integer(numero))\n\n\nRows: 99 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): numero\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCódigo\nset.seed(334)\nmuestra_loteria <- sample_n(loteria_tbl, 25) %>%\n  mutate(numero = numero/1000)\nmuestra_loteria %>% as.data.frame %>% head\n\n\n  id   numero\n1 87  348.341\n2  5 5851.982\n3 40 1891.786\n4 51 1815.455\n5 14 5732.907\n6 48 3158.414\n\n\nPodemos simular de una Pareto como sigue:\n\n\nCódigo\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n\n\nSimulamos de la inicial:\n\n\nCódigo\nsims_pareto_inicial <- tibble(\n  theta = rpareto(20000, 300, 1.1 ),\n  dist = \"inicial\")\n\n\nY con los datos de la muestra, simulamos de la posterior:\n\n\nCódigo\nsims_pareto_posterior <- tibble(\n  theta = rpareto(20000,\n                  max(c(300, muestra_loteria$numero)),\n                  nrow(muestra_loteria) + 1.1),\n  dist = \"posterior\")\nsims_theta <- bind_rows(sims_pareto_inicial, sims_pareto_posterior)\nggplot(sims_theta) +\n  geom_histogram(aes(x = theta, fill = dist),\n                 bins = 70, alpha = 0.5, position = \"identity\",\n                 boundary = max(muestra_loteria$numero))  +\n  xlim(0, 15000) + scale_y_sqrt() +\n  geom_rug(data = muestra_loteria, aes(x = numero))\n\n\nWarning: Removed 273 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 4 rows containing missing values (geom_bar).\n\n\n\n\n\nNótese que cortamos algunos valores de la inicial en la cola derecha: un defecto de esta distribución inicial, con una cola tan larga a la derecha, es que pone cierto peso en valores que son poco creíbles y la vuelve poco apropiada para este problema. Regresamos más adelante a este problema.\nSi obtenemos percentiles, obtenemos el intervalo\n\n\nCódigo\nf <- c(0.025, 0.5, 0.975)\nsims_theta %>% group_by(dist) %>%\n  summarise(cuantiles = quantile(theta, f) %>% round(2), f = f) %>%\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n\n`summarise()` has grouped output by 'dist'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 4\n# Groups:   dist [2]\n  dist      `0.025` `0.5` `0.975`\n  <chr>       <dbl> <dbl>   <dbl>\n1 inicial      307.  569.   8449.\n2 posterior   5858. 6010.   6732.\n\n\nEstimamos entre 5.8 millones y 6.7 millones de boletos. El máximo en la muestra es de\n\n\nCódigo\nmax(muestra_loteria$numero)\n\n\n[1] 5851.982\n\n\nEscoger la distribución pareto como inicial es conveniente y nos permitió resolver el problema sin dificultad, pero por su forma vemos que no necesariamente es apropiada para el problema por lo que señalamos arriba. Nos gustaría, por ejemplo, poner una inicial como la siguiente\n\n\nCódigo\nqplot(rgamma(2000, 5, 0.001), geom=\"histogram\", bins = 20) +\n  scale_x_continuous(breaks = seq(1000, 15000, by = 2000))\n\n\n\n\n\nSin embargo, los cálculos no son tan simples en este caso, pues la posterior no tiene un forma reconocible. Tendremos que usar otras estrategias de simulación para ejemplos como este (Monte Carlo por medio de Cadenas de Markov, que veremos más adelante)."
  },
  {
    "objectID": "modelos-bayesianos.html#probabilidad-a-priori",
    "href": "modelos-bayesianos.html#probabilidad-a-priori",
    "title": "9  Inferencia bayesiana",
    "section": "Probabilidad a priori",
    "text": "Probabilidad a priori\nLa inferencia bayesiana es conceptualmente simple: siempre hay que calcular la posterior a partir de verosimilitud (modelo de datos) y distribución inicial o a priori. Sin embargo, una crítica usual que se hace de la inferencia bayesiana es precisamente que hay que tener esa información inicial, y que distintos analistas llegan a distintos resultados si tienen información inicial distinta.\nEso realmente no es un defecto, es una ventaja de la inferencia bayesiana. Los datos y los problemas que queremos resolver no viven en un vacío donde podemos creer que la estatura de las personas, por ejemplo, puede variar de 0 a mil kilómetros, el número de boletos de una lotería puede ir de 2 o 3 boletos o también quizá 500 millones de boletos, o la proporción de personas infectadas de una enfermedad puede ser de unos cuantos hasta miles de millones.\n\nEn todos estos casos tenemos cierta información inicial que podemos usar para informar nuestras estimaciones. Esta información debe usarse.\nAntes de tener datos, las probabilidades iniciales deben ser examinadas en términos del conocimiento de expertos.\nLas probabilidades iniciales son supuestos que hacemos acerca del problema de interés, y también están sujetas a críticas y confrontación con datos."
  },
  {
    "objectID": "modelos-bayesianos.html#análisis-conjugado",
    "href": "modelos-bayesianos.html#análisis-conjugado",
    "title": "9  Inferencia bayesiana",
    "section": "Análisis conjugado",
    "text": "Análisis conjugado\nLos dos ejemplos que hemos visto arriba son ejemplos de análisis conjugado:\n\n(Beta-bernoulli) Si las observaciones \\(X_i\\) son \\(\\mathsf{Bernoulli}(p)\\) (\\(n\\) fija) queremos estimar \\(p\\), y tomamos como distribución inicial para \\(p\\) una \\(\\mathsf{Beta}(a,b)\\), entonces la posterior para \\(p\\) cuando \\(S_n=k\\) es \\(\\mathsf{Beta}(k + a, n - k + b)\\), donde \\(S_n = X_1 + X_2 +\\cdots +X_n\\).\n\nY más en general:\n\n(Beta-binomial) Si las observaciones \\(X_i, i=1,2,\\ldots, m\\) son \\(\\mathsf{Binomial}(n_i, p)\\) (\\(n_i\\)’s fijas) independientes, queremos estimar \\(p\\), y tomamos como distribución inicial para \\(p\\) una \\(\\mathsf{Beta}(a,b)\\), entonces la posterior para \\(p\\) cuando \\(S_m=k\\) es \\(\\mathsf{Beta}(k + a, n - k + b)\\), donde \\(S_m = X_1 + X_2 +\\cdots +X_m\\) y \\(n= n_1+n_2+\\cdots+n_m\\)\n\nTambién aplicamos:\n\n(Uniforme-Pareto) Si el modelo de datos \\(X_i\\) es uniforme \\(\\mathsf{U}[0,\\theta]\\) (\\(n\\) fija), queremos estimar \\(\\theta\\), y tomamos como distribución inicial para \\(\\theta\\) una Pareto \\((\\theta_0, \\alpha)\\), entonces la posterior para \\(p\\) si el máximo de las \\(X_i\\)’s es igual a \\(M\\) es Pareto con parámetros \\((\\max\\{\\theta_0, M\\}, \\alpha + n)\\).\n\nNótese que en estos casos, dada una forma de la verosimilitud, tenemos una familia conocida de iniciales tales que las posteriores están en la misma familia. Estos modelos son convenientes porque podemos hacer simulaciones de la posterior de manera fácil, o usar sus propiedades teóricas.\nOtro ejemplo típico es el modelo normal-normal:\n\n(Normal-normal) Si \\(X_i\\sim \\mathsf{N}(\\mu,\\sigma)\\), con \\(\\sigma\\) conocida, y tomamos como distribución inicial para \\(\\mu \\sim \\mathsf{N}(\\mu_0,\\sigma_0)\\), y definimos la precisión \\(\\tau\\) como el inverso de la varianza \\(\\sigma^2\\), entonces la posterior de \\(\\mu\\) es Normal con media \\((1-\\lambda) \\mu_0 + \\lambda\\overline{x}\\), y precisión \\(\\tau_0 + n\\tau\\), donde \\(\\lambda = \\frac{n\\tau}{\\tau_0 + n\\tau}\\)\n\nMás útil es el siguiente modelo:\n\n(Normal-Gamma inverso) Sean \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma)\\). Queremos estimar \\(\\mu\\) y \\(\\sigma\\). Tomamos como distribuciones iniciales (dadas por 4 parámetros: \\(\\mu_0, n_0, \\alpha,\\beta\\)):\n\n\\(\\tau = \\frac{1}{\\sigma^2} \\sim \\mathsf{Gamma}(\\alpha,\\beta)\\)\n\\(\\mu|\\sigma\\) es normal con media \\(\\mu_0\\) y varianza \\(\\sigma^2 / {n_0}\\) , y\n\\(p(\\mu, \\sigma) = p(\\mu|\\sigma)p(\\sigma)\\)\n\nEntonces la posterior es:\n\n\\(\\tau|x\\) es \\(\\mathsf{Gamma}(\\alpha', \\beta')\\), con \\(\\alpha' = \\alpha + n/2\\), \\(\\beta' = \\beta + \\frac{1}{2}\\sum_{i=1}^{n}(x_{i} - \\bar{x})^2 + \\frac{nn_0}{n+n_0}\\frac{({\\bar{x}}-\\mu_{0})^2}{2}\\)\n\\(\\mu|\\sigma,x\\) es normal con media \\(\\mu' = \\frac{n_0\\mu_{0}+n{\\bar{x}}}{n_0 +n}\\) y varianza $ ^2/({n_0 +n})$.\n\\(p(\\mu,\\sigma|x) = p(\\mu|x,\\sigma)p(\\sigma|x)\\)\n\n\nObservaciones\n\nNótese que este último ejemplo tienen más de un parámetro. En estos casos, el objeto de interés es la posterior conjunta de los parámetros \\(p(\\theta_1,\\theta_2,\\cdots, \\theta_p|x)\\). Este último ejemplo es relativamente simple pues por la selección de iniciales, para simular de la conjunta de \\(\\mu\\) y \\(\\tau\\) podemos simular primero \\(\\tau\\) (o \\(\\sigma\\)), y después usar este valor para simular de \\(\\mu\\): el par de valores resultantes son una simulación de la conjunta.\nLos parámetros \\(a,b\\) para la inicial de \\(\\tau\\) pueden interpretarse como sigue: \\(\\sqrt{b/a}\\) es un valor “típico” a priori para la varianza poblacional, y \\(a\\) indica qué tan seguros estamos de este valor típico.\nNótese que para que funcionen las fórmulas de la manera más simple, escogimos una dependencia a priori entre la media y la precisión: \\(\\tau = \\sigma^{-2}\\) indica la escala de variabilidad que hay en la población, la incial de la media tiene varianza \\(\\sigma^2/n_0\\). Si la escala de variabilidad de la población es más grande, tenemos más incertidumbre acerca de la localización de la media.\nAunque esto tiene sentido en algunas aplicaciones, y por convenviencia usamos esta familia conjugada, muchas veces es preferible otro tipo de especificaciones para las iniciales: por ejemplo, la media normal y la desviación estándar uniforme, o media normal, con iniciales independientes. Sin embargo, estos casos no son tratables con análisis conjugado (veremos más adelante cómo tratarlos con MCMC).\n\n\nEjemplo\nSupongamos que queremos estimar la estatura de los cantantes de tesitura tenor con una muestra iid de tenores de Estados Unidos. Usaremos el modelo normal de forma que \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma^2)\\).\nUna vez decidido el modelo, tenemos que poner distribución inicial para los parámetros \\((\\mu, \\sigma^2)\\).\nComenzamos con \\(\\sigma^2\\). Como está el modelo, esta inicial debe estar dada para la precisión \\(\\tau\\), pero podemos simular para ver cómo se ve nuestra inicial para la desviación estándar. En la población general la desviación estándar es alrededor de 7 centímetros\n\n\nCódigo\n# Comenzamos seleccionando un valor que creemos típico para la desviación estándar\nsigma_0 <- 7\n# seleccionamos un valor para a, por ejemplo: si es más chico sigma tendrá más\n# disperisón\na <- 3\n# ponemos 8 = sqrt(b/a) -> b = a * 64\nb <- a * sigma_0^2\nc(a = a, b = b)\n\n\n  a   b \n  3 147 \n\n\nAhora simulamos para calcular cuantiles\n\n\nCódigo\ntau <- rgamma(1000, a, b)\nquantile(tau, c(0.05, 0.95))\n\n\n         5%         95% \n0.005781607 0.042170161 \n\n\nCódigo\nsigma <- 1 / sqrt(tau)\nmean(sigma)\n\n\n[1] 8.002706\n\n\nCódigo\nquantile(sigma, c(0.05, 0.95))\n\n\n       5%       95% \n 4.869653 13.151520 \n\n\nQue es dispersión considerable: con poca probabilidad la desviación estándar es menor a 4 centímetros, y también creemos que es poco creíble la desviación estándar sea de más de 13 centímetros.\nComenzamos con \\(\\mu\\). Sabemos, por ejemplo, que con alta probabilidad la media debe ser algún número entre 1.60 y 1.80. Podemos investigar: la media nacional en estados unidos está alrededor de 1.75, y el percentil 90% es 1.82. Esto es variabilidad en la población: debe ser muy poco probable, por ejemplo, que la media de tenores sea 1.82 Quizá los cantantes tienden a ser un poco más altos o bajos que la población general, así que podríamos agregar algo de dispersión.\nPodemos establecer parámetros y simular de la marginal a partir de las fórmulas de arriba para entender cómo se ve la inicial de \\(\\mu\\):\n\n\nCódigo\nmu_0 <- 175 # valor medio de inicial\nn_0 <- 5 # cuánta concentración en la inicial\ntau <- rgamma(1000, a,b)\nsigma <- 1/sqrt(tau)\nmu <- map_dbl(sigma, ~ rnorm(1, mu_0, .x / sqrt(n_0)))\nquantile(mu, c(0.05, 0.5, 0.95))\n\n\n      5%      50%      95% \n168.7275 174.8412 180.7905 \n\n\nQue consideramos un rango en el que con alta probabilidad debe estar la media poblacional de los cantantes.\nPodemos checar nuestros supuestos simulando posibles muestras usando sólo nuestra información previa:\n\n\nCódigo\nsimular_normal_invgamma <- function(n, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular media\n  tau <- rgamma(1, a, b)\n  sigma <- 1 / sqrt(tau)\n  mu <- rnorm(1, mu_0, sigma/sqrt(n_0))\n  # simular sigma\n  rnorm(n, mu, sigma)\n}\nset.seed(3461)\nsims_tbl <- tibble(rep = 1:20) %>%\n  mutate(estatura = map(rep, ~ simular_normal_invgamma(500, c(mu_0, n_0, a, b)))) %>%\n  unnest(cols = c(estatura))\nggplot(sims_tbl, aes(x = estatura)) + geom_histogram() +\n  facet_wrap(~ rep) +\n  geom_vline(xintercept = c(150, 180), colour = \"red\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPusimos líneas de referencia en 150 y 180. Vemos que nuestras iniciales no producen simulaciones totalmente fuera del contexto, y parecen cubrir apropiadamente el espacio de posiblidades para estaturas de los tenores. Quizá hay algunas realizaciones poco creíbles, pero no extremadamente. En este punto, podemos regresar y ajustar la inicial para \\(\\sigma\\), que parece tomar valores demasiado grandes (produciendo por ejemplo una simulación con estatura de 220 y 140, que deberían ser menos probables).\nAhora podemos usar los datos para calcular nuestras posteriores.\n\n\nCódigo\nset.seed(3413)\ncantantes <- lattice::singer %>%\n  mutate(estatura_cm = round(2.54 * height)) %>%\n  filter(str_detect(voice.part, \"Tenor\")) %>%\n  sample_n(20)\ncantantes\n\n\n    height voice.part estatura_cm\n139     70    Tenor 1         178\n150     68    Tenor 2         173\n140     65    Tenor 1         165\n132     66    Tenor 1         168\n152     69    Tenor 2         175\n141     72    Tenor 1         183\n161     71    Tenor 2         180\n156     71    Tenor 2         180\n158     71    Tenor 2         180\n164     69    Tenor 2         175\n147     68    Tenor 1         173\n130     72    Tenor 1         183\n162     71    Tenor 2         180\n134     74    Tenor 1         188\n170     69    Tenor 2         175\n167     68    Tenor 2         173\n149     64    Tenor 1         163\n143     68    Tenor 1         173\n157     69    Tenor 2         175\n153     71    Tenor 2         180\n\n\nLos cálculos son un poco tediosos, pero podemos construir una función apropiada:\n\n\nCódigo\ncalcular_pars_posterior <- function(x, pars_inicial){\n  # iniciales\n  mu_0 <- pars_inicial[1]\n  n_0 <- pars_inicial[2]\n  a_0 <- pars_inicial[3]\n  b_0 <- pars_inicial[4]\n  # muestra\n  n <- length(x)\n  media <- mean(x)\n  S2 <- sum((x - media)^2)\n  # sigma post\n  a_1 <- a_0 + 0.5 * n\n  b_1 <- b_0 + 0.5 * S2 + 0.5 * (n * n_0) / (n + n_0) * (media - mu_0)^2\n  # posterior mu\n  mu_1 <- (n_0 * mu_0 + n * media) / (n + n_0)\n  n_1 <- n + n_0\n  c(mu_1, n_1, a_1, b_1)\n}\npars_posterior <- calcular_pars_posterior(cantantes$estatura_cm, c(mu_0, n_0, a, b))\npars_posterior\n\n\n[1] 175.8  25.0  13.0 509.0\n\n\n¿Cómo se ve nuestra posterior comparada con la inicial? Podemos hacer simulaciones:\n\n\nCódigo\nsim_params <- function(m, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular sigmas\n  sims <- tibble(tau = rgamma(m, a, b)) %>%\n    mutate(sigma = 1 / sqrt(tau))\n  # simular mu\n  sims <- sims %>% mutate(mu = rnorm(m, mu_0, sigma / sqrt(n_0)))\n  sims\n}\nsims_inicial <- sim_params(5000, c(mu_0, n_0, a, b)) %>%\n  mutate(dist = \"inicial\")\nsims_posterior <- sim_params(5000, pars_posterior) %>%\n  mutate(dist = \"posterior\")\nsims <- bind_rows(sims_inicial, sims_posterior)\nggplot(sims, aes(x = mu, y = sigma, colour = dist)) +\n  geom_point()\n\n\n\n\n\nY vemos que nuestra posterior es consistente con la información inicial que usamos, hemos aprendido considerablemente de la muestra. La posterior se ve como sigue. Hemos marcado también las medias posteriores de cada parámetro: media y desviación estándar.\n\n\nCódigo\nmedias_post <- sims %>% filter(dist == \"posterior\") %>%\n  select(-dist) %>%\n  summarise(across(everything(), mean))\nggplot(sims %>% filter(dist == \"posterior\"),\n    aes(x = mu, y = sigma)) +\n  geom_point(colour = \"#00BFC4\") +\n  geom_point(data = medias_post, size = 5, colour = \"black\") +\n  coord_equal()\n\n\n\n\n\nPodemos construir intervalos creíbles del 90% para estos dos parámetros, por ejemplo haciendo intervalos de percentiles:\n\n\nCódigo\nf <- c(0.05, 0.5, 0.95)\nsims %>%\n  pivot_longer(cols = mu:sigma, names_to = \"parametro\") %>%\n  group_by(dist, parametro) %>%\n  summarise(cuantil = quantile(value, f) %>% round(1), f= f) %>%\n  pivot_wider(names_from = f, values_from = cuantil)\n\n\n`summarise()` has grouped output by 'dist', 'parametro'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   dist, parametro [4]\n  dist      parametro `0.05` `0.5` `0.95`\n  <chr>     <chr>      <dbl> <dbl>  <dbl>\n1 inicial   mu         169.  175.   181. \n2 inicial   sigma        4.8   7.4   13.3\n3 posterior mu         174.  176.   178. \n4 posterior sigma        5.1   6.3    8.2\n\n\nComo comparación, los estimadores de máxima verosimlitud son\n\n\nCódigo\nmedia_mv <- mean(cantantes$estatura_cm)\nsigma_mv <- mean((cantantes$estatura_cm - media_mv)^2) %>% sqrt\nc(media_mv, sigma_mv)\n\n\n[1] 176   6\n\n\nAhora solo resta checar que el modelo es razonable. Veremos más adelante cómo hacer esto, usando la distribución predictiva posterior."
  },
  {
    "objectID": "modelos-bayesianos.html#pasos-de-un-análisis-de-datos-bayesiano",
    "href": "modelos-bayesianos.html#pasos-de-un-análisis-de-datos-bayesiano",
    "title": "9  Inferencia bayesiana",
    "section": "Pasos de un análisis de datos bayesiano",
    "text": "Pasos de un análisis de datos bayesiano\n\n\n\n\n\n\nTip\n\n\n\nComo vimos en los ejemplos, en general un análisis de datos bayesiano sigue los siguientes pasos:\n\nIdentificar los datos releventes a nuestra pregunta de investigación, el tipo de datos que vamos a describir, que variables queremos estimar.\nDefinir el modelo descriptivo para los datos. La forma matemática y los parámetros deben ser apropiados para los objetivos del análisis.\nEspecificar la distribución inicial de los parámetros.\nUtilizar inferencia bayesiana para reubicar la credibilidad a lo largo de los posibles valores de los parámetros.\nVerificar que la distribución posterior replique los datos de manera razonable, de no ser el caso considerar otros modelos descriptivos para los datos.\n\n\n\n\nElicitando probablidades subjetivas (opcional)\nNo siempre es fácil elicitar probabilidades subjetivas de manera que capturemos el verdadero conocimiento de dominio que tenemos. Una manera clásica de hacerlo es con apuestas\nConsidera una pregunta sencilla que puede afectar a un viajero: ¿Qué tanto crees que habrá una tormenta que ocasionará el cierre de la autopista México-Acapulco en el puente del \\(20\\) de noviembre? Como respuesta debes dar un número entre \\(0\\) y \\(1\\) que refleje tus creencias. Una manera de seleccionar dicho número es calibrar las creencias en relación a otros eventos cuyas probabilidades son claras.\nComo evento de comparación considera una experimento donde hay canicas en una urna: \\(5\\) rojas y \\(5\\) blancas. Seleccionamos una canica al azar. Usaremos esta urna como comparación para considerar la tormenta en la autopista. Ahora, considera el siguiente par de apuestas de las cuales puedes elegir una:\n\nA. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre.\nB. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(5\\) canicas rojas y \\(5\\) blancas.\n\nSi prefieres la apuesta B, quiere decir que consideras que la probabilidad de tormenta es menor a \\(0.5\\), por lo que al menos sabes que tu creencia subjetiva de una la probabilidad de tormenta es menor a \\(0.5\\). Podemos continuar con el proceso para tener una mejor estimación de la creencia subjetiva.\n\nA. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre.\nC. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(1\\) canica roja y \\(9\\) blancas.\n\nSi ahora seleccionas la apuesta \\(A\\), esto querría decir que consideras que la probabilidad de que ocurra una tormenta es mayor a \\(0.10\\). Si consideramos ambas comparaciones tenemos que tu probabilidad subjetiva se ubica entre \\(0.1\\) y \\(0.5\\)."
  },
  {
    "objectID": "modelos-bayesianos.html#verificación-predictiva-posterior",
    "href": "modelos-bayesianos.html#verificación-predictiva-posterior",
    "title": "9  Inferencia bayesiana",
    "section": "Verificación predictiva posterior",
    "text": "Verificación predictiva posterior\nUna vez que ajustamos un modelo bayesiano, podemos simular nuevas observaciones a partir del modelo. Esto tiene dos utilidades:\n\nHacer predicciones acerca de datos no observados.\nConfirmar que nuevas producidas simuladas con el modelo son similares a las que de hecho observamos. Esto nos permite confirmar la calidad del ajuste del modelo, y se llama verificación predictiva posterior.\n\nSupongamos que tenemos la posterior \\(p(\\theta | x)\\). Podemos generar una nueva replicación de los datos como sigue:\nLa distribución predictiva posterior genera nuevas observaciones a partir de la información observada. La denotamos como \\(p(\\tilde{x}|x)\\).\nPara simular de ella:\n\nMuestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\).\nSimulamos del modelo de las observaciones \\(\\tilde{x} \\sim p(\\tilde{x}|\\tilde{\\theta})\\).\nRepetimos el proceso hasta obtener una muestra grande.\nUsamos este método para producir, por ejemplo, intervalos de predicción para nuevos datos.\n\nSi queremos una replicación de las observaciones de la predictiva posterior,\n\nMuestreamos un valor \\(\\tilde{\\theta}\\) de la posterior \\(p(\\theta|x)\\).\nSimulamos del modelo de las observaciones \\(\\tilde{x}_1, \\tilde{x}_2,\\ldots, \\tilde{x}_n \\sim p(\\tilde{x}|\\tilde{\\theta})\\), done \\(n\\) es el tamaño de muestra de la muestra original \\(x\\).\nUsamos este método para producir conjuntos de datos simulados que comparamos con los observados para verificar nuestro modelo.\n\n\nEjemplo: estaturas de tenores\nEn este ejemplo, usaremos la posterior predictiva para checar nuestro modelo. Vamos a crear varias muestras, del mismo tamaño que la original, según nuestra predictiva posterior, y compararemos estas muestras con la observada.\nY ahora simulamos otra muestra\n\n\nCódigo\nmuestra_sim <- simular_normal_invgamma(20, pars_posterior)\nmuestra_sim %>% round(0)\n\n\n [1] 167 181 184 181 167 167 172 170 177 172 169 174 182 184 176 171 175 176 168\n[20] 181\n\n\nPodemos simular varias muestras y hacer una prueba de lineup:\n\n\nCódigo\nlibrary(nullabor)\nsims_obs <- tibble(.n = 1:19) %>%\n  mutate(estatura_cm = map(.n, ~ simular_normal_invgamma(20, pars_posterior))) %>%\n  unnest(estatura_cm)\nset.seed(9921)\npos <- sample(1:20, 1)\nlineup_tbl <- lineup(true = cantantes %>% select(estatura_cm),\n                     samples = sims_obs, pos = pos)\nggplot(lineup_tbl, aes(x = estatura_cm)) + geom_histogram(binwidth = 2.5) +\n  facet_wrap(~.sample)\n\n\n\n\n\nCon este tipo de gráficas podemos checar desajustes potenciales de nuestro modelo.\n\n¿Puedes encontrar los datos verdaderos? ¿Cuántos seleccionaron los datos correctos?\nPrueba hacer pruebas con una gráfica de cuantiles. ¿Qué problema ves y cómo lo resolverías?\n\n\n\nEjemplo: modelo Poisson\nSupongamos que pensamos el modelo para las observaciones es Poisson con parámetro \\(\\lambda\\). Pondremos como inicial para \\(\\lambda\\) una exponencial con media 10.\nNótese que la posterior está dada por\n\\[p(\\lambda|x_1,\\ldots, x_n) \\propto e^{-n\\lambda}\\lambda^{\\sum_i x_i} e^{-0.1\\lambda} = \\lambda^{n\\overline{x}}e^{-\\lambda(n + 0.1)}\\]\nque es una distribución gamma con parámetros \\((n\\overline{x} + 1, n+0.1)\\)\nAhora supongamos que observamos la siguiente muestra, ajustamos nuestro modelo y hacemos replicaciones posteriores de los datos observados:\n\n\nCódigo\nx <- rnbinom(250, mu = 20, size = 3)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) %>%\n  bind_rows() %>%\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\nY vemos claramente que nuestro modelo no explica apropiadamente la variación de los datos observados. Contrasta con:\n\n\nCódigo\nset.seed(223)\nx <- rpois(250, 15)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) %>%\n  bind_rows() %>%\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\nY verificamos que en este caso el ajuste del modelo es apropiado."
  },
  {
    "objectID": "modelos-bayesianos.html#predicción",
    "href": "modelos-bayesianos.html#predicción",
    "title": "9  Inferencia bayesiana",
    "section": "Predicción",
    "text": "Predicción\nCuando queremos hacer predicciones particulares acerca de datos que observemos en el futuro, también podemos usar la posterior predictiva. En este caso, tenemos que considerar\n\nLa variabilidad que produce la incertidumbre en la estimación de los parámetros\nLa variabilidad de las observaciones dados los parámetros.\n\nEs decir, tenemos que simular sobre todos las combinaciones factibles de los parámetros.\n\nEjemplo: cantantes\nSi un nuevo tenor llega a un coro, ¿cómo hacemos una predicción de su estatura? Como siempre, quisiéramos obtener un intervalo que exprese nuestra incertidumbre acerca del valor que vamos a observar. Entonces haríamos:\n\n\nCódigo\nsims_posterior <- sim_params(50000, pars_posterior) %>%\n  mutate(y_pred = rnorm(n(), mu, sigma))\nsims_posterior %>% head\n\n\n# A tibble: 6 × 4\n     tau sigma    mu y_pred\n   <dbl> <dbl> <dbl>  <dbl>\n1 0.0286  5.91  175.   181.\n2 0.0200  7.07  177.   178.\n3 0.0257  6.23  176.   170.\n4 0.0344  5.39  176.   174.\n5 0.0297  5.80  175.   169.\n6 0.0282  5.96  177.   170.\n\n\n\n\nCódigo\nf <- c(0.025, 0.5, 0.975)\nsims_posterior %>% summarise(f = f, y_pred = quantile(y_pred, f))\n\n\n# A tibble: 3 × 2\n      f y_pred\n  <dbl>  <dbl>\n1 0.025   163.\n2 0.5     176.\n3 0.975   189.\n\n\nY con esto obtenemos el intervalo (163, 189), al 95%, para una nueva observación. Nótese que este intervalo no puede construirse con una simulación particular de la posterior de los parámetros, pues sería demasiado corto.\nEs posible demostrar que en este caso, la posterior predictiva tiene una forma conocida:\n\nLa posterior predictiva para el modelo normal-gamma inverso es una distribución \\(t\\) con \\(2\\alpha'\\) grados de libertad, centrada en \\(\\mu'\\), y con escala \\(s^2 = \\frac{\\beta'}{\\alpha'}\\frac{n + n_0 + 1}{n +n_0}\\)\n\n\n\nCódigo\nmu_post <- pars_posterior[1]\nn_post <- pars_posterior[2]\nalpha_post <- pars_posterior[3]\nbeta_post <- pars_posterior[4]\ns <- sqrt(beta_post/alpha_post) * sqrt((n_post + 1)/n_post)\nqt(c(0.025, 0.5, 0.975), 2 * alpha_post) * s + mu_post\n\n\n[1] 162.6832 175.8000 188.9168\n\n\n\n\nEjemplo: posterior predictiva de Pareto-Uniforme.\nLa posterior predictiva del modelo Pareto-Uniforme no tiene un nombre estándar, pero podemos aproximarla usando simulación. Usando los mismos datos del ejercicio de la lotería, haríamos:\n\n\nCódigo\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n# Simulamos de la posterior de los parámetros\nlim_inf_post <- max(c(300, muestra_loteria$numero))\nk_posterior <- nrow(muestra_loteria) + 1.1\nsims_pareto_posterior <- tibble(\n  theta = rpareto(100000, lim_inf_post, k_posterior))\n# Simulamos una observación para cada una de las anteriores:\nsims_post_pred <- sims_pareto_posterior %>%\n  mutate(x_pred = map_dbl(theta, ~ runif(1, 0, .x)))\n# Graficamos\nggplot(sims_post_pred, aes(x = x_pred)) +\n  geom_histogram(binwidth = 50) +\n  geom_vline(xintercept = lim_inf_post, colour = \"red\")\n\n\n\n\n\nQue es una mezcla de una uniforme con una Pareto.\n\n\n\n\nChihara, Laura M., y Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and R. 2.ª ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nKruschke, John. 2015. Doing Bayesian Data Analysis (Second Edition). Academic Press."
  }
]